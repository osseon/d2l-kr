
Gaussian Process Priors
=======================

Understanding Gaussian processes (GPs) is important for reasoning about
model construction and generalization, and for achieving
state-of-the-art performance in a variety of applications, including
active learning, and hyperparameter tuning in deep learning. GPs are
everywhere, and it is in our interests to know what they are and how we
can use them.

In this section, we introduce Gaussian process *priors* over functions.
In the next notebook, we show how to use these priors to do *posterior
inference* and make predictions. The next section can be viewed as “GPs
in a nutshell”, quickly giving what you need to apply Gaussian processes
in practice.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    import numpy as np
    from scipy.spatial import distance_matrix
    from d2l import torch as d2l
    
    d2l.set_figsize()

Definition
----------

A Gaussian process is defined as *a collection of random variables, any
finite number of which have a joint Gaussian distribution*. If a
function :math:`f(x)` is a Gaussian process, with *mean function*
:math:`m(x)` and *covariance function* or *kernel* :math:`k(x,x')`,
:math:`f(x) \sim \mathcal{GP}(m, k)`, then any collection of function
values queried at any collection of input points :math:`x` (times,
spatial locations, image pixels, etc.), has a joint multivariate
Gaussian distribution with mean vector :math:`\mu` and covariance matrix
:math:`K`: :math:`f(x_1),\dots,f(x_n) \sim \mathcal{N}(\mu, K)`, where
:math:`\mu_i = E[f(x_i)] = m(x_i)` and
:math:`K_{ij} = \textrm{Cov}(f(x_i),f(x_j)) = k(x_i,x_j)`.

This definition may seem abstract and inaccessible, but Gaussian
processes are in fact very simple objects. Any function

.. math:: f(x) = w^{\top} \phi(x) = \langle w, \phi(x) \rangle,
   :label: eq_gp-function

with :math:`w` drawn from a Gaussian (normal) distribution, and
:math:`\phi` being any vector of basis functions, for example
:math:`\phi(x) = (1, x, x^2, ..., x^d)^{\top}`, is a Gaussian process.
Moreover, any Gaussian process f(x) can be expressed in the form of
equation :eq:`eq_gp-function`. Let’s consider a few concrete
examples, to begin getting acquainted with Gaussian processes, after
which we can appreciate how simple and useful they really are.

A Simple Gaussian Process
-------------------------

Suppose :math:`f(x) = w_0 + w_1 x`, and
:math:`w_0, w_1 \sim \mathcal{N}(0,1)`, with :math:`w_0, w_1, x` all in
one dimension. We can equivalently write this function as the inner
product :math:`f(x) = (w_0, w_1)(1, x)^{\top}`. In
:eq:`eq_gp-function` above, :math:`w = (w_0, w_1)^{\top}` and
:math:`\phi(x) = (1,x)^{\top}`.

For any :math:`x`, :math:`f(x)` is a sum of two Gaussian random
variables. Since Gaussians are closed under addition, :math:`f(x)` is
also a Gaussian random variable for any :math:`x`. In fact, we can
compute for any particular :math:`x` that :math:`f(x)` is
:math:`\mathcal{N}(0,1+x^2)`. Similarly, the joint distribution for any
collection of function values, :math:`(f(x_1),\dots,f(x_n))`, for any
collection of inputs :math:`x_1,\dots,x_n`, is a multivariate Gaussian
distribution. Therefore :math:`f(x)` is a Gaussian process.

In short, :math:`f(x)` is a *random function*, or a *distribution over
functions*. We can gain some insights into this distribution by
repeatedly sampling values for :math:`w_0, w_1`, and visualizing the
corresponding functions :math:`f(x)`, which are straight lines with
slopes and different intercepts, as follows:

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    def lin_func(x, n_sample):
        preds = np.zeros((n_sample, x.shape[0]))
        for ii in range(n_sample):
            w = np.random.normal(0, 1, 2)
            y = w[0] + w[1] * x
            preds[ii, :] = y
        return preds
    
    x_points = np.linspace(-5, 5, 50)
    outs = lin_func(x_points, 10)
    lw_bd = -2 * np.sqrt((1 + x_points ** 2))
    up_bd = 2 * np.sqrt((1 + x_points ** 2))
    
    d2l.plt.fill_between(x_points, lw_bd, up_bd, alpha=0.25)
    d2l.plt.plot(x_points, np.zeros(len(x_points)), linewidth=4, color='black')
    d2l.plt.plot(x_points, outs.T)
    d2l.plt.xlabel("x", fontsize=20)
    d2l.plt.ylabel("f(x)", fontsize=20)
    d2l.plt.show()

If :math:`w_0` and :math:`w_1` are instead drawn from
:math:`\mathcal{N}(0,\alpha^2)`, how do you imagine varying
:math:`\alpha` affects the distribution over functions?

From Weight Space to Function Space
-----------------------------------

In the plot above, we saw how a distribution over parameters in a model
induces a distribution over functions. While we often have ideas about
the functions we want to model — whether they’re smooth, periodic,
quickly varying, etc. — it is relatively tedious to reason about the
parameters, which are largely uninterpretable. Fortunately, Gaussian
processes provide an easy mechanism to reason *directly* about
functions. Since a Gaussian distribution is entirely defined by its
first two moments, its mean and covariance matrix, a Gaussian process by
extension is defined by its mean function and covariance function.

In the above example, the mean function

.. math:: m(x) = E[f(x)] = E[w_0 + w_1x] = E[w_0] + E[w_1]x = 0+0 = 0.

Similarly, the covariance function is

.. math:: k(x,x') = \textrm{Cov}(f(x),f(x')) = E[f(x)f(x')]-E[f(x)]E[f(x')] = E[w_0^2 + w_0w_1x' + w_1w_0x + w_1^2xx'] = 1 + xx'.

Our distribution over functions can now be directly specified and
sampled from, without needing to sample from the distribution over
parameters. For example, to draw from :math:`f(x)`, we can simply form
our multivariate Gaussian distribution associated with any collection of
:math:`x` we want to query, and sample from it directly. We will begin
to see just how advantageous this formulation will be.

First, we note that essentially the same derivation for the simple
straight line model above can be applied to find the mean and covariance
function for *any* model of the form :math:`f(x) = w^{\top} \phi(x)`,
with :math:`w \sim \mathcal{N}(u,S)`. In this case, the mean function
:math:`m(x) = u^{\top}\phi(x)`, and the covariance function
:math:`k(x,x') = \phi(x)^{\top}S\phi(x')`. Since :math:`\phi(x)` can
represent a vector of any non-linear basis functions, we are considering
a very general model class, including models with an even an *infinite*
number of parameters.

The Radial Basis Function (RBF) Kernel
--------------------------------------

The *radial basis function* (RBF) kernel is the most popular covariance
function for Gaussian processes, and kernel machines in general. This
kernel has the form
:math:`k_{\textrm{RBF}}(x,x') = a^2\exp\left(-\frac{1}{2\ell^2}||x-x'||^2\right)`,
where :math:`a` is an amplitude parameter, and :math:`\ell` is a
*lengthscale* hyperparameter.

Let’s derive this kernel starting from weight space. Consider the
function

.. math:: f(x) = \sum_{i=1}^J w_i \phi_i(x), w_i  \sim \mathcal{N}\left(0,\frac{\sigma^2}{J}\right), \phi_i(x) = \exp\left(-\frac{(x-c_i)^2}{2\ell^2 }\right).

:math:`f(x)` is a sum of radial basis functions, with width
:math:`\ell`, centred at the points :math:`c_i`, as shown in the
following figure.

We can recognize :math:`f(x)` as having the form
:math:`w^{\top} \phi(x)`, where :math:`w = (w_1,\dots,w_J)^{\top}` and
:math:`\phi(x)` is a vector containing each of the radial basis
functions. The covariance function of this Gaussian process is then

.. math:: k(x,x') = \frac{\sigma^2}{J} \sum_{i=1}^{J} \phi_i(x)\phi_i(x').

Now let’s consider what happens as we take the number of parameters (and
basis functions) to infinity. Let :math:`c_J = \log J`,
:math:`c_1 = -\log J`, and
:math:`c_{i+1}-c_{i} = \Delta c = 2\frac{\log J}{J}`, and
:math:`J \to \infty`. The covariance function becomes the Riemann sum:

.. math:: k(x,x') = \lim_{J \to \infty} \frac{\sigma^2}{J} \sum_{i=1}^{J} \phi_i(x)\phi_i(x') = \int_{c_0}^{c_\infty} \phi_c(x)\phi_c(x') dc.

By setting :math:`c_0 = -\infty` and :math:`c_\infty = \infty`, we
spread the infinitely many basis functions across the whole real line,
each a distance :math:`\Delta c \to 0` apart:

.. math:: k(x,x') = \int_{-\infty}^{\infty} \exp(-\frac{(x-c)^2}{2\ell^2}) \exp(-\frac{(x'-c)^2}{2\ell^2 }) dc = \sqrt{\pi}\ell \sigma^2 \exp(-\frac{(x-x')^2}{2(\sqrt{2} \ell)^2}) \propto k_{\textrm{RBF}}(x,x').

It is worth taking a moment to absorb what we have done here. By moving
into the function space representation, we have derived how to represent
a model with an *infinite* number of parameters, using a finite amount
of computation. A Gaussian process with an RBF kernel is a *universal
approximator*, capable of representing any continuous function to
arbitrary precision. We can intuitively see why from the above
derivation. We can collapse each radial basis function to a point mass
taking :math:`\ell \to 0`, and give each point mass any height we wish.

So a Gaussian process with an RBF kernel is a model with an infinite
number of parameters and much more flexibility than any finite neural
network. Perhaps all the fuss about *overparametrized* neural networks
is misplaced. As we will see, GPs with RBF kernels do not overfit, and
in fact provide especially compelling generalization performance on
small datasets. Moreover, the examples in
:cite:`zhang2021understanding`, such as the ability to fit images with
random labels perfectly, but still generalize well on structured
problems, (can be perfectly reproduced using Gaussian processes)
:cite:`wilson2020bayesian`. Neural networks are not as distinct as we
make them out to be.

We can build further intuition about Gaussian processes with RBF
kernels, and hyperparameters such as *length-scale*, by sampling
directly from the distribution over functions. As before, this involves
a simple procedure:

1. Choose the input :math:`x` points we want to query the GP:
   :math:`x_1,\dots,x_n`.
2. Evaluate :math:`m(x_i)`, :math:`i = 1,\dots,n`, and
   :math:`k(x_i,x_j)` for :math:`i,j = 1,\dots,n` to respectively form
   the mean vector and covariance matrix :math:`\mu` and :math:`K`,
   where :math:`(f(x_1),\dots,f(x_n)) \sim \mathcal{N}(\mu, K)`.
3. Sample from this multivariate Gaussian distribution to obtain the
   sample function values.
4. Sample more times to visualize more sample functions queried at those
   points.

We illustrate this process in the figure below.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    def rbfkernel(x1, x2, ls=4.):  #@save
        dist = distance_matrix(np.expand_dims(x1, 1), np.expand_dims(x2, 1))
        return np.exp(-(1. / ls / 2) * (dist ** 2))
    
    x_points = np.linspace(0, 5, 50)
    meanvec = np.zeros(len(x_points))
    covmat = rbfkernel(x_points,x_points, 1)
    
    prior_samples= np.random.multivariate_normal(meanvec, covmat, size=5);
    d2l.plt.plot(x_points, prior_samples.T, alpha=0.5)
    d2l.plt.show()

The Neural Network Kernel
-------------------------

Research on Gaussian processes in machine learning was triggered by
research on neural networks. Radford Neal was pursuing ever larger
Bayesian neural networks, ultimately showing in 1994 (later published in
1996, as it was one of the most infamous NeurIPS rejections) that such
networks with an infinite number of hidden units become Gaussian
processes with particular kernel functions :cite:`neal1996bayesian`.
Interest in this derivation has re-surfaced, with ideas like the neural
tangent kernel being used to investigate the generalization properties
of neural networks :cite:`matthews2018gaussian`
:cite:`novak2018bayesian`. We can derive the neural network kernel as
follows.

Consider a neural network function :math:`f(x)` with one hidden layer:

.. math:: f(x) = b + \sum_{i=1}^{J} v_i h(x; u_i).

:math:`b` is a bias, :math:`v_i` are the hidden to output weights,
:math:`h` is any bounded hidden unit transfer function, :math:`u_i` are
the input to hidden weights, and :math:`J` is the number of hidden
units. Let :math:`b` and :math:`v_i` be independent with zero mean and
variances :math:`\sigma_b^2` and :math:`\sigma_v^2/J`, respectively, and
let the :math:`u_i` have independent identical distributions. We can
then use the central limit theorem to show that any collection of
function values :math:`f(x_1),\dots,f(x_n)` has a joint multivariate
Gaussian distribution.

The mean and covariance function of the corresponding Gaussian process
are:

.. math:: m(x) = E[f(x)] = 0

.. math:: k(x,x') = \textrm{cov}[f(x),f(x')] = E[f(x)f(x')] = \sigma_b^2 + \frac{1}{J} \sum_{i=1}^{J} \sigma_v^2 E[h_i(x; u_i)h_i(x'; u_i)]

In some cases, we can essentially evaluate this covariance function in
closed form. Let
:math:`h(x; u) = \textrm{erf}(u_0 + \sum_{j=1}^{P} u_j x_j)`, where
:math:`\textrm{erf}(z) = \frac{2}{\sqrt{\pi}} \int_{0}^{z} e^{-t^2} dt`,
and :math:`u \sim \mathcal{N}(0,\Sigma)`. Then
:math:`k(x,x') = \frac{2}{\pi} \textrm{sin}(\frac{2 \tilde{x}^{\top} \Sigma \tilde{x}'}{\sqrt{(1 + 2 \tilde{x}^{\top} \Sigma \tilde{x})(1 + 2 \tilde{x}'^{\top} \Sigma \tilde{x}')}})`.

The RBF kernel is *stationary*, meaning that it is *translation
invariant*, and therefore can be written as a function of
:math:`\tau = x-x'`. Intuitively, stationarity means that the high-level
properties of the function, such as rate of variation, do not change as
we move in input space. The neural network kernel, however, is
*non-stationary*. Below, we show sample functions from a Gaussian
process with this kernel. We can see that the function looks
qualitatively different near the origin.

Summary
-------

The first step in performing Bayesian inference involves specifying a
prior. Gaussian processes can be used to specify a whole prior over
functions. Starting from a traditional “weight space” view of modelling,
we can induce a prior over functions by starting with the functional
form of a model, and introducing a distribution over its parameters. We
can alternatively specify a prior distribution directly in function
space, with properties controlled by a kernel. The function-space
approach has many advantages. We can build models that actually
correspond to an infinite number of parameters, but use a finite amount
of computation! Moreover, while these models have a great amount of
flexibility, they also make strong assumptions about what types of
functions are a priori likely, leading to relatively good generalization
on small datasets.

The assumptions of models in function space are intuitively controlled
by kernels, which often encode higher level properties of functions,
such as smoothness and periodicity. Many kernels are stationary, meaning
that they are translation invariant. Functions drawn from a Gaussian
process with a stationary kernel have roughly the same high-level
properties (such as rate of variation) regardless of where we look in
the input space.

Gaussian processes are a relatively general model class, containing many
examples of models we are already familiar with, including polynomials,
Fourier series, and so on, as long as we have a Gaussian prior over the
parameters. They also include neural networks with an infinite number of
parameters, even without Gaussian distributions over the parameters.
This connection, discovered by Radford Neal, triggered machine learning
researchers to move away from neural networks, and towards Gaussian
processes.

Exercises
---------

1. Draw sample prior functions from a GP with an Ornstein-Uhlenbeck (OU)
   kernel,
   :math:`k_{\textrm{OU}}(x,x') = \exp\left(-\frac{1}{2\ell}||x - x'|\right)`.
   If you fix the lengthscale :math:`\ell` to be the same, how do these
   functions look different than sample functions from a GP with an RBF
   kernel?

2. How does changing the *amplitude* :math:`a^2` of the RBF kernel
   affect the distribution over functions?

3. Suppose we form :math:`u(x) = f(x) + 2g(x)`, where
   :math:`f(x) \sim \mathcal{GP}(m_1,k_1)` and
   :math:`g(x) \sim \mathcal{GP}(m_2,k_2)`. Is :math:`u(x)` a Gaussian
   process, and if so, what is its mean and covariance function?

4. Suppose we form :math:`g(x) = a(x)f(x)`, where
   :math:`f(x) \sim \mathcal{GP}(0,k)` and :math:`a(x) = x^2`. Is
   :math:`g(x)` a Gaussian process, and if so, what is its mean and
   covariance function? What is the effect of :math:`a(x)`? What do
   sample functions drawn from :math:`g(x)` look like?

5. Suppose we form :math:`u(x) = f(x)g(x)`, where
   :math:`f(x) \sim \mathcal{GP}(m_1,k_1)` and
   :math:`g(x) \sim \mathcal{GP}(m_2,k_2)`. Is :math:`u(x)` a Gaussian
   process, and if so, what is its mean and covariance function?

`Discussions <https://discuss.d2l.ai/t/12116>`__
