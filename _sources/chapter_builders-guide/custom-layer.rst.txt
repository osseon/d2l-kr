
Custom Layers
=============

One factor behind deep learning’s success is the availability of a wide
range of layers that can be composed in creative ways to design
architectures suitable for a wide variety of tasks. For instance,
researchers have invented layers specifically for handling images, text,
looping over sequential data, and performing dynamic programming. Sooner
or later, you will need a layer that does not exist yet in the deep
learning framework. In these cases, you must build a custom layer. In
this section, we show you how.



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-1-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-1-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#jax-1-2" onclick="tagClick('jax'); return false;" class="mdl-tabs__tab ">jax</a><a href="#tensorflow-1-3" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-1-0">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    import torch
    from torch import nn
    from torch.nn import functional as F
    from d2l import torch as d2l



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-1-1">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    from mxnet import np, npx
    from mxnet.gluon import nn
    from d2l import mxnet as d2l
    
    npx.set_np()



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="jax-1-2">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    import jax
    from flax import linen as nn
    from jax import numpy as jnp
    from d2l import jax as d2l



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="tensorflow-1-3">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    import tensorflow as tf
    from d2l import tensorflow as d2l



.. raw:: html

    </div>



.. raw:: html

    </div>

Layers without Parameters
-------------------------

To start, we construct a custom layer that does not have any parameters
of its own. This should look familiar if you recall our introduction to
modules in :numref:`sec_model_construction`. The following
``CenteredLayer`` class simply subtracts the mean from its input. To
build it, we simply need to inherit from the base layer class and
implement the forward propagation function.



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-3-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-3-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#jax-3-2" onclick="tagClick('jax'); return false;" class="mdl-tabs__tab ">jax</a><a href="#tensorflow-3-3" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-3-0">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    class CenteredLayer(nn.Module):
        def __init__(self):
            super().__init__()
    
        def forward(self, X):
            return X - X.mean()



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-3-1">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    class CenteredLayer(nn.Block):
        def __init__(self, **kwargs):
            super().__init__(**kwargs)
    
        def forward(self, X):
            return X - X.mean()



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="jax-3-2">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    class CenteredLayer(nn.Module):
        def __call__(self, X):
            return X - X.mean()



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="tensorflow-3-3">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    class CenteredLayer(tf.keras.Model):
        def __init__(self):
            super().__init__()
    
        def call(self, X):
            return X - tf.reduce_mean(X)



.. raw:: html

    </div>



.. raw:: html

    </div>

Let’s verify that our layer works as intended by feeding some data
through it.



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-5-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-5-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#jax-5-2" onclick="tagClick('jax'); return false;" class="mdl-tabs__tab ">jax</a><a href="#tensorflow-5-3" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-5-0">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    layer = CenteredLayer()
    layer(torch.tensor([1.0, 2, 3, 4, 5]))



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-5-1">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    layer = CenteredLayer()
    layer(np.array([1.0, 2, 3, 4, 5]))



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="jax-5-2">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    layer = CenteredLayer()
    layer(jnp.array([1.0, 2, 3, 4, 5]))



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="tensorflow-5-3">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    layer = CenteredLayer()
    layer(tf.constant([1.0, 2, 3, 4, 5]))



.. raw:: html

    </div>



.. raw:: html

    </div>

We can now incorporate our layer as a component in constructing more
complex models.



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-7-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-7-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#jax-7-2" onclick="tagClick('jax'); return false;" class="mdl-tabs__tab ">jax</a><a href="#tensorflow-7-3" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-7-0">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    net = nn.Sequential(nn.LazyLinear(128), CenteredLayer())



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-7-1">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    net = nn.Sequential()
    net.add(nn.Dense(128), CenteredLayer())
    net.initialize()



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="jax-7-2">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    net = nn.Sequential([nn.Dense(128), CenteredLayer()])



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="tensorflow-7-3">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    net = tf.keras.Sequential([tf.keras.layers.Dense(128), CenteredLayer()])



.. raw:: html

    </div>



.. raw:: html

    </div>

As an extra sanity check, we can send random data through the network
and check that the mean is in fact 0. Because we are dealing with
floating point numbers, we may still see a very small nonzero number due
to quantization.



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar text"><a href="#pytorch-9-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-9-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#jax-9-2" onclick="tagClick('jax'); return false;" class="mdl-tabs__tab ">jax</a><a href="#tensorflow-9-3" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-9-0">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    Y = net(torch.rand(4, 8))
    Y.mean()



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-9-1">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    Y = net(np.random.rand(4, 8))
    Y.mean()



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="jax-9-2">

Here we utilize the ``init_with_output`` method which returns both the
output of the network as well as the parameters. In this case we only
focus on the output.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    Y, _ = net.init_with_output(d2l.get_key(), jax.random.uniform(d2l.get_key(),
                                                                  (4, 8)))
    Y.mean()



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="tensorflow-9-3">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    Y = net(tf.random.uniform((4, 8)))
    tf.reduce_mean(Y)



.. raw:: html

    </div>



.. raw:: html

    </div>

Layers with Parameters
----------------------

Now that we know how to define simple layers, let’s move on to defining
layers with parameters that can be adjusted through training. We can use
built-in functions to create parameters, which provide some basic
housekeeping functionality. In particular, they govern access,
initialization, sharing, saving, and loading model parameters. This way,
among other benefits, we will not need to write custom serialization
routines for every custom layer.

Now let’s implement our own version of the fully connected layer. Recall
that this layer requires two parameters, one to represent the weight and
the other for the bias. In this implementation, we bake in the ReLU
activation as a default. This layer requires two input arguments:
``in_units`` and ``units``, which denote the number of inputs and
outputs, respectively.



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-11-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-11-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#jax-11-2" onclick="tagClick('jax'); return false;" class="mdl-tabs__tab ">jax</a><a href="#tensorflow-11-3" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-11-0">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    class MyLinear(nn.Module):
        def __init__(self, in_units, units):
            super().__init__()
            self.weight = nn.Parameter(torch.randn(in_units, units))
            self.bias = nn.Parameter(torch.randn(units,))
    
        def forward(self, X):
            linear = torch.matmul(X, self.weight.data) + self.bias.data
            return F.relu(linear)

Next, we instantiate the ``MyLinear`` class and access its model
parameters.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    linear = MyLinear(5, 3)
    linear.weight



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-11-1">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    class MyDense(nn.Block):
        def __init__(self, units, in_units, **kwargs):
            super().__init__(**kwargs)
            self.weight = self.params.get('weight', shape=(in_units, units))
            self.bias = self.params.get('bias', shape=(units,))
    
        def forward(self, x):
            linear = np.dot(x, self.weight.data(ctx=x.ctx)) + self.bias.data(
                ctx=x.ctx)
            return npx.relu(linear)

Next, we instantiate the ``MyDense`` class and access its model
parameters.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    dense = MyDense(units=3, in_units=5)
    dense.params



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="jax-11-2">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    class MyDense(nn.Module):
        in_units: int
        units: int
    
        def setup(self):
            self.weight = self.param('weight', nn.initializers.normal(stddev=1),
                                     (self.in_units, self.units))
            self.bias = self.param('bias', nn.initializers.zeros, self.units)
    
        def __call__(self, X):
            linear = jnp.matmul(X, self.weight) + self.bias
            return nn.relu(linear)

Next, we instantiate the ``MyDense`` class and access its model
parameters.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    dense = MyDense(5, 3)
    params = dense.init(d2l.get_key(), jnp.zeros((3, 5)))
    params



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="tensorflow-11-3">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    class MyDense(tf.keras.Model):
        def __init__(self, units):
            super().__init__()
            self.units = units
    
        def build(self, X_shape):
            self.weight = self.add_weight(name='weight',
                shape=[X_shape[-1], self.units],
                initializer=tf.random_normal_initializer())
            self.bias = self.add_weight(
                name='bias', shape=[self.units],
                initializer=tf.zeros_initializer())
    
        def call(self, X):
            linear = tf.matmul(X, self.weight) + self.bias
            return tf.nn.relu(linear)

Next, we instantiate the ``MyDense`` class and access its model
parameters.

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    dense = MyDense(3)
    dense(tf.random.uniform((2, 5)))
    dense.get_weights()



.. raw:: html

    </div>



.. raw:: html

    </div>

We can directly carry out forward propagation calculations using custom
layers.



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-13-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-13-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#jax-13-2" onclick="tagClick('jax'); return false;" class="mdl-tabs__tab ">jax</a><a href="#tensorflow-13-3" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-13-0">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    linear(torch.rand(2, 5))



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-13-1">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    dense.initialize()
    dense(np.random.uniform(size=(2, 5)))



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="jax-13-2">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    dense.apply(params, jax.random.uniform(d2l.get_key(),
                                           (2, 5)))



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="tensorflow-13-3">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    dense(tf.random.uniform((2, 5)))



.. raw:: html

    </div>



.. raw:: html

    </div>

We can also construct models using custom layers. Once we have that we
can use it just like the built-in fully connected layer.



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-15-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-15-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#jax-15-2" onclick="tagClick('jax'); return false;" class="mdl-tabs__tab ">jax</a><a href="#tensorflow-15-3" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-15-0">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))
    net(torch.rand(2, 64))



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-15-1">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    net = nn.Sequential()
    net.add(MyDense(8, in_units=64),
            MyDense(1, in_units=8))
    net.initialize()
    net(np.random.uniform(size=(2, 64)))



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="jax-15-2">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    net = nn.Sequential([MyDense(64, 8), MyDense(8, 1)])
    Y, _ = net.init_with_output(d2l.get_key(), jax.random.uniform(d2l.get_key(),
                                                                  (2, 64)))
    Y



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="tensorflow-15-3">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    net = tf.keras.models.Sequential([MyDense(8), MyDense(1)])
    net(tf.random.uniform((2, 64)))



.. raw:: html

    </div>



.. raw:: html

    </div>

Summary
-------

We can design custom layers via the basic layer class. This allows us to
define flexible new layers that behave differently from any existing
layers in the library. Once defined, custom layers can be invoked in
arbitrary contexts and architectures. Layers can have local parameters,
which can be created through built-in functions.

Exercises
---------

1. Design a layer that takes an input and computes a tensor reduction,
   i.e., it returns :math:`y_k = \sum_{i, j} W_{ijk} x_i x_j`.
2. Design a layer that returns the leading half of the Fourier
   coefficients of the data.



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar text"><a href="#pytorch-17-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-17-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#jax-17-2" onclick="tagClick('jax'); return false;" class="mdl-tabs__tab ">jax</a><a href="#tensorflow-17-3" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-17-0">

`Discussions <https://discuss.d2l.ai/t/59>`__



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-17-1">

`Discussions <https://discuss.d2l.ai/t/58>`__



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="jax-17-2">

`Discussions <https://discuss.d2l.ai/t/17993>`__



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="tensorflow-17-3">

`Discussions <https://discuss.d2l.ai/t/279>`__



.. raw:: html

    </div>



.. raw:: html

    </div>
