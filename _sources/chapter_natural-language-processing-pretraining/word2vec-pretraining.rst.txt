
.. _sec_word2vec_pretraining:

Pretraining word2vec
====================


We go on to implement the skip-gram model defined in
:numref:`sec_word2vec`. Then we will pretrain word2vec using negative
sampling on the PTB dataset. First of all, let’s obtain the data
iterator and the vocabulary for this dataset by calling the
``d2l.load_data_ptb`` function, which was described in
:numref:`sec_word2vec_data`



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-1-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-1-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-1-0">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    import math
    import torch
    from torch import nn
    from d2l import torch as d2l
    
    batch_size, max_window_size, num_noise_words = 512, 5, 5
    data_iter, vocab = d2l.load_data_ptb(batch_size, max_window_size,
                                         num_noise_words)



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-1-1">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    import math
    from mxnet import autograd, gluon, np, npx
    from mxnet.gluon import nn
    from d2l import mxnet as d2l
    
    npx.set_np()
    
    batch_size, max_window_size, num_noise_words = 512, 5, 5
    data_iter, vocab = d2l.load_data_ptb(batch_size, max_window_size,
                                         num_noise_words)



.. raw:: html

    </div>



.. raw:: html

    </div>

The Skip-Gram Model
-------------------

We implement the skip-gram model by using embedding layers and batch
matrix multiplications. First, let’s review how embedding layers work.

Embedding Layer
~~~~~~~~~~~~~~~

As described in :numref:`sec_seq2seq`, an embedding layer maps a
token’s index to its feature vector. The weight of this layer is a
matrix whose number of rows equals to the dictionary size
(``input_dim``) and number of columns equals to the vector dimension for
each token (``output_dim``). After a word embedding model is trained,
this weight is what we need.



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-3-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-3-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-3-0">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    embed = nn.Embedding(num_embeddings=20, embedding_dim=4)
    print(f'Parameter embedding_weight ({embed.weight.shape}, '
          f'dtype={embed.weight.dtype})')



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-3-1">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    embed = nn.Embedding(input_dim=20, output_dim=4)
    embed.initialize()
    embed.weight



.. raw:: html

    </div>



.. raw:: html

    </div>

The input of an embedding layer is the index of a token (word). For any
token index :math:`i`, its vector representation can be obtained from
the :math:`i^\textrm{th}` row of the weight matrix in the embedding
layer. Since the vector dimension (``output_dim``) was set to 4, the
embedding layer returns vectors with shape (2, 3, 4) for a minibatch of
token indices with shape (2, 3).



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-5-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-5-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-5-0">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    x = torch.tensor([[1, 2, 3], [4, 5, 6]])
    embed(x)



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-5-1">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    x = np.array([[1, 2, 3], [4, 5, 6]])
    embed(x)



.. raw:: html

    </div>



.. raw:: html

    </div>

Defining the Forward Propagation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the forward propagation, the input of the skip-gram model includes
the center word indices ``center`` of shape (batch size, 1) and the
concatenated context and noise word indices ``contexts_and_negatives``
of shape (batch size, ``max_len``), where ``max_len`` is defined in
:numref:`subsec_word2vec-minibatch-loading`. These two variables are
first transformed from the token indices into vectors via the embedding
layer, then their batch matrix multiplication (described in
:numref:`subsec_batch_dot`) returns an output of shape (batch size, 1,
``max_len``). Each element in the output is the dot product of a center
word vector and a context or noise word vector.



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-7-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-7-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-7-0">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    def skip_gram(center, contexts_and_negatives, embed_v, embed_u):
        v = embed_v(center)
        u = embed_u(contexts_and_negatives)
        pred = torch.bmm(v, u.permute(0, 2, 1))
        return pred



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-7-1">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    def skip_gram(center, contexts_and_negatives, embed_v, embed_u):
        v = embed_v(center)
        u = embed_u(contexts_and_negatives)
        pred = npx.batch_dot(v, u.swapaxes(1, 2))
        return pred



.. raw:: html

    </div>



.. raw:: html

    </div>

Let’s print the output shape of this ``skip_gram`` function for some
example inputs.



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-9-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-9-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-9-0">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    skip_gram(torch.ones((2, 1), dtype=torch.long),
              torch.ones((2, 4), dtype=torch.long), embed, embed).shape



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-9-1">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    skip_gram(np.ones((2, 1)), np.ones((2, 4)), embed, embed).shape



.. raw:: html

    </div>



.. raw:: html

    </div>

Training
--------

Before training the skip-gram model with negative sampling, let’s first
define its loss function.

Binary Cross-Entropy Loss
~~~~~~~~~~~~~~~~~~~~~~~~~

According to the definition of the loss function for negative sampling
in :numref:`subsec_negative-sampling`, we will use the binary
cross-entropy loss.



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-11-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-11-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-11-0">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    class SigmoidBCELoss(nn.Module):
        # Binary cross-entropy loss with masking
        def __init__(self):
            super().__init__()
    
        def forward(self, inputs, target, mask=None):
            out = nn.functional.binary_cross_entropy_with_logits(
                inputs, target, weight=mask, reduction="none")
            return out.mean(dim=1)
    
    loss = SigmoidBCELoss()



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-11-1">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    loss = gluon.loss.SigmoidBCELoss()



.. raw:: html

    </div>



.. raw:: html

    </div>

Recall our descriptions of the mask variable and the label variable in
:numref:`subsec_word2vec-minibatch-loading`. The following calculates
the binary cross-entropy loss for the given variables.



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-13-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-13-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-13-0">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    pred = torch.tensor([[1.1, -2.2, 3.3, -4.4]] * 2)
    label = torch.tensor([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]])
    mask = torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0]])
    loss(pred, label, mask) * mask.shape[1] / mask.sum(axis=1)



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-13-1">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    pred = np.array([[1.1, -2.2, 3.3, -4.4]] * 2)
    label = np.array([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]])
    mask = np.array([[1, 1, 1, 1], [1, 1, 0, 0]])
    loss(pred, label, mask) * mask.shape[1] / mask.sum(axis=1)



.. raw:: html

    </div>



.. raw:: html

    </div>

Below shows how the above results are calculated (in a less efficient
way) using the sigmoid activation function in the binary cross-entropy
loss. We can consider the two outputs as two normalized losses that are
averaged over non-masked predictions.



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-15-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-15-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-15-0">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    def sigmd(x):
        return -math.log(1 / (1 + math.exp(-x)))
    
    print(f'{(sigmd(1.1) + sigmd(2.2) + sigmd(-3.3) + sigmd(4.4)) / 4:.4f}')
    print(f'{(sigmd(-1.1) + sigmd(-2.2)) / 2:.4f}')



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-15-1">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    def sigmd(x):
        return -math.log(1 / (1 + math.exp(-x)))
    
    print(f'{(sigmd(1.1) + sigmd(2.2) + sigmd(-3.3) + sigmd(4.4)) / 4:.4f}')
    print(f'{(sigmd(-1.1) + sigmd(-2.2)) / 2:.4f}')



.. raw:: html

    </div>



.. raw:: html

    </div>

Initializing Model Parameters
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We define two embedding layers for all the words in the vocabulary when
they are used as center words and context words, respectively. The word
vector dimension ``embed_size`` is set to 100.



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-17-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-17-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-17-0">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    embed_size = 100
    net = nn.Sequential(nn.Embedding(num_embeddings=len(vocab),
                                     embedding_dim=embed_size),
                        nn.Embedding(num_embeddings=len(vocab),
                                     embedding_dim=embed_size))



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-17-1">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    embed_size = 100
    net = nn.Sequential()
    net.add(nn.Embedding(input_dim=len(vocab), output_dim=embed_size),
            nn.Embedding(input_dim=len(vocab), output_dim=embed_size))



.. raw:: html

    </div>



.. raw:: html

    </div>

Defining the Training Loop
~~~~~~~~~~~~~~~~~~~~~~~~~~

The training loop is defined below. Because of the existence of padding,
the calculation of the loss function is slightly different compared to
the previous training functions.



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-19-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-19-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-19-0">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    def train(net, data_iter, lr, num_epochs, device=d2l.try_gpu()):
        def init_weights(module):
            if type(module) == nn.Embedding:
                nn.init.xavier_uniform_(module.weight)
        net.apply(init_weights)
        net = net.to(device)
        optimizer = torch.optim.Adam(net.parameters(), lr=lr)
        animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                                xlim=[1, num_epochs])
        # Sum of normalized losses, no. of normalized losses
        metric = d2l.Accumulator(2)
        for epoch in range(num_epochs):
            timer, num_batches = d2l.Timer(), len(data_iter)
            for i, batch in enumerate(data_iter):
                optimizer.zero_grad()
                center, context_negative, mask, label = [
                    data.to(device) for data in batch]
    
                pred = skip_gram(center, context_negative, net[0], net[1])
                l = (loss(pred.reshape(label.shape).float(), label.float(), mask)
                         / mask.sum(axis=1) * mask.shape[1])
                l.sum().backward()
                optimizer.step()
                metric.add(l.sum(), l.numel())
                if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                    animator.add(epoch + (i + 1) / num_batches,
                                 (metric[0] / metric[1],))
        print(f'loss {metric[0] / metric[1]:.3f}, '
              f'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-19-1">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    def train(net, data_iter, lr, num_epochs, device=d2l.try_gpu()):
        net.initialize(ctx=device, force_reinit=True)
        trainer = gluon.Trainer(net.collect_params(), 'adam',
                                {'learning_rate': lr})
        animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                                xlim=[1, num_epochs])
        # Sum of normalized losses, no. of normalized losses
        metric = d2l.Accumulator(2)
        for epoch in range(num_epochs):
            timer, num_batches = d2l.Timer(), len(data_iter)
            for i, batch in enumerate(data_iter):
                center, context_negative, mask, label = [
                    data.as_in_ctx(device) for data in batch]
                with autograd.record():
                    pred = skip_gram(center, context_negative, net[0], net[1])
                    l = (loss(pred.reshape(label.shape), label, mask) *
                         mask.shape[1] / mask.sum(axis=1))
                l.backward()
                trainer.step(batch_size)
                metric.add(l.sum(), l.size)
                if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                    animator.add(epoch + (i + 1) / num_batches,
                                 (metric[0] / metric[1],))
        print(f'loss {metric[0] / metric[1]:.3f}, '
              f'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')



.. raw:: html

    </div>



.. raw:: html

    </div>

Now we can train a skip-gram model using negative sampling.



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-21-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-21-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-21-0">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    lr, num_epochs = 0.002, 5
    train(net, data_iter, lr, num_epochs)



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-21-1">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    lr, num_epochs = 0.002, 5
    train(net, data_iter, lr, num_epochs)



.. raw:: html

    </div>



.. raw:: html

    </div>

.. _subsec_apply-word-embed:

Applying Word Embeddings
------------------------


After training the word2vec model, we can use the cosine similarity of
word vectors from the trained model to find words from the dictionary
that are most semantically similar to an input word.



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-23-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-23-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-23-0">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    def get_similar_tokens(query_token, k, embed):
        W = embed.weight.data
        x = W[vocab[query_token]]
        # Compute the cosine similarity. Add 1e-9 for numerical stability
        cos = torch.mv(W, x) / torch.sqrt(torch.sum(W * W, dim=1) *
                                          torch.sum(x * x) + 1e-9)
        topk = torch.topk(cos, k=k+1)[1].cpu().numpy().astype('int32')
        for i in topk[1:]:  # Remove the input words
            print(f'cosine sim={float(cos[i]):.3f}: {vocab.to_tokens(i)}')
    
    get_similar_tokens('chip', 3, net[0])



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-23-1">

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    def get_similar_tokens(query_token, k, embed):
        W = embed.weight.data()
        x = W[vocab[query_token]]
        # Compute the cosine similarity. Add 1e-9 for numerical stability
        cos = np.dot(W, x) / np.sqrt(np.sum(W * W, axis=1) * np.sum(x * x) + 1e-9)
        topk = npx.topk(cos, k=k+1, ret_typ='indices').asnumpy().astype('int32')
        for i in topk[1:]:  # Remove the input words
            print(f'cosine sim={float(cos[i]):.3f}: {vocab.to_tokens(i)}')
    
    get_similar_tokens('chip', 3, net[0])



.. raw:: html

    </div>



.. raw:: html

    </div>

Summary
-------

-  We can train a skip-gram model with negative sampling using embedding
   layers and the binary cross-entropy loss.
-  Applications of word embeddings include finding semantically similar
   words for a given word based on the cosine similarity of word
   vectors.

Exercises
---------

1. Using the trained model, find semantically similar words for other
   input words. Can you improve the results by tuning hyperparameters?
2. When a training corpus is huge, we often sample context words and
   noise words for the center words in the current minibatch *when
   updating model parameters*. In other words, the same center word may
   have different context words or noise words in different training
   epochs. What are the benefits of this method? Try to implement this
   training method.



.. raw:: html

    <div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar text"><a href="#pytorch-25-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-25-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a></div>



.. raw:: html

    <div class="mdl-tabs__panel is-active" id="pytorch-25-0">

`Discussions <https://discuss.d2l.ai/t/1335>`__



.. raw:: html

    </div>



.. raw:: html

    <div class="mdl-tabs__panel " id="mxnet-25-1">

`Discussions <https://discuss.d2l.ai/t/384>`__



.. raw:: html

    </div>



.. raw:: html

    </div>
