<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>References &#8212; Dive into Deep Learning 1.0.3 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="23.8. The d2l API Document" href="../chapter_appendix-tools-for-deep-learning/d2l.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link is-active">References</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_references/zreferences.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="http://preview.d2l.ai/d2l-en/master">
                  <i class="fas fa-book"></i>
                  Preview Version
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai/d2l-en.pdf">
                  <i class="fas fa-file-pdf"></i>
                  PyTorch
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai/d2l-en-mxnet.pdf">
                  <i class="fas fa-file-pdf"></i>
                  MXNet
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai/d2l-en.zip">
                  <i class="fab fa-python"></i>
                  Notebooks
              </a>
          
              <a  class="mdl-navigation__link" href="https://courses.d2l.ai">
                  <i class="fas fa-user-graduate"></i>
                  Courses
              </a>
          
              <a  class="mdl-navigation__link" href="https://github.com/d2l-ai/d2l-en">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://zh.d2l.ai">
                  <i class="fas fa-external-link-alt"></i>
                  中文版
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="Dive into Deep Learning"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index.html">Notation</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. Preliminaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html">2.1. Data Manipulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html">2.2. Data Preprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html">2.3. Linear Algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html">2.4. Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html">2.5. Automatic Differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html">2.6. Probability and Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html">2.7. Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-regression/index.html">3. Linear Neural Networks for Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression.html">3.1. Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/oo-design.html">3.2. Object-Oriented Design for Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/synthetic-regression-data.html">3.3. Synthetic Regression Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression-scratch.html">3.4. Linear Regression Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression-concise.html">3.5. Concise Implementation of Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/generalization.html">3.6. Generalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/weight-decay.html">3.7. Weight Decay</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-classification/index.html">4. Linear Neural Networks for Classification</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression.html">4.1. Softmax Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/image-classification-dataset.html">4.2. The Image Classification Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/classification.html">4.3. The Base Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression-scratch.html">4.4. Softmax Regression Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression-concise.html">4.5. Concise Implementation of Softmax Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/generalization-classification.html">4.6. Generalization in Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/environment-and-distribution-shift.html">4.7. Environment and Distribution Shift</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multilayer-perceptrons/index.html">5. Multilayer Perceptrons</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp.html">5.1. Multilayer Perceptrons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-implementation.html">5.2. Implementation of Multilayer Perceptrons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/backprop.html">5.3. Forward Propagation, Backward Propagation, and Computational Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html">5.4. Numerical Stability and Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/generalization-deep.html">5.5. Generalization in Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/dropout.html">5.6. Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/kaggle-house-price.html">5.7. Predicting House Prices on Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_builders-guide/index.html">6. Builders’ Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/model-construction.html">6.1. Layers and Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/parameters.html">6.2. Parameter Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/init-param.html">6.3. Parameter Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/lazy-init.html">6.4. Lazy Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/custom-layer.html">6.5. Custom Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/read-write.html">6.6. File I/O</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/use-gpu.html">6.7. GPUs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index.html">7. Convolutional Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv.html">7.1. From Fully Connected Layers to Convolutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer.html">7.2. Convolutions for Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides.html">7.3. Padding and Stride</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels.html">7.4. Multiple Input and Multiple Output Channels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling.html">7.5. Pooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet.html">7.6. Convolutional Neural Networks (LeNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index.html">8. Modern Convolutional Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet.html">8.1. Deep Convolutional Neural Networks (AlexNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/vgg.html">8.2. Networks Using Blocks (VGG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/nin.html">8.3. Network in Network (NiN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet.html">8.4. Multi-Branch Networks (GoogLeNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm.html">8.5. Batch Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet.html">8.6. Residual Networks (ResNet) and ResNeXt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet.html">8.7. Densely Connected Networks (DenseNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/cnn-design.html">8.8. Designing Convolution Network Architectures</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index.html">9. Recurrent Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/sequence.html">9.1. Working with Sequences</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/text-sequence.html">9.2. Converting Raw Text into Sequence Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/language-model.html">9.3. Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn.html">9.4. Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch.html">9.5. Recurrent Neural Network Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-concise.html">9.6. Concise Implementation of Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/bptt.html">9.7. Backpropagation Through Time</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index.html">10. Modern Recurrent Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm.html">10.1. Long Short-Term Memory (LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru.html">10.2. Gated Recurrent Units (GRU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn.html">10.3. Deep Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/bi-rnn.html">10.4. Bidirectional Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset.html">10.5. Machine Translation and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder.html">10.6. The Encoder–Decoder Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq.html">10.7. Sequence-to-Sequence Learning for Machine Translation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search.html">10.8. Beam Search</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/index.html">11. Attention Mechanisms and Transformers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/queries-keys-values.html">11.1. Queries, Keys, and Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/attention-pooling.html">11.2. Attention Pooling by Similarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html">11.3. Attention Scoring Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/bahdanau-attention.html">11.4. The Bahdanau Attention Mechanism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/multihead-attention.html">11.5. Multi-Head Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html">11.6. Self-Attention and Positional Encoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/transformer.html">11.7. The Transformer Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/vision-transformer.html">11.8. Transformers for Vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html">11.9. Large-Scale Pretraining with Transformers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_optimization/index.html">12. Optimization Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro.html">12.1. Optimization and Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/convexity.html">12.2. Convexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/gd.html">12.3. Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/sgd.html">12.4. Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/minibatch-sgd.html">12.5. Minibatch Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/momentum.html">12.6. Momentum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adagrad.html">12.7. Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/rmsprop.html">12.8. RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adadelta.html">12.9. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adam.html">12.10. Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/lr-scheduler.html">12.11. Learning Rate Scheduling</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational-performance/index.html">13. Computational Performance</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hybridize.html">13.1. Compilers and Interpreters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/async-computation.html">13.2. Asynchronous Computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/auto-parallelism.html">13.3. Automatic Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hardware.html">13.4. Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus.html">13.5. Training on Multiple GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus-concise.html">13.6. Concise Implementation for Multiple GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/parameterserver.html">13.7. Parameter Servers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index.html">14. Computer Vision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation.html">14.1. Image Augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning.html">14.2. Fine-Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box.html">14.3. Object Detection and Bounding Boxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor.html">14.4. Anchor Boxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection.html">14.5. Multiscale Object Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset.html">14.6. The Object Detection Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd.html">14.7. Single Shot Multibox Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn.html">14.8. Region-based CNNs (R-CNNs)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset.html">14.9. Semantic Segmentation and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv.html">14.10. Transposed Convolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn.html">14.11. Fully Convolutional Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style.html">14.12. Neural Style Transfer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10.html">14.13. Image Classification (CIFAR-10) on Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog.html">14.14. Dog Breed Identification (ImageNet Dogs) on Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index.html">15. Natural Language Processing: Pretraining</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec.html">15.1. Word Embedding (word2vec)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training.html">15.2. Approximate Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html">15.3. The Dataset for Pretraining Word Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html">15.4. Pretraining word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove.html">15.5. Word Embedding with Global Vectors (GloVe)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding.html">15.6. Subword Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy.html">15.7. Word Similarity and Analogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert.html">15.8. Bidirectional Encoder Representations from Transformers (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset.html">15.9. The Dataset for Pretraining BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining.html">15.10. Pretraining BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index.html">16. Natural Language Processing: Applications</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html">16.1. Sentiment Analysis and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html">16.2. Sentiment Analysis: Using Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html">16.3. Sentiment Analysis: Using Convolutional Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html">16.4. Natural Language Inference and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html">16.5. Natural Language Inference: Using Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert.html">16.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html">16.7. Natural Language Inference: Fine-Tuning BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement-learning/index.html">17. Reinforcement Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/mdp.html">17.1. Markov Decision Process (MDP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/value-iter.html">17.2. Value Iteration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/qlearning.html">17.3. Q-Learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_gaussian-processes/index.html">18. Gaussian Processes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-intro.html">18.1. Introduction to Gaussian Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-priors.html">18.2. Gaussian Process Priors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-inference.html">18.3. Gaussian Process Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_hyperparameter-optimization/index.html">19. Hyperparameter Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/hyperopt-intro.html">19.1. What Is Hyperparameter Optimization?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/hyperopt-api.html">19.2. Hyperparameter Optimization API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/rs-async.html">19.3. Asynchronous Random Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/sh-intro.html">19.4. Multi-Fidelity Hyperparameter Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/sh-async.html">19.5. Asynchronous Successive Halving</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_generative-adversarial-networks/index.html">20. Generative Adversarial Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/gan.html">20.1. Generative Adversarial Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/dcgan.html">20.2. Deep Convolutional Generative Adversarial Networks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender-systems/index.html">21. Recommender Systems</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/recsys-intro.html">21.1. Overview of Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/movielens.html">21.2. The MovieLens Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/mf.html">21.3. Matrix Factorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/autorec.html">21.4. AutoRec: Rating Prediction with Autoencoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ranking.html">21.5. Personalized Ranking for Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/neumf.html">21.6. Neural Collaborative Filtering for Personalized Ranking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/seqrec.html">21.7. Sequence-Aware Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ctr.html">21.8. Feature-Rich Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/fm.html">21.9. Factorization Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/deepfm.html">21.10. Deep Factorization Machines</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/index.html">22. Appendix: Mathematics for Deep Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html">22.1. Geometry and Linear Algebraic Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/eigendecomposition.html">22.2. Eigendecompositions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/single-variable-calculus.html">22.3. Single Variable Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html">22.4. Multivariable Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/integral-calculus.html">22.5. Integral Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/random-variables.html">22.6. Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html">22.7. Maximum Likelihood</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/distributions.html">22.8. Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/naive-bayes.html">22.9. Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/statistics.html">22.10. Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/information-theory.html">22.11. Information Theory</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index.html">23. Appendix: Tools for Deep Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/jupyter.html">23.1. Using Jupyter Notebooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/sagemaker.html">23.2. Using Amazon SageMaker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/aws.html">23.3. Using AWS EC2 Instances</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/colab.html">23.4. Using Google Colab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html">23.5. Selecting Servers and GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/contributing.html">23.6. Contributing to This Book</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/utils.html">23.7. Utility Functions and Classes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/d2l.html">23.8. The <code class="docutils literal notranslate"><span class="pre">d2l</span></code> API Document</a></li>
</ul>
</li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">References</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="Dive into Deep Learning"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index.html">Notation</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. Preliminaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html">2.1. Data Manipulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html">2.2. Data Preprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html">2.3. Linear Algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html">2.4. Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html">2.5. Automatic Differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html">2.6. Probability and Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html">2.7. Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-regression/index.html">3. Linear Neural Networks for Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression.html">3.1. Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/oo-design.html">3.2. Object-Oriented Design for Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/synthetic-regression-data.html">3.3. Synthetic Regression Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression-scratch.html">3.4. Linear Regression Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression-concise.html">3.5. Concise Implementation of Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/generalization.html">3.6. Generalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/weight-decay.html">3.7. Weight Decay</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-classification/index.html">4. Linear Neural Networks for Classification</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression.html">4.1. Softmax Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/image-classification-dataset.html">4.2. The Image Classification Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/classification.html">4.3. The Base Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression-scratch.html">4.4. Softmax Regression Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression-concise.html">4.5. Concise Implementation of Softmax Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/generalization-classification.html">4.6. Generalization in Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/environment-and-distribution-shift.html">4.7. Environment and Distribution Shift</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multilayer-perceptrons/index.html">5. Multilayer Perceptrons</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp.html">5.1. Multilayer Perceptrons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-implementation.html">5.2. Implementation of Multilayer Perceptrons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/backprop.html">5.3. Forward Propagation, Backward Propagation, and Computational Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html">5.4. Numerical Stability and Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/generalization-deep.html">5.5. Generalization in Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/dropout.html">5.6. Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/kaggle-house-price.html">5.7. Predicting House Prices on Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_builders-guide/index.html">6. Builders’ Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/model-construction.html">6.1. Layers and Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/parameters.html">6.2. Parameter Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/init-param.html">6.3. Parameter Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/lazy-init.html">6.4. Lazy Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/custom-layer.html">6.5. Custom Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/read-write.html">6.6. File I/O</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/use-gpu.html">6.7. GPUs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index.html">7. Convolutional Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv.html">7.1. From Fully Connected Layers to Convolutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer.html">7.2. Convolutions for Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides.html">7.3. Padding and Stride</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels.html">7.4. Multiple Input and Multiple Output Channels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling.html">7.5. Pooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet.html">7.6. Convolutional Neural Networks (LeNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index.html">8. Modern Convolutional Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet.html">8.1. Deep Convolutional Neural Networks (AlexNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/vgg.html">8.2. Networks Using Blocks (VGG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/nin.html">8.3. Network in Network (NiN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet.html">8.4. Multi-Branch Networks (GoogLeNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm.html">8.5. Batch Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet.html">8.6. Residual Networks (ResNet) and ResNeXt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet.html">8.7. Densely Connected Networks (DenseNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/cnn-design.html">8.8. Designing Convolution Network Architectures</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index.html">9. Recurrent Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/sequence.html">9.1. Working with Sequences</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/text-sequence.html">9.2. Converting Raw Text into Sequence Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/language-model.html">9.3. Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn.html">9.4. Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch.html">9.5. Recurrent Neural Network Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-concise.html">9.6. Concise Implementation of Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/bptt.html">9.7. Backpropagation Through Time</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index.html">10. Modern Recurrent Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm.html">10.1. Long Short-Term Memory (LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru.html">10.2. Gated Recurrent Units (GRU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn.html">10.3. Deep Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/bi-rnn.html">10.4. Bidirectional Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset.html">10.5. Machine Translation and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder.html">10.6. The Encoder–Decoder Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq.html">10.7. Sequence-to-Sequence Learning for Machine Translation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search.html">10.8. Beam Search</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/index.html">11. Attention Mechanisms and Transformers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/queries-keys-values.html">11.1. Queries, Keys, and Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/attention-pooling.html">11.2. Attention Pooling by Similarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html">11.3. Attention Scoring Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/bahdanau-attention.html">11.4. The Bahdanau Attention Mechanism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/multihead-attention.html">11.5. Multi-Head Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html">11.6. Self-Attention and Positional Encoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/transformer.html">11.7. The Transformer Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/vision-transformer.html">11.8. Transformers for Vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html">11.9. Large-Scale Pretraining with Transformers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_optimization/index.html">12. Optimization Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro.html">12.1. Optimization and Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/convexity.html">12.2. Convexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/gd.html">12.3. Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/sgd.html">12.4. Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/minibatch-sgd.html">12.5. Minibatch Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/momentum.html">12.6. Momentum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adagrad.html">12.7. Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/rmsprop.html">12.8. RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adadelta.html">12.9. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adam.html">12.10. Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/lr-scheduler.html">12.11. Learning Rate Scheduling</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational-performance/index.html">13. Computational Performance</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hybridize.html">13.1. Compilers and Interpreters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/async-computation.html">13.2. Asynchronous Computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/auto-parallelism.html">13.3. Automatic Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hardware.html">13.4. Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus.html">13.5. Training on Multiple GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus-concise.html">13.6. Concise Implementation for Multiple GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/parameterserver.html">13.7. Parameter Servers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index.html">14. Computer Vision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation.html">14.1. Image Augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning.html">14.2. Fine-Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box.html">14.3. Object Detection and Bounding Boxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor.html">14.4. Anchor Boxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection.html">14.5. Multiscale Object Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset.html">14.6. The Object Detection Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd.html">14.7. Single Shot Multibox Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn.html">14.8. Region-based CNNs (R-CNNs)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset.html">14.9. Semantic Segmentation and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv.html">14.10. Transposed Convolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn.html">14.11. Fully Convolutional Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style.html">14.12. Neural Style Transfer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10.html">14.13. Image Classification (CIFAR-10) on Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog.html">14.14. Dog Breed Identification (ImageNet Dogs) on Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index.html">15. Natural Language Processing: Pretraining</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec.html">15.1. Word Embedding (word2vec)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training.html">15.2. Approximate Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html">15.3. The Dataset for Pretraining Word Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html">15.4. Pretraining word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove.html">15.5. Word Embedding with Global Vectors (GloVe)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding.html">15.6. Subword Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy.html">15.7. Word Similarity and Analogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert.html">15.8. Bidirectional Encoder Representations from Transformers (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset.html">15.9. The Dataset for Pretraining BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining.html">15.10. Pretraining BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index.html">16. Natural Language Processing: Applications</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html">16.1. Sentiment Analysis and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html">16.2. Sentiment Analysis: Using Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html">16.3. Sentiment Analysis: Using Convolutional Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html">16.4. Natural Language Inference and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html">16.5. Natural Language Inference: Using Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert.html">16.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html">16.7. Natural Language Inference: Fine-Tuning BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement-learning/index.html">17. Reinforcement Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/mdp.html">17.1. Markov Decision Process (MDP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/value-iter.html">17.2. Value Iteration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/qlearning.html">17.3. Q-Learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_gaussian-processes/index.html">18. Gaussian Processes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-intro.html">18.1. Introduction to Gaussian Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-priors.html">18.2. Gaussian Process Priors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-inference.html">18.3. Gaussian Process Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_hyperparameter-optimization/index.html">19. Hyperparameter Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/hyperopt-intro.html">19.1. What Is Hyperparameter Optimization?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/hyperopt-api.html">19.2. Hyperparameter Optimization API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/rs-async.html">19.3. Asynchronous Random Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/sh-intro.html">19.4. Multi-Fidelity Hyperparameter Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/sh-async.html">19.5. Asynchronous Successive Halving</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_generative-adversarial-networks/index.html">20. Generative Adversarial Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/gan.html">20.1. Generative Adversarial Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/dcgan.html">20.2. Deep Convolutional Generative Adversarial Networks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender-systems/index.html">21. Recommender Systems</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/recsys-intro.html">21.1. Overview of Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/movielens.html">21.2. The MovieLens Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/mf.html">21.3. Matrix Factorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/autorec.html">21.4. AutoRec: Rating Prediction with Autoencoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ranking.html">21.5. Personalized Ranking for Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/neumf.html">21.6. Neural Collaborative Filtering for Personalized Ranking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/seqrec.html">21.7. Sequence-Aware Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ctr.html">21.8. Feature-Rich Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/fm.html">21.9. Factorization Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/deepfm.html">21.10. Deep Factorization Machines</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/index.html">22. Appendix: Mathematics for Deep Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html">22.1. Geometry and Linear Algebraic Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/eigendecomposition.html">22.2. Eigendecompositions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/single-variable-calculus.html">22.3. Single Variable Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/multivariable-calculus.html">22.4. Multivariable Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/integral-calculus.html">22.5. Integral Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/random-variables.html">22.6. Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html">22.7. Maximum Likelihood</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/distributions.html">22.8. Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/naive-bayes.html">22.9. Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/statistics.html">22.10. Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-mathematics-for-deep-learning/information-theory.html">22.11. Information Theory</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index.html">23. Appendix: Tools for Deep Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/jupyter.html">23.1. Using Jupyter Notebooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/sagemaker.html">23.2. Using Amazon SageMaker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/aws.html">23.3. Using AWS EC2 Instances</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/colab.html">23.4. Using Google Colab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html">23.5. Selecting Servers and GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/contributing.html">23.6. Contributing to This Book</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/utils.html">23.7. Utility Functions and Classes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/d2l.html">23.8. The <code class="docutils literal notranslate"><span class="pre">d2l</span></code> API Document</a></li>
</ul>
</li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">References</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="references">
<h1>References<a class="headerlink" href="#references" title="Permalink to this heading">¶</a><div class="d2l-tabs" style="float:right"><div class="d2l-tabs__tab"><a href="https://colab.research.google.com/github/d2l-ai/d2l-pytorch-colab/blob/master/chapter_references/zreferences.ipynb" onclick="captureOutboundLink('https://colab.research.google.com/github/d2l-ai/d2l-pytorch-colab/blob/master/chapter_references/zreferences.ipynb'); return false;"> <button style="float:right", id="Colab_[pytorch]" class="mdl-button mdl-js-button mdl-button--primary mdl-js-ripple-effect"> <i class=" fas fa-external-link-alt"></i> Colab [pytorch] </button></a><div class="mdl-tooltip" data-mdl-for="Colab_[pytorch]"> Open the notebook in Colab</div></div><div class="d2l-tabs__tab"><a href="https://colab.research.google.com/github/d2l-ai/d2l-en-colab/blob/master/chapter_references/zreferences.ipynb" onclick="captureOutboundLink('https://colab.research.google.com/github/d2l-ai/d2l-en-colab/blob/master/chapter_references/zreferences.ipynb'); return false;"> <button style="float:right", id="Colab_[mxnet]" class="mdl-button mdl-js-button mdl-button--primary mdl-js-ripple-effect"> <i class=" fas fa-external-link-alt"></i> Colab [mxnet] </button></a><div class="mdl-tooltip" data-mdl-for="Colab_[mxnet]"> Open the notebook in Colab</div></div><div class="d2l-tabs__tab"><a href="https://colab.research.google.com/github/d2l-ai/d2l-jax-colab/blob/master/chapter_references/zreferences.ipynb" onclick="captureOutboundLink('https://colab.research.google.com/github/d2l-ai/d2l-jax-colab/blob/master/chapter_references/zreferences.ipynb'); return false;"> <button style="float:right", id="Colab_[jax]" class="mdl-button mdl-js-button mdl-button--primary mdl-js-ripple-effect"> <i class=" fas fa-external-link-alt"></i> Colab [jax] </button></a><div class="mdl-tooltip" data-mdl-for="Colab_[jax]"> Open the notebook in Colab</div></div><div class="d2l-tabs__tab"><a href="https://colab.research.google.com/github/d2l-ai/d2l-tensorflow-colab/blob/master/chapter_references/zreferences.ipynb" onclick="captureOutboundLink('https://colab.research.google.com/github/d2l-ai/d2l-tensorflow-colab/blob/master/chapter_references/zreferences.ipynb'); return false;"> <button style="float:right", id="Colab_[tensorflow]" class="mdl-button mdl-js-button mdl-button--primary mdl-js-ripple-effect"> <i class=" fas fa-external-link-alt"></i> Colab [tensorflow] </button></a><div class="mdl-tooltip" data-mdl-for="Colab_[tensorflow]"> Open the notebook in Colab</div></div></div></h1>
</div>
<div class="docutils container" id="id1">
<dl class="citation">
<dt class="label" id="id2"><span class="brackets">Abadi et al., 2016</span></dt>
<dd><p>Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., … et al. (2016). TensorFlow: a system for large-scale machine learning. <em>12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</em> (pp. 265–283).</p>
</dd>
<dt class="label" id="id3"><span class="brackets">Abdel-Hamid et al., 2014</span></dt>
<dd><p>Abdel-Hamid, O., Mohamed, A.-R., Jiang, H., Deng, L., Penn, G., &amp; Yu, D. (2014). Convolutional neural networks for speech recognition. <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, <em>22</em>(10), 1533–1545.</p>
</dd>
<dt class="label" id="id4"><span class="brackets">Ahmed et al., 2012</span></dt>
<dd><p>Ahmed, A., Aly, M., Gonzalez, J., Narayanamurthy, S., &amp; Smola, A. J. (2012). Scalable inference in latent variable models. <em>Proceedings of the Fifth ACM International Conference on Web Search and Data Mining</em> (pp. 123–132).</p>
</dd>
<dt class="label" id="id441"><span class="brackets">Akiba et al., 2019</span></dt>
<dd><p>Akiba, T., Sano, S., Yanase, T., Ohta, T., &amp; Koyama, M. (2019). Optuna: a next-generation hyperparameter optimization framework. <em>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>.</p>
</dd>
<dt class="label" id="id408"><span class="brackets">Alayrac et al., 2022</span></dt>
<dd><p>Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., … et al. (2022). Flamingo: a visual language model for few-shot learning. <em>ArXiv:2204.14198</em>.</p>
</dd>
<dt class="label" id="id7"><span class="brackets">Alsallakh et al., 2020</span></dt>
<dd><p>Alsallakh, B., Kokhlikyan, N., Miglani, V., Yuan, J., &amp; Reblitz-Richardson, O. (2020). Mind the PAD – CNNs can develop blind spots. <em>ArXiv:2010.02178</em>.</p>
</dd>
<dt class="label" id="id480"><span class="brackets">Anil et al., 2023</span></dt>
<dd><p>Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., … et al. (2023). PaLM 2 Technical Report. <em>ArXiv:2305.10403</em>.</p>
</dd>
<dt class="label" id="id427"><span class="brackets">Anil et al., 2020</span></dt>
<dd><p>Anil, R., Gupta, V., Koren, T., Regan, K., &amp; Singer, Y. (2020). Scalable second-order optimization for deep learning. <em>ArXiv:2002.09018</em>.</p>
</dd>
<dt class="label" id="id8"><span class="brackets">Aronszajn, 1950</span></dt>
<dd><p>Aronszajn, N. (1950). Theory of reproducing kernels. <em>Transactions of the American Mathematical Society</em>, <em>68</em>(3), 337–404.</p>
</dd>
<dt class="label" id="id9"><span class="brackets">Ba et al., 2016</span></dt>
<dd><p>Ba, J. L., Kiros, J. R., &amp; Hinton, G. E. (2016). Layer normalization. <em>ArXiv:1607.06450</em>.</p>
</dd>
<dt class="label" id="id383"><span class="brackets">Baevski &amp; Auli, 2018</span></dt>
<dd><p>Baevski, A., &amp; Auli, M. (2018). Adaptive input representations for neural language modeling. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id10"><span class="brackets">Bahdanau et al., 2014</span></dt>
<dd><p>Bahdanau, D., Cho, K., &amp; Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate. <em>ArXiv:1409.0473</em>.</p>
</dd>
<dt class="label" id="id483"><span class="brackets">Bai et al., 2022</span></dt>
<dd><p>Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., … et al. (2022). Constitutional AI: harmlessness from AI feedback. <em>ArXiv:2212.08073</em>.</p>
</dd>
<dt class="label" id="id444"><span class="brackets">Baptista &amp; Poloczek, 2018</span></dt>
<dd><p>Baptista, R., &amp; Poloczek, M. (2018). Bayesian optimization of combinatorial structures. <em>Proceedings of the 35th International Conference on Machine Learning</em>.</p>
</dd>
<dt class="label" id="id434"><span class="brackets">Bardenet et al., 2013</span></dt>
<dd><p>Bardenet, R., Brendel, M., Kégl, B., &amp; Sebag, M. (2013). Collaborative hyperparameter tuning. <em>Proceedings of the 30th International Conference on Machine Learning (ICML'13)</em>.</p>
</dd>
<dt class="label" id="id12"><span class="brackets">Bay et al., 2006</span></dt>
<dd><p>Bay, H., Tuytelaars, T., &amp; Van Gool, L. (2006). SURF: Speeded up robust features. <em>European Conference on Computer Vision</em> (pp. 404–417).</p>
</dd>
<dt class="label" id="id437"><span class="brackets">Bellman, 1966</span></dt>
<dd><p>Bellman, R. (1966). Dynamic programming. <em>Science</em>, <em>153</em>, 34–37.</p>
</dd>
<dt class="label" id="id463"><span class="brackets">Bellman, 1952</span></dt>
<dd><p>Bellman, R. (1952). On the theory of dynamic programming. <em>Proceedings of the National Academy of Sciences</em>, <em>38</em>(8), 716–719.</p>
</dd>
<dt class="label" id="id464"><span class="brackets">Bellman, 1957a</span></dt>
<dd><p>Bellman, R. (1957). A Markovian decision process. <em>Journal of Mathematics and Mechanics</em>, <em>6</em>(5), 679–684. URL: <a class="reference external" href="http://www.jstor.org/stable/24900506">http://www.jstor.org/stable/24900506</a></p>
</dd>
<dt class="label" id="id465"><span class="brackets">Bellman, 1957b</span></dt>
<dd><p>Bellman, R. (1957). <em>Dynamic Programming</em>. Dover Publications.</p>
</dd>
<dt class="label" id="id454"><span class="brackets">Beltagy et al., 2020</span></dt>
<dd><p>Beltagy, I., Peters, M. E., &amp; Cohan, A. (2020). Longformer: the long-document transformer. <em>ArXiv:2004.05150</em>.</p>
</dd>
<dt class="label" id="id13"><span class="brackets">Bengio et al., 2003</span></dt>
<dd><p>Bengio, Y., Ducharme, R., Vincent, P., &amp; Jauvin, C. (2003). A neural probabilistic language model. <em>Journal of Machine Learning Research</em>, <em>3</em>(Feb), 1137–1155.</p>
</dd>
<dt class="label" id="id14"><span class="brackets">Bengio et al., 1994</span></dt>
<dd><p>Bengio, Y., Simard, P., &amp; Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. <em>IEEE Transactions on Neural Networks</em>, <em>5</em>(2), 157–166.</p>
</dd>
<dt class="label" id="id453"><span class="brackets">Bergstra et al., 2011</span></dt>
<dd><p>Bergstra, J., Bardenet, R., Bengio, Y., &amp; Kégl, B. (2011). Algorithms for hyper-parameter optimization. <em>Advances in Neural Information Processing Systems</em>, <em>24</em>.</p>
</dd>
<dt class="label" id="id15"><span class="brackets">Bergstra et al., 2010</span></dt>
<dd><p>Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., … Bengio, Y. (2010). Theano: a CPU and GPU math compiler in Python. <em>Proc. 9th Python in Science Conference</em> (pp. 3–10).</p>
</dd>
<dt class="label" id="id16"><span class="brackets">Beutel et al., 2014</span></dt>
<dd><p>Beutel, A., Murray, K., Faloutsos, C., &amp; Smola, A. J. (2014). CoBaFi: collaborative Bayesian filtering. <em>Proceedings of the 23rd International Conference on World Wide Web</em> (pp. 97–108).</p>
</dd>
<dt class="label" id="id17"><span class="brackets">Bishop, 1995</span></dt>
<dd><p>Bishop, C. M. (1995). Training with noise is equivalent to Tikhonov regularization. <em>Neural Computation</em>, <em>7</em>(1), 108–116.</p>
</dd>
<dt class="label" id="id18"><span class="brackets">Bishop, 2006</span></dt>
<dd><p>Bishop, C. M. (2006). <em>Pattern Recognition and Machine Learning</em>. Springer.</p>
</dd>
<dt class="label" id="id19"><span class="brackets">Black &amp; Scholes, 1973</span></dt>
<dd><p>Black, F., &amp; Scholes, M. (1973). The pricing of options and corporate liabilities. <em>Journal of Political Economy</em>, <em>81</em>, 637–654.</p>
</dd>
<dt class="label" id="id20"><span class="brackets">Bodla et al., 2017</span></dt>
<dd><p>Bodla, N., Singh, B., Chellappa, R., &amp; Davis, L. S. (2017). Soft-NMS-improving object detection with one line of code. <em>Proceedings of the IEEE International Conference on Computer Vision</em> (pp. 5561–5569).</p>
</dd>
<dt class="label" id="id21"><span class="brackets">Bojanowski et al., 2017</span></dt>
<dd><p>Bojanowski, P., Grave, E., Joulin, A., &amp; Mikolov, T. (2017). Enriching word vectors with subword information. <em>Transactions of the Association for Computational Linguistics</em>, <em>5</em>, 135–146.</p>
</dd>
<dt class="label" id="id22"><span class="brackets">Bollobas, 1999</span></dt>
<dd><p>Bollobás, B. (1999). <em>Linear Analysis</em>. Cambridge University Press.</p>
</dd>
<dt class="label" id="id420"><span class="brackets">Bommasani et al., 2021</span></dt>
<dd><p>Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., … et al. (2021). On the opportunities and risks of foundation models. <em>ArXiv:2108.07258</em>.</p>
</dd>
<dt class="label" id="id23"><span class="brackets">Bottou, 2010</span></dt>
<dd><p>Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent. <em>Proceedings of COMPSTAT'2010</em> (pp. 177–186). Springer.</p>
</dd>
<dt class="label" id="id24"><span class="brackets">Bottou &amp; Le Cun, 1988</span></dt>
<dd><p>Bottou, L., &amp; Le Cun, Y. (1988). SN: a simulator for connectionist models. <em>Proceedings of NeuroNimes 88</em> (pp. 371–382). Nimes, France. URL: <a class="reference external" href="http://leon.bottou.org/papers/bottou-lecun-88">http://leon.bottou.org/papers/bottou-lecun-88</a></p>
</dd>
<dt class="label" id="id351"><span class="brackets">Boucheron et al., 2005</span></dt>
<dd><p>Boucheron, S., Bousquet, O., &amp; Lugosi, G. (2005). Theory of classification: a survey of some recent advances. <em>ESAIM: Probability and Statistics</em>, <em>9</em>, 323–375.</p>
</dd>
<dt class="label" id="id25"><span class="brackets">Bowman et al., 2015</span></dt>
<dd><p>Bowman, S. R., Angeli, G., Potts, C., &amp; Manning, C. D. (2015). A large annotated corpus for learning natural language inference. <em>ArXiv:1508.05326</em>.</p>
</dd>
<dt class="label" id="id26"><span class="brackets">Boyd &amp; Vandenberghe, 2004</span></dt>
<dd><p>Boyd, S., &amp; Vandenberghe, L. (2004). <em>Convex Optimization</em>. Cambridge, England: Cambridge University Press.</p>
</dd>
<dt class="label" id="id27"><span class="brackets">Bradley &amp; Terry, 1952</span></dt>
<dd><p>Bradley, R. A., &amp; Terry, M. E. (1952). Rank analysis of incomplete block designs: I. The method of paired comparisons. <em>Biometrika</em>, <em>39</em>(3/4), 324–345.</p>
</dd>
<dt class="label" id="id30"><span class="brackets">Brown &amp; Sandholm, 2017</span></dt>
<dd><p>Brown, N., &amp; Sandholm, T. (2017). Libratus: the superhuman AI for no-limit poker. <em>IJCAI</em> (pp. 5226–5228).</p>
</dd>
<dt class="label" id="id29"><span class="brackets">Brown et al., 1990</span></dt>
<dd><p>Brown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra, V. J., Jelinek, F., Lafferty, J., … Roossin, P. S. (1990). A statistical approach to machine translation. <em>Computational Linguistics</em>, <em>16</em>(2), 79–85.</p>
</dd>
<dt class="label" id="id28"><span class="brackets">Brown et al., 1988</span></dt>
<dd><p>Brown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra, V. J., Jelinek, F., Mercer, R. L., &amp; Roossin, P. (1988). A statistical approach to language translation. <em>COLING Budapest 1988 Volume 1: International Conference on Computational Linguistics</em>.</p>
</dd>
<dt class="label" id="id380"><span class="brackets">Brown et al., 2020</span></dt>
<dd><p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., … et al. (2020). Language models are few-shot learners. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 1877–1901.</p>
</dd>
<dt class="label" id="id31"><span class="brackets">Buslaev et al., 2020</span></dt>
<dd><p>Buslaev, A., Iglovikov, V. I., Khvedchenya, E., Parinov, A., Druzhinin, M., &amp; Kalinin, A. A. (2020). Albumentations: Fast and flexible image augmentations. <em>Information</em>, <em>11</em>(2), 125.</p>
</dd>
<dt class="label" id="id33"><span class="brackets">Campbell et al., 2002</span></dt>
<dd><p>Campbell, M., Hoane Jr, A. J., &amp; Hsu, F.-h. (2002). Deep blue. <em>Artificial Intelligence</em>, <em>134</em>(1-2), 57–83.</p>
</dd>
<dt class="label" id="id34"><span class="brackets">Canny, 1987</span></dt>
<dd><p>Canny, J. (1987). A computational approach to edge detection. <em>Readings in Computer Vision</em> (pp. 184–203). Elsevier.</p>
</dd>
<dt class="label" id="id36"><span class="brackets">Cer et al., 2017</span></dt>
<dd><p>Cer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., &amp; Specia, L. (2017). SemEval-2017 Task 1: semantic textual similarity multilingual and crosslingual focused evaluation. <em>Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</em> (pp. 1–14).</p>
</dd>
<dt class="label" id="id270"><span class="brackets">Chan et al., 2015</span></dt>
<dd><p>Chan, W., Jaitly, N., Le, Q. V., &amp; Vinyals, O. (2015). Listen, attend and spell. <em>ArXiv:1508.01211</em>.</p>
</dd>
<dt class="label" id="id456"><span class="brackets">Chen et al., 2021</span></dt>
<dd><p>Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., … Mordatch, I. (2021). Decision transformer: reinforcement learning via sequence modeling. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>, 15084–15097.</p>
</dd>
<dt class="label" id="id37"><span class="brackets">Chen et al., 2015</span></dt>
<dd><p>Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., … Zhang, Z. (2015). MXNET: a flexible and efficient machine learning library for heterogeneous distributed systems. <em>ArXiv:1512.01274</em>.</p>
</dd>
<dt class="label" id="id38"><span class="brackets">Cheng et al., 2016</span></dt>
<dd><p>Cheng, J., Dong, L., &amp; Lapata, M. (2016). Long short-term memory-networks for machine reading. <em>Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em> (pp. 551–561).</p>
</dd>
<dt class="label" id="id39"><span class="brackets">Chetlur et al., 2014</span></dt>
<dd><p>Chetlur, S., Woolley, C., Vandermersch, P., Cohen, J., Tran, J., Catanzaro, B., &amp; Shelhamer, E. (2014). CuDNN: Efficient primitives for deep learning. <em>ArXiv:1410.0759</em>.</p>
</dd>
<dt class="label" id="id40"><span class="brackets">Cho et al., 2014a</span></dt>
<dd><p>Cho, K., Van Merriënboer, B., Bahdanau, D., &amp; Bengio, Y. (2014). On the properties of neural machine translation: Encoder–decoder approaches. <em>ArXiv:1409.1259</em>.</p>
</dd>
<dt class="label" id="id41"><span class="brackets">Cho et al., 2014b</span></dt>
<dd><p>Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp; Bengio, Y. (2014). Learning phrase representations using RNN encoder–decoder for statistical machine translation. <em>ArXiv:1406.1078</em>.</p>
</dd>
<dt class="label" id="id401"><span class="brackets">Chowdhery et al., 2022</span></dt>
<dd><p>Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., … et al. (2022). PaLM: scaling language modeling with pathways. <em>ArXiv:2204.02311</em>.</p>
</dd>
<dt class="label" id="id43"><span class="brackets">Chung et al., 2014</span></dt>
<dd><p>Chung, J., Gulcehre, C., Cho, K., &amp; Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. <em>ArXiv:1412.3555</em>.</p>
</dd>
<dt class="label" id="id387"><span class="brackets">Clark et al., 2020</span></dt>
<dd><p>Clark, K., Luong, M.-T., Le, Q. V., &amp; Manning, C. D. (2020). ELECTRA: pre-training text encoders as discriminators rather than generators. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id44"><span class="brackets">Collobert et al., 2011</span></dt>
<dd><p>Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., &amp; Kuksa, P. (2011). Natural language processing (almost) from scratch. <em>Journal of Machine Learning Research</em>, <em>12</em>, 2493–2537.</p>
</dd>
<dt class="label" id="id379"><span class="brackets">Cordonnier et al., 2020</span></dt>
<dd><p>Cordonnier, J.-B., Loukas, A., &amp; Jaggi, M. (2020). On the relationship between self-attention and convolutional layers. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id46"><span class="brackets">Cover &amp; Thomas, 1999</span></dt>
<dd><p>Cover, T., &amp; Thomas, J. (1999). <em>Elements of Information Theory</em>. John Wiley &amp; Sons.</p>
</dd>
<dt class="label" id="id48"><span class="brackets">Csiszar, 2008</span></dt>
<dd><p>Csiszár, I. (2008). Axiomatic characterizations of information measures. <em>Entropy</em>, <em>10</em>(3), 261–273.</p>
</dd>
<dt class="label" id="id49"><span class="brackets">Cybenko, 1989</span></dt>
<dd><p>Cybenko, G. (1989). Approximation by superpositions of a sigmoidal function. <em>Mathematics of Control, Signals and Systems</em>, <em>2</em>(4), 303–314.</p>
</dd>
<dt class="label" id="id50"><span class="brackets">Dalal &amp; Triggs, 2005</span></dt>
<dd><p>Dalal, N., &amp; Triggs, B. (2005). Histograms of oriented gradients for human detection. <em>2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)</em> (pp. 886–893).</p>
</dd>
<dt class="label" id="id51"><span class="brackets">DeCock, 2011</span></dt>
<dd><p>De Cock, D. (2011). Ames, Iowa: alternative to the Boston housing data as an end of semester regression project. <em>Journal of Statistics Education</em>, <em>19</em>(3).</p>
</dd>
<dt class="label" id="id54"><span class="brackets">Dean et al., 2012</span></dt>
<dd><p>Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V., … et al. (2012). Large scale distributed deep networks. <em>Proceedings of the 25th International Conference on Neural Information Processing Systems, Volume 1</em> (pp. 1223–1231).</p>
</dd>
<dt class="label" id="id53"><span class="brackets">DeCandia et al., 2007</span></dt>
<dd><p>DeCandia, G., Hastorun, D., Jampani, M., Kakulapati, G., Lakshman, A., Pilchin, A., … Vogels, W. (2007). Dynamo: Amazon's highly available key-value store. <em>ACM SIGOPS Operating Systems Review</em> (pp. 205–220).</p>
</dd>
<dt class="label" id="id55"><span class="brackets">Deng et al., 2009</span></dt>
<dd><p>Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., &amp; Fei-Fei, L. (2009). Imagenet: a large-scale hierarchical image database. <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 248–255).</p>
</dd>
<dt class="label" id="id56"><span class="brackets">DerKiureghian &amp; Ditlevsen, 2009</span></dt>
<dd><p>Der Kiureghian, A., &amp; Ditlevsen, O. (2009). Aleatory or epistemic? does it matter? <em>Structural Safety</em>, <em>31</em>(2), 105–112.</p>
</dd>
<dt class="label" id="id57"><span class="brackets">Devlin et al., 2018</span></dt>
<dd><p>Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. <em>ArXiv:1810.04805</em>.</p>
</dd>
<dt class="label" id="id477"><span class="brackets">Dinh et al., 2014</span></dt>
<dd><p>Dinh, L., Krueger, D., &amp; Bengio, Y. (2014). NICE: non-linear independent components estimation. <em>ArXiv:1410.8516</em>.</p>
</dd>
<dt class="label" id="id478"><span class="brackets">Dinh et al., 2017</span></dt>
<dd><p>Dinh, L., Sohl-Dickstein, J., &amp; Bengio, S. (2017). Density estimation using real NVP. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id58"><span class="brackets">Doersch et al., 2015</span></dt>
<dd><p>Doersch, C., Gupta, A., &amp; Efros, A. A. (2015). Unsupervised visual representation learning by context prediction. <em>Proceedings of the IEEE International Conference on Computer Vision</em> (pp. 1422–1430).</p>
</dd>
<dt class="label" id="id59"><span class="brackets">Dosovitskiy et al., 2021</span></dt>
<dd><p>Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., … et al. (2021). An image is worth 16 x 16 words: transformers for image recognition at scale. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id62"><span class="brackets">Duchi et al., 2011</span></dt>
<dd><p>Duchi, J., Hazan, E., &amp; Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. <em>Journal of Machine Learning Research</em>, <em>12</em>, 2121–2159.</p>
</dd>
<dt class="label" id="id63"><span class="brackets">Dumoulin &amp; Visin, 2016</span></dt>
<dd><p>Dumoulin, V., &amp; Visin, F. (2016). A guide to convolution arithmetic for deep learning. <em>ArXiv:1603.07285</em>.</p>
</dd>
<dt class="label" id="id457"><span class="brackets">Dwivedi &amp; Bresson, 2020</span></dt>
<dd><p>Dwivedi, V. P., &amp; Bresson, X. (2020). A generalization of transformer networks to graphs. <em>ArXiv:2012.09699</em>.</p>
</dd>
<dt class="label" id="id353"><span class="brackets">Dwork et al., 2015</span></dt>
<dd><p>Dwork, C., Feldman, V., Hardt, M., Pitassi, T., Reingold, O., &amp; Roth, A. L. (2015). Preserving statistical validity in adaptive data analysis. <em>Proceedings of the 47th Annual ACM Symposium on Theory of Computing</em> (pp. 117–126).</p>
</dd>
<dt class="label" id="id65"><span class="brackets">Elman, 1990</span></dt>
<dd><p>Elman, J. L. (1990). Finding structure in time. <em>Cognitive Science</em>, <em>14</em>(2), 179–211.</p>
</dd>
<dt class="label" id="id450"><span class="brackets">Elsken et al., 2018</span></dt>
<dd><p>Elsken, T., Metzen, J. H., &amp; Hutter, F. (2018). Neural architecture search: a ssurvey. <em>ArXiv:1808.05377 [stat.ML]</em>.</p>
</dd>
<dt class="label" id="id66"><span class="brackets">Fechner, 1860</span></dt>
<dd><p>Fechner, G. T. (1860). <em>Elemente der Psychophysik</em>. Vol. 2. Breitkopf u. Härtel.</p>
</dd>
<dt class="label" id="id411"><span class="brackets">Fedus et al., 2022</span></dt>
<dd><p>Fedus, W., Zoph, B., &amp; Shazeer, N. (2022). Switch transformers: scaling to trillion parameter models with simple and efficient sparsity. <em>Journal of Machine Learning Research</em>, <em>23</em>(120), 1–39.</p>
</dd>
<dt class="label" id="id67"><span class="brackets">Fernando, 2004</span></dt>
<dd><p>Fernando, R. (2004). <em>GPU Gems: Programming Techniques, Tips, and Tricks for Real-Time Graphics</em>. Addison-Wesley.</p>
</dd>
<dt class="label" id="id451"><span class="brackets">Feurer &amp; Hutter, 2018</span></dt>
<dd><p>Feurer, M., &amp; Hutter, F. (2018). Hyperparameter ptimization. <em>Automatic Machine Learning: Methods, Systems, Challenges</em>. Springer.</p>
</dd>
<dt class="label" id="id432"><span class="brackets">Feurer et al., 2022</span></dt>
<dd><p>Feurer, M., Letham, B., Hutter, F., &amp; Bakshy, E. (2022). Practical transfer learning for Bayesian optimization. <em>ArXiv:1802.02219 [stat.ML]</em>.</p>
</dd>
<dt class="label" id="id68"><span class="brackets">Field, 1987</span></dt>
<dd><p>Field, D. J. (1987). Relations between the statistics of natural images and the response properties of cortical cells. <em>JOSA A</em>, <em>4</em>(12), 2379–2394.</p>
</dd>
<dt class="label" id="id69"><span class="brackets">Fisher, 1925</span></dt>
<dd><p>Fisher, R. A. (1925). <em>Statistical Methods for Research Workers.</em> Oliver &amp; Boyd.</p>
</dd>
<dt class="label" id="id70"><span class="brackets">Flammarion &amp; Bach, 2015</span></dt>
<dd><p>Flammarion, N., &amp; Bach, F. (2015). From averaging to acceleration, there is only a step-size. <em>Conference on Learning Theory</em> (pp. 658–695).</p>
</dd>
<dt class="label" id="id431"><span class="brackets">Forrester et al., 2007</span></dt>
<dd><p>Forrester, A. I., Sóbester, A., &amp; Keane, A. J. (2007). Multi-fidelity optimization via surrogate modelling. <em>Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences</em>, <em>463</em>(2088), 3251–3269.</p>
</dd>
<dt class="label" id="id446"><span class="brackets">Franceschi et al., 2017</span></dt>
<dd><p>Franceschi, L., Donini, M., Frasconi, P., &amp; Pontil, M. (2017). Forward and reverse gradient-based hyperparameter optimization. <em>Proceedings of the 34th International Conference on Machine Learning (ICML'17)</em>.</p>
</dd>
<dt class="label" id="id71"><span class="brackets">Frankle &amp; Carbin, 2018</span></dt>
<dd><p>Frankle, J., &amp; Carbin, M. (2018). The lottery ticket hypothesis: finding sparse, trainable neural networks. <em>ArXiv:1803.03635</em>.</p>
</dd>
<dt class="label" id="id72"><span class="brackets">Frazier, 2018</span></dt>
<dd><p>Frazier, P. I. (2018). A tutorial on Bayesian optimization. <em>ArXiv:1807.02811</em>.</p>
</dd>
<dt class="label" id="id73"><span class="brackets">Freund &amp; Schapire, 1996</span></dt>
<dd><p>Freund, Y., &amp; Schapire, R. E. (1996). Experiments with a new boosting algorithm. <em>Proceedings of the International Conference on Machine Learning</em> (pp. 148–156).</p>
</dd>
<dt class="label" id="id340"><span class="brackets">Friedman, 1987</span></dt>
<dd><p>Friedman, J. H. (1987). Exploratory projection pursuit. <em>Journal of the American Statistical Association</em>, <em>82</em>(397), 249–266.</p>
</dd>
<dt class="label" id="id75"><span class="brackets">Frostig et al., 2018</span></dt>
<dd><p>Frostig, R., Johnson, M. J., &amp; Leary, C. (2018). Compiling machine learning programs via high-level tracing. <em>Proceedings of Systems for Machine Learning</em>.</p>
</dd>
<dt class="label" id="id76"><span class="brackets">Fukushima, 1982</span></dt>
<dd><p>Fukushima, K. (1982). Neocognitron: a self-organizing neural network model for a mechanism of visual pattern recognition. <em>Competition and Cooperation in Neural Nets</em> (pp. 267–285). Springer.</p>
</dd>
<dt class="label" id="id77"><span class="brackets">Gardner et al., 2018</span></dt>
<dd><p>Gardner, J., Pleiss, G., Weinberger, K. Q., Bindel, D., &amp; Wilson, A. G. (2018). GPyTorch: blackbox matrix–matrix Gaussian process inference with GPU acceleration. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id78"><span class="brackets">Garg et al., 2021</span></dt>
<dd><p>Garg, S., Balakrishnan, S., Kolter, Z., &amp; Lipton, Z. (2021). RATT: leveraging unlabeled data to guarantee generalization. <em>International Conference on Machine Learning</em> (pp. 3598–3609).</p>
</dd>
<dt class="label" id="id79"><span class="brackets">Gatys et al., 2016</span></dt>
<dd><p>Gatys, L. A., Ecker, A. S., &amp; Bethge, M. (2016). Image style transfer using convolutional neural networks. <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 2414–2423).</p>
</dd>
<dt class="label" id="id80"><span class="brackets">Gauss, 1809</span></dt>
<dd><p>Gauss, C. F. (1809). Theoria motus corporum coelestum. <em>Werke</em>. Königlich Preussische Akademie der Wissenschaften.</p>
</dd>
<dt class="label" id="id81"><span class="brackets">Gibbs, 1902</span></dt>
<dd><p>Gibbs, J. W. (1902). <em>Elementary Principles of Statistical Mhanics</em>. Scribner's.</p>
</dd>
<dt class="label" id="id82"><span class="brackets">Ginibre, 1965</span></dt>
<dd><p>Ginibre, J. (1965). Statistical ensembles of complex, quaternion, and real matrices. <em>Journal of Mathematical Physics</em>, <em>6</em>(3), 440–449.</p>
</dd>
<dt class="label" id="id83"><span class="brackets">Girshick, 2015</span></dt>
<dd><p>Girshick, R. (2015). Fast R-CNN. <em>Proceedings of the IEEE International Conference on Computer Vision</em> (pp. 1440–1448).</p>
</dd>
<dt class="label" id="id84"><span class="brackets">Girshick et al., 2014</span></dt>
<dd><p>Girshick, R., Donahue, J., Darrell, T., &amp; Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 580–587).</p>
</dd>
<dt class="label" id="id86"><span class="brackets">Glorot &amp; Bengio, 2010</span></dt>
<dd><p>Glorot, X., &amp; Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. <em>Proceedings of the 13th International Conference on Artificial Intelligence and Statistics</em> (pp. 249–256).</p>
</dd>
<dt class="label" id="id87"><span class="brackets">Goh, 2017</span></dt>
<dd><p>Goh, G. (2017). Why momentum really works. <em>Distill</em>. URL: <a class="reference external" href="http://distill.pub/2017/momentum">http://distill.pub/2017/momentum</a></p>
</dd>
<dt class="label" id="id88"><span class="brackets">Goldberg et al., 1992</span></dt>
<dd><p>Goldberg, D., Nichols, D., Oki, B. M., &amp; Terry, D. (1992). Using collaborative filtering to weave an information tapestry. <em>Communications of the ACM</em>, <em>35</em>(12), 61–71.</p>
</dd>
<dt class="label" id="id89"><span class="brackets">Golub &amp; VanLoan, 1996</span></dt>
<dd><p>Golub, G. H., &amp; Van Loan, C. F. (1996). <em>Matrix Computations</em>. Johns Hopkins University Press.</p>
</dd>
<dt class="label" id="id90"><span class="brackets">Goodfellow et al., 2016</span></dt>
<dd><p>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <em>Deep Learning</em>. MIT Press. <a class="reference external" href="http://www.deeplearningbook.org">http://www.deeplearningbook.org</a>.</p>
</dd>
<dt class="label" id="id91"><span class="brackets">Goodfellow et al., 2014</span></dt>
<dd><p>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … Bengio, Y. (2014). Generative adversarial nets. <em>Advances in Neural Information Processing Systems</em> (pp. 2672–2680).</p>
</dd>
<dt class="label" id="id92"><span class="brackets">Gotmare et al., 2018</span></dt>
<dd><p>Gotmare, A., Keskar, N. S., Xiong, C., &amp; Socher, R. (2018). A closer look at deep learning heuristics: learning rate restarts, warmup and distillation. <em>ArXiv:1810.13243</em>.</p>
</dd>
<dt class="label" id="id93"><span class="brackets">Goyal et al., 2021</span></dt>
<dd><p>Goyal, A., Bochkovskiy, A., Deng, J., &amp; Koltun, V. (2021). Non-deep networks. <em>ArXiv:2110.07641</em>.</p>
</dd>
<dt class="label" id="id94"><span class="brackets">Graham, 2014</span></dt>
<dd><p>Graham, B. (2014). Fractional max-pooling. <em>ArXiv:1412.6071</em>.</p>
</dd>
<dt class="label" id="id95"><span class="brackets">Graves, 2013</span></dt>
<dd><p>Graves, A. (2013). Generating sequences with recurrent neural networks. <em>ArXiv:1308.0850</em>.</p>
</dd>
<dt class="label" id="id97"><span class="brackets">Graves et al., 2008</span></dt>
<dd><p>Graves, A., Liwicki, M., Fernández, S., Bertolami, R., Bunke, H., &amp; Schmidhuber, J. (2008). A novel connectionist system for unconstrained handwriting recognition. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <em>31</em>(5), 855–868.</p>
</dd>
<dt class="label" id="id96"><span class="brackets">Graves &amp; Schmidhuber, 2005</span></dt>
<dd><p>Graves, A., &amp; Schmidhuber, J. (2005). Framewise phoneme classification with bidirectional LSTM and other neural network architectures. <em>Neural Networks</em>, <em>18</em>(5-6), 602–610.</p>
</dd>
<dt class="label" id="id98"><span class="brackets">Griewank, 1989</span></dt>
<dd><p>Griewank, A. (1989). On automatic differentiation. <em>Mathematical Programming: Recent Developments and Applications</em> (pp. 83–107). Kluwer.</p>
</dd>
<dt class="label" id="id455"><span class="brackets">Gulati et al., 2020</span></dt>
<dd><p>Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., … et al. (2020). Conformer: convolution-augmented transformer for speech recognition. <em>Proc. Interspeech 2020</em>, pp. 5036–5040.</p>
</dd>
<dt class="label" id="id99"><span class="brackets">Gunawardana &amp; Shani, 2015</span></dt>
<dd><p>Gunawardana, A., &amp; Shani, G. (2015). Evaluating recommender systems. <em>Recommender Systems Handbook</em> (pp. 265–308). Springer.</p>
</dd>
<dt class="label" id="id100"><span class="brackets">Guo et al., 2017</span></dt>
<dd><p>Guo, H., Tang, R., Ye, Y., Li, Z., &amp; He, X. (2017). Deepfm: a factorization-machine based neural network for ctr prediction. <em>Proceedings of the 26th International Joint Conference on Artificial Intelligence</em> (pp. 1725–1731).</p>
</dd>
<dt class="label" id="id5"><span class="brackets">Guyon et al., 2008</span></dt>
<dd><p>Guyon, I., Gunn, S., Nikravesh, M., &amp; Zadeh, L. A. (2008). <em>Feature Extraction: Foundations and Applications</em>. Springer.</p>
</dd>
<dt class="label" id="id101"><span class="brackets">Hadjis et al., 2016</span></dt>
<dd><p>Hadjis, S., Zhang, C., Mitliagkas, I., Iter, D., &amp; Ré, C. (2016). Omnivore: an optimizer for multi-device deep learning on CPUs and GPUs. <em>ArXiv:1606.04487</em>.</p>
</dd>
<dt class="label" id="id102"><span class="brackets">Hartley &amp; Zisserman, 2000</span></dt>
<dd><p>Hartley, R., &amp; Zisserman, A. (2000). <em>Multiple View Geometry in Computer Vision</em>. Cambridge University Press.</p>
</dd>
<dt class="label" id="id418"><span class="brackets">Hartley &amp; Kahl, 2009</span></dt>
<dd><p>Hartley, R. I., &amp; Kahl, F. (2009). Global optimization through rotation space search. <em>International Journal of Computer Vision</em>, <em>82</em>(1), 64–79.</p>
</dd>
<dt class="label" id="id391"><span class="brackets">He et al., 2022</span></dt>
<dd><p>He, K., Chen, X., Xie, S., Li, Y., Dollár, P., &amp; Girshick, R. (2022). Masked autoencoders are scalable vision learners. <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 16000–16009).</p>
</dd>
<dt class="label" id="id105"><span class="brackets">He et al., 2017a</span></dt>
<dd><p>He, K., Gkioxari, G., Dollár, P., &amp; Girshick, R. (2017). Mask R-CNN. <em>Proceedings of the IEEE International Conference on Computer Vision</em> (pp. 2961–2969).</p>
</dd>
<dt class="label" id="id107"><span class="brackets">He et al., 2015</span></dt>
<dd><p>He, K., Zhang, X., Ren, S., &amp; Sun, J. (2015). Delving deep into rectifiers: surpassing human-level performance on ImageNet classification. <em>Proceedings of the IEEE International Conference on Computer Vision</em> (pp. 1026–1034).</p>
</dd>
<dt class="label" id="id108"><span class="brackets">He et al., 2016a</span></dt>
<dd><p>He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Deep residual learning for image recognition. <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 770–778).</p>
</dd>
<dt class="label" id="id109"><span class="brackets">He et al., 2016b</span></dt>
<dd><p>He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Identity mappings in deep residual networks. <em>European Conference on Computer Vision</em> (pp. 630–645).</p>
</dd>
<dt class="label" id="id104"><span class="brackets">He &amp; Chua, 2017</span></dt>
<dd><p>He, X., &amp; Chua, T.-S. (2017). Neural factorization machines for sparse predictive analytics. <em>Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</em> (pp. 355–364).</p>
</dd>
<dt class="label" id="id106"><span class="brackets">He et al., 2017b</span></dt>
<dd><p>He, X., Liao, L., Zhang, H., Nie, L., Hu, X., &amp; Chua, T.-S. (2017). Neural collaborative filtering. <em>Proceedings of the 26th International Conference on World Wide Web</em> (pp. 173–182).</p>
</dd>
<dt class="label" id="id110"><span class="brackets">Hebb, 1949</span></dt>
<dd><p>Hebb, D. O. (1949). <em>The Organization of Behavior</em>. Wiley.</p>
</dd>
<dt class="label" id="id111"><span class="brackets">Hendrycks &amp; Gimpel, 2016</span></dt>
<dd><p>Hendrycks, D., &amp; Gimpel, K. (2016). Gaussian error linear units (GELUs). <em>ArXiv:1606.08415</em>.</p>
</dd>
<dt class="label" id="id113"><span class="brackets">Hennessy &amp; Patterson, 2011</span></dt>
<dd><p>Hennessy, J. L., &amp; Patterson, D. A. (2011). <em>Computer Architecture: A Quantitative Approach</em>. Elsevier.</p>
</dd>
<dt class="label" id="id114"><span class="brackets">Herlocker et al., 1999</span></dt>
<dd><p>Herlocker, J. L., Konstan, J. A., Borchers, A., &amp; Riedl, J. (1999). An algorithmic framework for performing collaborative filtering. <em>22nd Annual International ACM Conference on Research and Development in Information Retrieval, SIGIR 1999</em> (pp. 230–237).</p>
</dd>
<dt class="label" id="id115"><span class="brackets">Hidasi et al., 2015</span></dt>
<dd><p>Hidasi, B., Karatzoglou, A., Baltrunas, L., &amp; Tikk, D. (2015). Session-based recommendations with recurrent neural networks. <em>ArXiv:1511.06939</em>.</p>
</dd>
<dt class="label" id="id461"><span class="brackets">Ho et al., 2020</span></dt>
<dd><p>Ho, J., Jain, A., &amp; Abbeel, P. (2020). Denoising diffusion probabilistic models. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 6840–6851.</p>
</dd>
<dt class="label" id="id116"><span class="brackets">Hochreiter et al., 2001</span></dt>
<dd><p>Hochreiter, S., Bengio, Y., Frasconi, P., &amp; Schmidhuber, J. (2001). Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. <em>A Field Guide to Dynamical Recurrent Neural Networks</em>. IEEE Press.</p>
</dd>
<dt class="label" id="id117"><span class="brackets">Hochreiter &amp; Schmidhuber, 1997</span></dt>
<dd><p>Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. <em>Neural Computation</em>, <em>9</em>(8), 1735–1780.</p>
</dd>
<dt class="label" id="id399"><span class="brackets">Hoffmann et al., 2022</span></dt>
<dd><p>Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., … et al. (2022). Training compute-optimal large language models. <em>ArXiv:2203.15556</em>.</p>
</dd>
<dt class="label" id="id118"><span class="brackets">Howard et al., 2019</span></dt>
<dd><p>Howard, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B., Tan, M., … Adam, H. (2019). Searching for MobileNetV3. <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> (pp. 1314–1324).</p>
</dd>
<dt class="label" id="id119"><span class="brackets">Hoyer et al., 2009</span></dt>
<dd><p>Hoyer, P. O., Janzing, D., Mooij, J. M., Peters, J., &amp; Schölkopf, B. (2009). Nonlinear causal discovery with additive noise models. <em>Advances in Neural Information Processing Systems</em> (pp. 689–696).</p>
</dd>
<dt class="label" id="id126"><span class="brackets">Hu et al., 2018</span></dt>
<dd><p>Hu, J., Shen, L., &amp; Sun, G. (2018). Squeeze-and-excitation networks. <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 7132–7141).</p>
</dd>
<dt class="label" id="id120"><span class="brackets">Hu et al., 2008</span></dt>
<dd><p>Hu, Y., Koren, Y., &amp; Volinsky, C. (2008). Collaborative filtering for implicit feedback datasets. <em>2008 8th IEEE International Conference on Data Mining</em> (pp. 263–272).</p>
</dd>
<dt class="label" id="id121"><span class="brackets">Hu et al., 2022</span></dt>
<dd><p>Hu, Z., Lee, R. K.-W., Aggarwal, C. C., &amp; Zhang, A. (2022). Text style transfer: a review and experimental evaluation. <em>SIGKDD Explor. Newsl.</em>, <em>24</em>(1). URL: <a class="reference external" href="https://doi.org/10.1145/3544903.3544906">https://doi.org/10.1145/3544903.3544906</a></p>
</dd>
<dt class="label" id="id367"><span class="brackets">Huang et al., 2018</span></dt>
<dd><p>Huang, C.-Z. A., Vaswani, A., Uszkoreit, J., Simon, I., Hawthorne, C., Shazeer, N., … Eck, D. (2018). Music transformer: generating music with long-term structure. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id127"><span class="brackets">Huang et al., 2017</span></dt>
<dd><p>Huang, G., Liu, Z., Van Der Maaten, L., &amp; Weinberger, K. Q. (2017). Densely connected convolutional networks. <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 4700–4708).</p>
</dd>
<dt class="label" id="id128"><span class="brackets">Huang et al., 2015</span></dt>
<dd><p>Huang, Z., Xu, W., &amp; Yu, K. (2015). Bidirectional LSTM–CRF models for sequence tagging. <em>ArXiv:1508.01991</em>.</p>
</dd>
<dt class="label" id="id129"><span class="brackets">Hubel &amp; Wiesel, 1959</span></dt>
<dd><p>Hubel, D. H., &amp; Wiesel, T. N. (1959). Receptive fields of single neurones in the cat's striate cortex. <em>Journal of Physiology</em>, <em>148</em>(3), 574–591.</p>
</dd>
<dt class="label" id="id130"><span class="brackets">Hubel &amp; Wiesel, 1962</span></dt>
<dd><p>Hubel, D. H., &amp; Wiesel, T. N. (1962). Receptive fields, binocular interaction and functional architecture in the cat's visual cortex. <em>Journal of Physiology</em>, <em>160</em>(1), 106–154.</p>
</dd>
<dt class="label" id="id131"><span class="brackets">Hubel &amp; Wiesel, 1968</span></dt>
<dd><p>Hubel, D. H., &amp; Wiesel, T. N. (1968). Receptive fields and functional architecture of monkey striate cortex. <em>Journal of Physiology</em>, <em>195</em>(1), 215–243.</p>
</dd>
<dt class="label" id="id445"><span class="brackets">Hutter et al., 2011</span></dt>
<dd><p>Hutter, F., Hoos, H., &amp; Leyton-Brown, K. (2011). Sequential model-based optimization for general algorithm configuration. <em>Proceedings of the Fifth International Conference on Learning and Intelligent Optimization (LION'11)</em>.</p>
</dd>
<dt class="label" id="id448"><span class="brackets">Hutter et al., 2019</span></dt>
<dd><p>Hutter, F., Kotthoff, L., &amp; Vanschoren, J. (Eds.) (2019). <em>Automated Machine Learning: Methods, Systems, Challenges</em>. Springer.</p>
</dd>
<dt class="label" id="id132"><span class="brackets">Ioffe, 2017</span></dt>
<dd><p>Ioffe, S. (2017). Batch renormalization: towards reducing minibatch dependence in batch-normalized models. <em>Advances in Neural Information Processing Systems</em> (pp. 1945–1953).</p>
</dd>
<dt class="label" id="id133"><span class="brackets">Ioffe &amp; Szegedy, 2015</span></dt>
<dd><p>Ioffe, S., &amp; Szegedy, C. (2015). Batch normalization: accelerating deep network training by reducing internal covariate shift. <em>ArXiv:1502.03167</em>.</p>
</dd>
<dt class="label" id="id134"><span class="brackets">Izmailov et al., 2018</span></dt>
<dd><p>Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., &amp; Wilson, A. G. (2018). Averaging weights leads to wider optima and better generalization. <em>ArXiv:1803.05407</em>.</p>
</dd>
<dt class="label" id="id135"><span class="brackets">Jacot et al., 2018</span></dt>
<dd><p>Jacot, A., Gabriel, F., &amp; Hongler, C. (2018). Neural tangent kernel: convergence and generalization in neural networks. <em>Advances in Neural Information Processing Systems</em>.</p>
</dd>
<dt class="label" id="id136"><span class="brackets">Jaeger, 2002</span></dt>
<dd><p>Jaeger, H. (2002). <em>Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the “echo state network” approach</em>. GMD-Forschungszentrum Informationstechnik Bonn.</p>
</dd>
<dt class="label" id="id440"><span class="brackets">Jamieson &amp; Talwalkar, 2016</span></dt>
<dd><p>Jamieson, K., &amp; Talwalkar, A. (2016). Non-stochastic best arm identification and hyperparameter optimization. <em>Proceedings of the 17th International Conference on Artificial Intelligence and Statistics</em>.</p>
</dd>
<dt class="label" id="id435"><span class="brackets">Jenatton et al., 2017</span></dt>
<dd><p>Jenatton, R., Archambeau, C., González, J., &amp; Seeger, M. (2017). Bayesian optimization with tree-structured dependencies. <em>Proceedings of the 34th International Conference on Machine Learning (ICML'17)</em>.</p>
</dd>
<dt class="label" id="id139"><span class="brackets">Jia et al., 2018</span></dt>
<dd><p>Jia, X., Song, S., He, W., Wang, Y., Rong, H., Zhou, F., … et al. (2018). Highly scalable deep learning training system with mixed-precision: training ImageNet in four minutes. <em>ArXiv:1807.11205</em>.</p>
</dd>
<dt class="label" id="id138"><span class="brackets">Jia et al., 2014</span></dt>
<dd><p>Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., … Darrell, T. (2014). Caffe: convolutional architecture for fast feature embedding. <em>Proceedings of the 22nd ACM International Conference on Multimedia</em> (pp. 675–678).</p>
</dd>
<dt class="label" id="id413"><span class="brackets">Joshi et al., 2020</span></dt>
<dd><p>Joshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, L., &amp; Levy, O. (2020). SpanBERT: improving pre-training by representing and predicting spans. <em>Transactions of the Association for Computational Linguistics</em>, <em>8</em>, 64–77.</p>
</dd>
<dt class="label" id="id140"><span class="brackets">Jouppi et al., 2017</span></dt>
<dd><p>Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., … et al. (2017). In-datacenter performance analysis of a tensor processing unit. <em>2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)</em> (pp. 1–12).</p>
</dd>
<dt class="label" id="id141"><span class="brackets">Kalchbrenner et al., 2014</span></dt>
<dd><p>Kalchbrenner, N., Grefenstette, E., &amp; Blunsom, P. (2014). A convolutional neural network for modelling sentences. <em>ArXiv:1404.2188</em>.</p>
</dd>
<dt class="label" id="id142"><span class="brackets">Kalman &amp; Kwasny, 1992</span></dt>
<dd><p>Kalman, B. L., &amp; Kwasny, S. C. (1992). Why tanh: choosing a sigmoidal function. <em>Proceedings of the International Joint Conference on Neural Networks (IJCNN)</em> (pp. 578–581).</p>
</dd>
<dt class="label" id="id393"><span class="brackets">Kaplan et al., 2020</span></dt>
<dd><p>Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., … Amodei, D. (2020). Scaling laws for neural language models. <em>ArXiv:2001.08361</em>.</p>
</dd>
<dt class="label" id="id439"><span class="brackets">Karnin et al., 2013</span></dt>
<dd><p>Karnin, Z., Koren, T., &amp; Somekh, O. (2013). Almost optimal exploration in multi-armed bandits. <em>Proceedings of the 30th International Conference on Machine Learning (ICML'13)</em>.</p>
</dd>
<dt class="label" id="id143"><span class="brackets">Karras et al., 2017</span></dt>
<dd><p>Karras, T., Aila, T., Laine, S., &amp; Lehtinen, J. (2017). Progressive growing of GANs for improved quality, stability, and variation. <em>ArXiv:1710.10196</em>.</p>
</dd>
<dt class="label" id="id429"><span class="brackets">Kim et al., 2017</span></dt>
<dd><p>Kim, J., El-Khamy, M., &amp; Lee, J. (2017). Residual LSTM: design of a deep recurrent architecture for distant speech recognition. <em>ArXiv:1701.03360</em>.</p>
</dd>
<dt class="label" id="id145"><span class="brackets">Kim, 2014</span></dt>
<dd><p>Kim, Y. (2014). Convolutional neural networks for sentence classification. <em>ArXiv:1408.5882</em>.</p>
</dd>
<dt class="label" id="id146"><span class="brackets">Kimeldorf &amp; Wahba, 1971</span></dt>
<dd><p>Kimeldorf, G. S., &amp; Wahba, G. (1971). Some results on Tchebycheffian spline functions. <em>J. Math. Anal. Appl.</em>, <em>33</em>, 82–95.</p>
</dd>
<dt class="label" id="id147"><span class="brackets">Kingma &amp; Ba, 2014</span></dt>
<dd><p>Kingma, D. P., &amp; Ba, J. (2014). Adam: a method for stochastic optimization. <em>ArXiv:1412.6980</em>.</p>
</dd>
<dt class="label" id="id148"><span class="brackets">Kingma &amp; Welling, 2014</span></dt>
<dd><p>Kingma, D. P., &amp; Welling, M. (2014). Auto-encoding variational Bayes. <em>International Conference on Learning Representations (ICLR)</em>.</p>
</dd>
<dt class="label" id="id149"><span class="brackets">Kipf &amp; Welling, 2016</span></dt>
<dd><p>Kipf, T. N., &amp; Welling, M. (2016). Semi-supervised classification with graph convolutional networks. <em>ArXiv:1609.02907</em>.</p>
</dd>
<dt class="label" id="id472"><span class="brackets">Kojima et al., 2022</span></dt>
<dd><p>Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., &amp; Iwasawa, Y. (2022). Large language models are zero-shot reasoners. <em>arxiv.org/abs/2205.11916</em>.</p>
</dd>
<dt class="label" id="id150"><span class="brackets">Koller &amp; Friedman, 2009</span></dt>
<dd><p>Koller, D., &amp; Friedman, N. (2009). <em>Probabilistic Graphical Models: Principles and Techniques</em>. MIT Press.</p>
</dd>
<dt class="label" id="id151"><span class="brackets">Kolmogorov, 1933</span></dt>
<dd><p>Kolmogorov, A. (1933). Sulla determinazione empirica di una legge di distribuzione. <em>Inst. Ital. Attuari, Giorn.</em>, <em>4</em>, 83–91.</p>
</dd>
<dt class="label" id="id152"><span class="brackets">Kolter, 2008</span></dt>
<dd><p>Kolter, Z. (2008). Linear algebra review and reference. <em>Available online: http://cs229.stanford.edu/section/cs229-linalg.pdf</em>.</p>
</dd>
<dt class="label" id="id154"><span class="brackets">Koren et al., 2009</span></dt>
<dd><p>Koren, Y., Bell, R., &amp; Volinsky, C. (2009). Matrix factorization techniques for recommender systems. <em>Computer</em>, pp. 30–37.</p>
</dd>
<dt class="label" id="id155"><span class="brackets">Krizhevsky et al., 2012</span></dt>
<dd><p>Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. <em>Advances in Neural Information Processing Systems</em> (pp. 1097–1105).</p>
</dd>
<dt class="label" id="id157"><span class="brackets">Kung, 1988</span></dt>
<dd><p>Kung, S. Y. (1988). VLSI Array Processors. <em>Prentice Hall</em>.</p>
</dd>
<dt class="label" id="id158"><span class="brackets">Kuzovkin et al., 2018</span></dt>
<dd><p>Kuzovkin, I., Vicente, R., Petton, M., Lachaux, J.-P., Baciu, M., Kahane, P., … Aru, J. (2018). Activations of deep convolutional neural networks are aligned with gamma band activity of human visual cortex. <em>Communications Biology</em>, <em>1</em>(1), 1–12.</p>
</dd>
<dt class="label" id="id389"><span class="brackets">Lan et al., 2019</span></dt>
<dd><p>Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., &amp; Soricut, R. (2019). ALBERT: a lite BERT for self-supervised learning of language representations. <em>ArXiv:1909.11942</em>.</p>
</dd>
<dt class="label" id="id358"><span class="brackets">Lavin &amp; Gray, 2016</span></dt>
<dd><p>Lavin, A., &amp; Gray, S. (2016). Fast algorithms for convolutional neural networks. <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 4013–4021).</p>
</dd>
<dt class="label" id="id422"><span class="brackets">Le, 2013</span></dt>
<dd><p>Le, Q. V. (2013). Building high-level features using large scale unsupervised learning. <em>Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</em> (pp. 8595–8598).</p>
</dd>
<dt class="label" id="id159"><span class="brackets">LeCun et al., 1995a</span></dt>
<dd><p>LeCun, Y., Bengio, Y., &amp; et al. (1995). Convolutional networks for images, speech, and time series. <em>The Handbook of Brain Theory and Neural Networks</em> (p. 3361). MIT Press.</p>
</dd>
<dt class="label" id="id160"><span class="brackets">LeCun et al., 1989</span></dt>
<dd><p>LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., &amp; Jackel, L. D. (1989). Backpropagation applied to handwritten zip code recognition. <em>Neural Computation</em>, <em>1</em>(4), 541–551.</p>
</dd>
<dt class="label" id="id162"><span class="brackets">LeCun et al., 1998a</span></dt>
<dd><p>LeCun, Y., Bottou, L., Orr, G., &amp; Muller, K.-R. (1998). Efficient backprop. <em>Neural Networks: Tricks of the Trade</em>. Springer.</p>
</dd>
<dt class="label" id="id161"><span class="brackets">LeCun et al., 1998b</span></dt>
<dd><p>LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient-based learning applied to document recognition. <em>Proceedings of the IEEE</em>, <em>86</em>(11), 2278–2324.</p>
</dd>
<dt class="label" id="id163"><span class="brackets">LeCun et al., 1995b</span></dt>
<dd><p>LeCun, Y., Jackel, L., Bottou, L., Brunot, A., Cortes, C., Denker, J., … et al. (1995). Comparison of learning algorithms for handwritten digit recognition. <em>International Conference on Artificial Neural Networks</em> (pp. 53–60).</p>
</dd>
<dt class="label" id="id164"><span class="brackets">Legendre, 1805</span></dt>
<dd><p>Legendre, A. M. (1805). <em>Mémoire sur les Opérations Trigonométriques: dont les Résultats Dépendent de la Figure de la Terre</em>. F. Didot.</p>
</dd>
<dt class="label" id="id386"><span class="brackets">Lewis et al., 2019</span></dt>
<dd><p>Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., … Zettlemoyer, L. (2019). BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. <em>ArXiv:1910.13461</em>.</p>
</dd>
<dt class="label" id="id416"><span class="brackets">Lewkowycz et al., 2022</span></dt>
<dd><p>Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., … et al. (2022). Solving quantitative reasoning problems with language models. <em>ArXiv:2206.14858</em>.</p>
</dd>
<dt class="label" id="id436"><span class="brackets">Li et al., 2018</span></dt>
<dd><p>Li, L., Jamieson, K., Rostamizadeh, A., Gonina, K., Hardt, M., Recht, B., &amp; Talwalkar, A. (2018). Massively parallel hyperparameter tuning. <em>ArXiv:1810.05934</em>.</p>
</dd>
<dt class="label" id="id165"><span class="brackets">Li, 2017</span></dt>
<dd><p>Li, M. (2017). <em>Scaling Distributed Machine Learning with System and Algorithm Co-design</em> (Doctoral dissertation). PhD Thesis, CMU.</p>
</dd>
<dt class="label" id="id166"><span class="brackets">Li et al., 2014a</span></dt>
<dd><p>Li, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed, A., Josifovski, V., … Su, B.-Y. (2014). Scaling distributed machine learning with the parameter server. <em>11th Symposium on Operating Systems Design and Implementation (OSDI 14)</em> (pp. 583–598).</p>
</dd>
<dt class="label" id="id167"><span class="brackets">Li et al., 2014b</span></dt>
<dd><p>Li, M., Zhang, T., Chen, Y., &amp; Smola, A. J. (2014). Efficient mini-batch training for stochastic optimization. <em>Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (pp. 661–670).</p>
</dd>
<dt class="label" id="id442"><span class="brackets">Liaw et al., 2018</span></dt>
<dd><p>Liaw, R., Liang, E., Nishihara, R., Moritz, P., Gonzalez, J., &amp; Stoica, I. (2018). Tune: a research platform for distributed model selection and training. <em>ArXiv:1807.05118</em>.</p>
</dd>
<dt class="label" id="id168"><span class="brackets">Lin et al., 2013</span></dt>
<dd><p>Lin, M., Chen, Q., &amp; Yan, S. (2013). Network in network. <em>ArXiv:1312.4400</em>.</p>
</dd>
<dt class="label" id="id170"><span class="brackets">Lin et al., 2017a</span></dt>
<dd><p>Lin, T.-Y., Goyal, P., Girshick, R., He, K., &amp; Dollár, P. (2017). Focal loss for dense object detection. <em>Proceedings of the IEEE International Conference on Computer Vision</em> (pp. 2980–2988).</p>
</dd>
<dt class="label" id="id171"><span class="brackets">Lin et al., 2010</span></dt>
<dd><p>Lin, Y., Lv, F., Zhu, S., Yang, M., Cour, T., Yu, K., … others. (2010). ImageNet classification: fast descriptor coding and large-scale SVM training. <em>Large Scale Visual Recognition Challenge</em>.</p>
</dd>
<dt class="label" id="id169"><span class="brackets">Lin et al., 2017b</span></dt>
<dd><p>Lin, Z., Feng, M., Santos, C. N. d., Yu, M., Xiang, B., Zhou, B., &amp; Bengio, Y. (2017). A structured self-attentive sentence embedding. <em>ArXiv:1703.03130</em>.</p>
</dd>
<dt class="label" id="id174"><span class="brackets">Lipton et al., 2015</span></dt>
<dd><p>Lipton, Z. C., Berkowitz, J., &amp; Elkan, C. (2015). A critical review of recurrent neural networks for sequence learning. <em>ArXiv:1506.00019</em>.</p>
</dd>
<dt class="label" id="id172"><span class="brackets">Lipton et al., 2016</span></dt>
<dd><p>Lipton, Z. C., Kale, D. C., Elkan, C., &amp; Wetzel, R. (2016). Learning to diagnose with LSTM recurrent neural networks. <em>International Conference on Learning Representations (ICLR)</em>.</p>
</dd>
<dt class="label" id="id173"><span class="brackets">Lipton &amp; Steinhardt, 2018</span></dt>
<dd><p>Lipton, Z. C., &amp; Steinhardt, J. (2018). Troubling trends in machine learning scholarship. <em>Communications of the ACM</em>, <em>17</em>, 45–77.</p>
</dd>
<dt class="label" id="id176"><span class="brackets">Liu &amp; Nocedal, 1989</span></dt>
<dd><p>Liu, D. C., &amp; Nocedal, J. (1989). On the limited memory BFGS method for large scale optimization. <em>Mathematical Programming</em>, <em>45</em>(1), 503–528.</p>
</dd>
<dt class="label" id="id361"><span class="brackets">Liu et al., 2018</span></dt>
<dd><p>Liu, H., Simonyan, K., &amp; Yang, Y. (2018). DARTS: differentiable architecture search. <em>ArXiv:1806.09055</em>.</p>
</dd>
<dt class="label" id="id175"><span class="brackets">Liu et al., 2016</span></dt>
<dd><p>Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., &amp; Berg, A. C. (2016). SSD: single shot multibox detector. <em>European Conference on Computer Vision</em> (pp. 21–37).</p>
</dd>
<dt class="label" id="id177"><span class="brackets">Liu et al., 2019</span></dt>
<dd><p>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., … Stoyanov, V. (2019). RoBERTa: a robustly optimized BERT pretraining approach. <em>ArXiv:1907.11692</em>.</p>
</dd>
<dt class="label" id="id365"><span class="brackets">Liu et al., 2021</span></dt>
<dd><p>Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., … Guo, B. (2021). Swin transformer: hierarchical vision transformer using shifted windows. <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> (pp. 10012–10022).</p>
</dd>
<dt class="label" id="id362"><span class="brackets">Liu et al., 2022</span></dt>
<dd><p>Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., &amp; Xie, S. (2022). A convNet for the 2020s. <em>ArXiv:2201.03545</em>.</p>
</dd>
<dt class="label" id="id178"><span class="brackets">Long et al., 2015</span></dt>
<dd><p>Long, J., Shelhamer, E., &amp; Darrell, T. (2015). Fully convolutional networks for semantic segmentation. <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 3431–3440).</p>
</dd>
<dt class="label" id="id179"><span class="brackets">Loshchilov &amp; Hutter, 2016</span></dt>
<dd><p>Loshchilov, I., &amp; Hutter, F. (2016). SGDR: stochastic gradient descent with warm restarts. <em>ArXiv:1608.03983</em>.</p>
</dd>
<dt class="label" id="id180"><span class="brackets">Lowe, 2004</span></dt>
<dd><p>Lowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. <em>International Journal of Computer Vision</em>, <em>60</em>(2), 91–110.</p>
</dd>
<dt class="label" id="id181"><span class="brackets">Luo et al., 2018</span></dt>
<dd><p>Luo, P., Wang, X., Shao, W., &amp; Peng, Z. (2018). Towards understanding regularization in batch normalization. <em>ArXiv:1809.00846</em>.</p>
</dd>
<dt class="label" id="id182"><span class="brackets">Maas et al., 2011</span></dt>
<dd><p>Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., &amp; Potts, C. (2011). Learning word vectors for sentiment analysis. <em>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Volume 1</em> (pp. 142–150).</p>
</dd>
<dt class="label" id="id123"><span class="brackets">Mack &amp; Silverman, 1982</span></dt>
<dd><p>Mack, Y.-P., &amp; Silverman, B. W. (1982). Weak and strong uniform consistency of kernel regression estimates. <em>Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete</em>, <em>61</em>(3), 405–415.</p>
</dd>
<dt class="label" id="id356"><span class="brackets">MacKay, 2003</span></dt>
<dd><p>MacKay, D. J. (2003). <em>Information Theory, Inference and Learning Algorithms</em>. Cambridge University Press.</p>
</dd>
<dt class="label" id="id447"><span class="brackets">Maclaurin et al., 2015</span></dt>
<dd><p>Maclaurin, D., Duvenaud, D., &amp; Adams, R. (2015). Gradient-based hyperparameter optimization through reversible learning. <em>Proceedings of the 32nd International Conference on Machine Learning (ICML'15)</em>.</p>
</dd>
<dt class="label" id="id183"><span class="brackets">Mangasarian, 1965</span></dt>
<dd><p>Mangasarian, O. L. (1965). Linear and nonlinear separation of patterns by linear programming. <em>Oper. Res.</em>, <em>13</em>, 444-452.</p>
</dd>
<dt class="label" id="id184"><span class="brackets">Mangram, 2013</span></dt>
<dd><p>Mangram, M. E. (2013). A simplified perspective of the Markowitz portfolio theory. <em>Global Journal of Business Research</em>, <em>7</em>(1), 59–70.</p>
</dd>
<dt class="label" id="id370"><span class="brackets">Matthews et al., 2018</span></dt>
<dd><p>Matthews, A. G. d. G., Rowland, M., Hron, J., Turner, R. E., &amp; Ghahramani, Z. (2018). Gaussian process behaviour in wide deep neural networks. <em>ArXiv:1804.11271</em>.</p>
</dd>
<dt class="label" id="id185"><span class="brackets">McCann et al., 2017</span></dt>
<dd><p>McCann, B., Bradbury, J., Xiong, C., &amp; Socher, R. (2017). Learned in translation: Contextualized word vectors. <em>Advances in Neural Information Processing Systems</em> (pp. 6294–6305).</p>
</dd>
<dt class="label" id="id186"><span class="brackets">McCulloch &amp; Pitts, 1943</span></dt>
<dd><p>McCulloch, W. S., &amp; Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. <em>Bulletin of Mathematical Biophysics</em>, <em>5</em>(4), 115–133.</p>
</dd>
<dt class="label" id="id187"><span class="brackets">McMahan et al., 2013</span></dt>
<dd><p>McMahan, H. B., Holt, G., Sculley, D., Young, M., Ebner, D., Grady, J., … et al. (2013). Ad click prediction: a view from the trenches. <em>Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (pp. 1222–1230).</p>
</dd>
<dt class="label" id="id188"><span class="brackets">Mead, 1980</span></dt>
<dd><p>Mead, C. (1980). Introduction to VLSI systems. <em>IEE Proceedings I-Solid-State and Electron Devices</em>, <em>128</em>(1), 18.</p>
</dd>
<dt class="label" id="id189"><span class="brackets">Merity et al., 2016</span></dt>
<dd><p>Merity, S., Xiong, C., Bradbury, J., &amp; Socher, R. (2016). Pointer sentinel mixture models. <em>ArXiv:1609.07843</em>.</p>
</dd>
<dt class="label" id="id354"><span class="brackets">Micchelli, 1984</span></dt>
<dd><p>Micchelli, C. A. (1984). Interpolation of scattered data: distance matrices and conditionally positive definite functions. <em>Approximation Theory and Spline Functions</em> (pp. 143–145). Springer.</p>
</dd>
<dt class="label" id="id191"><span class="brackets">Mikolov et al., 2013a</span></dt>
<dd><p>Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Efficient estimation of word representations in vector space. <em>ArXiv:1301.3781</em>.</p>
</dd>
<dt class="label" id="id192"><span class="brackets">Mikolov et al., 2013b</span></dt>
<dd><p>Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dean, J. (2013). Distributed representations of words and phrases and their compositionality. <em>Advances in Neural Information Processing Systems</em> (pp. 3111–3119).</p>
</dd>
<dt class="label" id="id193"><span class="brackets">Miller, 1995</span></dt>
<dd><p>Miller, G. A. (1995). WordNet: a lexical database for English. <em>Communications of the ACM</em>, <em>38</em>(11), 39–41.</p>
</dd>
<dt class="label" id="id194"><span class="brackets">Mirhoseini et al., 2017</span></dt>
<dd><p>Mirhoseini, A., Pham, H., Le, Q. V., Steiner, B., Larsen, R., Zhou, Y., … Dean, J. (2017). Device placement optimization with reinforcement learning. <em>Proceedings of the 34th International Conference on Machine Learning</em> (pp. 2430–2439).</p>
</dd>
<dt class="label" id="id195"><span class="brackets">Mnih et al., 2014</span></dt>
<dd><p>Mnih, V., Heess, N., Graves, A., &amp; others. (2014). Recurrent models of visual attention. <em>Advances in Neural Information Processing Systems</em> (pp. 2204–2212).</p>
</dd>
<dt class="label" id="id466"><span class="brackets">Mnih et al., 2013</span></dt>
<dd><p>Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2013). Playing Atari with deep reinforcement learning. <em>ArXiv:1312.5602</em>.</p>
</dd>
<dt class="label" id="id417"><span class="brackets">Mnih et al., 2015</span></dt>
<dd><p>Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., … et al. (2015). Human-level control through deep reinforcement learning. <em>Nature</em>, <em>518</em>(7540), 529–533.</p>
</dd>
<dt class="label" id="id196"><span class="brackets">Moon et al., 2010</span></dt>
<dd><p>Moon, T., Smola, A., Chang, Y., &amp; Zheng, Z. (2010). Intervalrank: isotonic regression with listwise and pairwise constraints. <em>Proceedings of the 3rd ACM International Conference on Web Search and Data Mining</em> (pp. 151–160).</p>
</dd>
<dt class="label" id="id197"><span class="brackets">Morey et al., 2016</span></dt>
<dd><p>Morey, R. D., Hoekstra, R., Rouder, J. N., Lee, M. D., &amp; Wagenmakers, E.-J. (2016). The fallacy of placing confidence in confidence intervals. <em>Psychonomic Bulletin &amp; Review</em>, <em>23</em>(1), 103–123.</p>
</dd>
<dt class="label" id="id375"><span class="brackets">Morozov, 1984</span></dt>
<dd><p>Morozov, V. A. (1984). <em>Methods for Solving Incorrectly Posed Problems</em>. Springer.</p>
</dd>
<dt class="label" id="id199"><span class="brackets">Nadaraya, 1964</span></dt>
<dd><p>Nadaraya, E. A. (1964). On estimating regression. <em>Theory of Probability &amp; its Applications</em>, <em>9</em>(1), 141–142.</p>
</dd>
<dt class="label" id="id201"><span class="brackets">Nair &amp; Hinton, 2010</span></dt>
<dd><p>Nair, V., &amp; Hinton, G. E. (2010). Rectified linear units improve restricted Boltzmann machines. <em>ICML</em>.</p>
</dd>
<dt class="label" id="id373"><span class="brackets">Nakkiran et al., 2021</span></dt>
<dd><p>Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., &amp; Sutskever, I. (2021). Deep double descent: where bigger models and more data hurt. <em>Journal of Statistical Mechanics: Theory and Experiment</em>, <em>2021</em>(12), 124003.</p>
</dd>
<dt class="label" id="id202"><span class="brackets">Naor &amp; Reingold, 1999</span></dt>
<dd><p>Naor, M., &amp; Reingold, O. (1999). On the construction of pseudorandom permutations: Luby–Rackoff revisited. <em>Journal of Cryptology</em>, <em>12</em>(1), 29–66.</p>
</dd>
<dt class="label" id="id369"><span class="brackets">Neal, 1996</span></dt>
<dd><p>Neal, R. M. (1996). <em>Bayesian Learning for Neural Networks</em>. Springer.</p>
</dd>
<dt class="label" id="id203"><span class="brackets">Nesterov, 2018</span></dt>
<dd><p>Nesterov, Y. (2018). <em>Lectures on Convex Optimization</em>. Springer.</p>
</dd>
<dt class="label" id="id204"><span class="brackets">Nesterov &amp; Vial, 2000</span></dt>
<dd><p>Nesterov, Y., &amp; Vial, J.-P. (2000). Confidence level solutions for stochastic programming. <em>Automatica</em>, <em>44</em>(6), 1559–1568.</p>
</dd>
<dt class="label" id="id205"><span class="brackets">Neyman, 1937</span></dt>
<dd><p>Neyman, J. (1937). Outline of a theory of statistical estimation based on the classical theory of probability. <em>Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences</em>, <em>236</em>(767), 333–380.</p>
</dd>
<dt class="label" id="id125"><span class="brackets">Norelli et al., 2022</span></dt>
<dd><p>Norelli, A., Fumero, M., Maiorca, V., Moschella, L., Rodolà, E., &amp; Locatello, F. (2022). ASIF: coupled data turns unimodal models to multimodal without training. <em>ArXiv:2210.01738</em>.</p>
</dd>
<dt class="label" id="id371"><span class="brackets">Novak et al., 2018</span></dt>
<dd><p>Novak, R., Xiao, L., Lee, J., Bahri, Y., Yang, G., Hron, J., … Sohl-Dickstein, J. (2018). Bayesian deep convolutional networks with many channels are Gaussian processes. <em>ArXiv:1810.05148</em>.</p>
</dd>
<dt class="label" id="id425"><span class="brackets">Novikoff, 1962</span></dt>
<dd><p>Novikoff, A. B. J. (1962). On convergence proofs on perceptrons. <em>Proceedings of the Symposium on the Mathematical Theory of Automata</em> (pp. 615–622).</p>
</dd>
<dt class="label" id="id423"><span class="brackets">Olshausen &amp; Field, 1996</span></dt>
<dd><p>Olshausen, B. A., &amp; Field, D. J. (1996). Emergence of simple-cell receptive field properties by learning a sparse code for natural images. <em>Nature</em>, <em>381</em>(6583), 607–609.</p>
</dd>
<dt class="label" id="id341"><span class="brackets">Ong et al., 2005</span></dt>
<dd><p>Ong, C. S., Smola, A., &amp; Williamson, R. (2005). Learning the kernel with hyperkernels. <em>Journal of Machine Learning Research</em>, <em>6</em>, 1043–1071.</p>
</dd>
<dt class="label" id="id479"><span class="brackets">OpenAI, 2023</span></dt>
<dd><p>OpenAI. (2023). GPT-4 Technical Report. <em>ArXiv:2303.08774</em>.</p>
</dd>
<dt class="label" id="id467"><span class="brackets">Ouyang et al., 2022</span></dt>
<dd><p>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., … et al. (2022). Training language models to follow instructions with human feedback. <em>ArXiv:2203.02155</em>.</p>
</dd>
<dt class="label" id="id206"><span class="brackets">Papineni et al., 2002</span></dt>
<dd><p>Papineni, K., Roukos, S., Ward, T., &amp; Zhu, W.-J. (2002). BLEU: a method for automatic evaluation of machine translation. <em>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em> (pp. 311–318).</p>
</dd>
<dt class="label" id="id207"><span class="brackets">Parikh et al., 2016</span></dt>
<dd><p>Parikh, A. P., Täckström, O., Das, D., &amp; Uszkoreit, J. (2016). A decomposable attention model for natural language inference. <em>ArXiv:1606.01933</em>.</p>
</dd>
<dt class="label" id="id208"><span class="brackets">Park et al., 2019</span></dt>
<dd><p>Park, T., Liu, M.-Y., Wang, T.-C., &amp; Zhu, J.-Y. (2019). Semantic image synthesis with spatially-adaptive normalization. <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 2337–2346).</p>
</dd>
<dt class="label" id="id122"><span class="brackets">Parzen, 1957</span></dt>
<dd><p>Parzen, E. (1957). On consistent estimates of the spectrum of a stationary time series. <em>Annals of Mathematical Statistics</em>, <em>28</em>, 329–348.</p>
</dd>
<dt class="label" id="id209"><span class="brackets">Paszke et al., 2019</span></dt>
<dd><p>Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., … et al. (2019). PyTorch: an imperative style, high-performance deep learning library. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>, 8026–8037.</p>
</dd>
<dt class="label" id="id210"><span class="brackets">Paulus et al., 2017</span></dt>
<dd><p>Paulus, R., Xiong, C., &amp; Socher, R. (2017). A deep reinforced model for abstractive summarization. <em>ArXiv:1705.04304</em>.</p>
</dd>
<dt class="label" id="id489"><span class="brackets">Penedo et al., 2023</span></dt>
<dd><p>Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Alobeidli, H., … Launay, J. (2023). The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. <em>ArXiv:2306.01116</em>.</p>
</dd>
<dt class="label" id="id211"><span class="brackets">Pennington et al., 2017</span></dt>
<dd><p>Pennington, J., Schoenholz, S., &amp; Ganguli, S. (2017). Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. <em>Advances in Neural Information Processing Systems</em> (pp. 4785–4795).</p>
</dd>
<dt class="label" id="id212"><span class="brackets">Pennington et al., 2014</span></dt>
<dd><p>Pennington, J., Socher, R., &amp; Manning, C. (2014). GloVe: global vectors for word representation. <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em> (pp. 1532–1543).</p>
</dd>
<dt class="label" id="id214"><span class="brackets">Peters et al., 2017a</span></dt>
<dd><p>Peters, J., Janzing, D., &amp; Schölkopf, B. (2017). <em>Elements of Causal Inference: Foundations and Learning Algorithms</em>. MIT Press.</p>
</dd>
<dt class="label" id="id213"><span class="brackets">Peters et al., 2017b</span></dt>
<dd><p>Peters, M., Ammar, W., Bhagavatula, C., &amp; Power, R. (2017). Semi-supervised sequence tagging with bidirectional language models. <em>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Volume 1</em> (pp. 1756–1765).</p>
</dd>
<dt class="label" id="id215"><span class="brackets">Peters et al., 2018</span></dt>
<dd><p>Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., &amp; Zettlemoyer, L. (2018). Deep contextualized word representations. <em>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1</em> (pp. 2227–2237).</p>
</dd>
<dt class="label" id="id216"><span class="brackets">Petersen &amp; Pedersen, 2008</span></dt>
<dd><p>Petersen, K. B., &amp; Pedersen, M. S. (2008). <em>The Matrix Cookbook</em>. Technical University of Denmark.</p>
</dd>
<dt class="label" id="id377"><span class="brackets">Pleiss et al., 2017</span></dt>
<dd><p>Pleiss, G., Chen, D., Huang, G., Li, T., Van Der Maaten, L., &amp; Weinberger, K. Q. (2017). Memory-efficient implementation of densenets. <em>ArXiv:1707.06990</em>.</p>
</dd>
<dt class="label" id="id217"><span class="brackets">Polyak, 1964</span></dt>
<dd><p>Polyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods. <em>USSR Computational Mathematics and Mathematical Physics</em>, <em>4</em>(5), 1–17.</p>
</dd>
<dt class="label" id="id428"><span class="brackets">Prakash et al., 2016</span></dt>
<dd><p>Prakash, A., Hasan, S. A., Lee, K., Datla, V., Qadir, A., Liu, J., &amp; Farri, O. (2016). Neural paraphrase generation with stacked residual LSTM networks. <em>ArXiv:1610.03098</em>.</p>
</dd>
<dt class="label" id="id487"><span class="brackets">Qin et al., 2023</span></dt>
<dd><p>Qin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., &amp; Yang, D. (2023). Is ChatGPT a general-purpose natural language processing task solver? <em>ArXiv:2302.06476</em>.</p>
</dd>
<dt class="label" id="id219"><span class="brackets">Quadrana et al., 2018</span></dt>
<dd><p>Quadrana, M., Cremonesi, P., &amp; Jannach, D. (2018). Sequence-aware recommender systems. <em>ACM Computing Surveys</em>, <em>51</em>(4), 66.</p>
</dd>
<dt class="label" id="id220"><span class="brackets">Quinlan, 1993</span></dt>
<dd><p>Quinlan, J. R. (1993). <em>C4.5: Programs for Machine Learning</em>. Elsevier.</p>
</dd>
<dt class="label" id="id271"><span class="brackets">Rabiner &amp; Juang, 1993</span></dt>
<dd><p>Rabiner, L., &amp; Juang, B.-H. (1993). <em>Fundamentals of Speech Recognition</em>. Prentice-Hall.</p>
</dd>
<dt class="label" id="id405"><span class="brackets">Radford et al., 2021</span></dt>
<dd><p>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., … et al. (2021). Learning transferable visual models from natural language supervision. <em>International Conference on Machine Learning</em> (pp. 8748–8763).</p>
</dd>
<dt class="label" id="id221"><span class="brackets">Radford et al., 2015</span></dt>
<dd><p>Radford, A., Metz, L., &amp; Chintala, S. (2015). Unsupervised representation learning with deep convolutional generative adversarial networks. <em>ArXiv:1511.06434</em>.</p>
</dd>
<dt class="label" id="id222"><span class="brackets">Radford et al., 2018</span></dt>
<dd><p>Radford, A., Narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). Improving language understanding by generative pre-training. <em>OpenAI</em>.</p>
</dd>
<dt class="label" id="id223"><span class="brackets">Radford et al., 2019</span></dt>
<dd><p>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp; Sutskever, I. (2019). Language models are unsupervised multitask learners. <em>OpenAI Blog</em>, <em>1</em>(8), 9.</p>
</dd>
<dt class="label" id="id363"><span class="brackets">Radosavovic et al., 2019</span></dt>
<dd><p>Radosavovic, I., Johnson, J., Xie, S., Lo, W.-Y., &amp; Dollár, P. (2019). On network design spaces for visual recognition. <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> (pp. 1882–1890).</p>
</dd>
<dt class="label" id="id225"><span class="brackets">Radosavovic et al., 2020</span></dt>
<dd><p>Radosavovic, I., Kosaraju, R. P., Girshick, R., He, K., &amp; Dollár, P. (2020). Designing network design spaces. <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (pp. 10428–10436).</p>
</dd>
<dt class="label" id="id394"><span class="brackets">Rae et al., 2021</span></dt>
<dd><p>Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., … et al. (2021). Scaling language models: methods, analysis &amp; insights from training gopher. <em>ArXiv:2112.11446</em>.</p>
</dd>
<dt class="label" id="id381"><span class="brackets">Raffel et al., 2020</span></dt>
<dd><p>Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., … Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. <em>Journal of Machine Learning Research</em>, <em>21</em>, 1–67.</p>
</dd>
<dt class="label" id="id226"><span class="brackets">Rajpurkar et al., 2016</span></dt>
<dd><p>Rajpurkar, P., Zhang, J., Lopyrev, K., &amp; Liang, P. (2016). SQuAD: 100,000+ questions for machine comprehension of text. <em>ArXiv:1606.05250</em>.</p>
</dd>
<dt class="label" id="id378"><span class="brackets">Ramachandran et al., 2019</span></dt>
<dd><p>Ramachandran, P., Parmar, N., Vaswani, A., Bello, I., Levskaya, A., &amp; Shlens, J. (2019). Stand-alone self-attention in vision models. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</p>
</dd>
<dt class="label" id="id227"><span class="brackets">Ramachandran et al., 2017</span></dt>
<dd><p>Ramachandran, P., Zoph, B., &amp; Le, Q. V. (2017). Searching for activation functions. <em>ArXiv:1710.05941</em>.</p>
</dd>
<dt class="label" id="id407"><span class="brackets">Ramesh et al., 2022</span></dt>
<dd><p>Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., &amp; Chen, M. (2022). Hierarchical text-conditional image generation with clip latents. <em>ArXiv:2204.06125</em>.</p>
</dd>
<dt class="label" id="id32"><span class="brackets">Cajal &amp; Azoulay, 1894</span></dt>
<dd><p>Ramón y Cajal, Santiago, &amp; Azoulay, L. (1894). <em>Les Nouvelles Idées sur la Structure du Système Nerveux chez l'Homme et chez les Vertébrés</em>. Paris, C. Reinwald &amp; Cie.</p>
</dd>
<dt class="label" id="id228"><span class="brackets">Ranzato et al., 2007</span></dt>
<dd><p>Ranzato, M.-A., Boureau, Y.-L., Chopra, S., &amp; LeCun, Y. (2007). A unified energy-based framework for unsupervised learning. <em>Artificial Intelligence and Statistics</em> (pp. 371–379).</p>
</dd>
<dt class="label" id="id357"><span class="brackets">Rasmussen &amp; Williams, 2006</span></dt>
<dd><p>Rasmussen, C. E., &amp; Williams, C. K. (2006). <em>Gaussian Processes for Machine Learning</em>. MIT Press.</p>
</dd>
<dt class="label" id="id229"><span class="brackets">Reddi et al., 2019</span></dt>
<dd><p>Reddi, S. J., Kale, S., &amp; Kumar, S. (2019). On the convergence of Adam and beyond. <em>ArXiv:1904.09237</em>.</p>
</dd>
<dt class="label" id="id230"><span class="brackets">Redmon et al., 2016</span></dt>
<dd><p>Redmon, J., Divvala, S., Girshick, R., &amp; Farhadi, A. (2016). You only look once: unified, real-time object detection. <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 779–788).</p>
</dd>
<dt class="label" id="id231"><span class="brackets">Redmon &amp; Farhadi, 2018</span></dt>
<dd><p>Redmon, J., &amp; Farhadi, A. (2018). YOLOv3: an incremental improvement. <em>ArXiv:1804.02767</em>.</p>
</dd>
<dt class="label" id="id232"><span class="brackets">Reed &amp; DeFreitas, 2015</span></dt>
<dd><p>Reed, S., &amp; De Freitas, N. (2015). Neural programmer-interpreters. <em>ArXiv:1511.06279</em>.</p>
</dd>
<dt class="label" id="id410"><span class="brackets">Reed et al., 2022</span></dt>
<dd><p>Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., … et al. (2022). A generalist agent. <em>ArXiv:2205.06175</em>.</p>
</dd>
<dt class="label" id="id233"><span class="brackets">Ren et al., 2015</span></dt>
<dd><p>Ren, S., He, K., Girshick, R., &amp; Sun, J. (2015). Faster R-CNN: towards real-time object detection with region proposal networks. <em>Advances in Neural Information Processing Systems</em> (pp. 91–99).</p>
</dd>
<dt class="label" id="id234"><span class="brackets">Rendle, 2010</span></dt>
<dd><p>Rendle, S. (2010). Factorization machines. <em>2010 IEEE International Conference on Data Mining</em> (pp. 995–1000).</p>
</dd>
<dt class="label" id="id235"><span class="brackets">Rendle et al., 2009</span></dt>
<dd><p>Rendle, S., Freudenthaler, C., Gantner, Z., &amp; Schmidt-Thieme, L. (2009). BPR: Bayesian personalized ranking from implicit feedback. <em>Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence</em> (pp. 452–461).</p>
</dd>
<dt class="label" id="id236"><span class="brackets">Revels et al., 2016</span></dt>
<dd><p>Revels, J., Lubin, M., &amp; Papamarkou, T. (2016). Forward-mode automatic differentiation in Julia. <em>ArXiv:1607.07892</em>.</p>
</dd>
<dt class="label" id="id475"><span class="brackets">Rezende et al., 2014</span></dt>
<dd><p>Rezende, D. J., Mohamed, S., &amp; Wierstra, D. (2014). Stochastic backpropagation and approximate inference in deep generative models. <em>International Conference on Machine Learning</em> (pp. 1278–1286).</p>
</dd>
<dt class="label" id="id237"><span class="brackets">Riesenhuber &amp; Poggio, 1999</span></dt>
<dd><p>Riesenhuber, M., &amp; Poggio, T. (1999). Hierarchical models of object recognition in cortex. <em>Nature Neuroscience</em>, <em>2</em>(11), 1019–1025.</p>
</dd>
<dt class="label" id="id238"><span class="brackets">Rockafellar, 1970</span></dt>
<dd><p>Rockafellar, R. T. (1970). <em>Convex Analysis</em>. Princeton University Press.</p>
</dd>
<dt class="label" id="id239"><span class="brackets">Rolnick et al., 2017</span></dt>
<dd><p>Rolnick, D., Veit, A., Belongie, S., &amp; Shavit, N. (2017). Deep learning is robust to massive label noise. <em>ArXiv:1705.10694</em>.</p>
</dd>
<dt class="label" id="id241"><span class="brackets">Rudin, 1973</span></dt>
<dd><p>Rudin, W. (1973). <em>Functional Analysis</em>. McGraw-Hill.</p>
</dd>
<dt class="label" id="id242"><span class="brackets">Rumelhart et al., 1988</span></dt>
<dd><p>Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1988). Learning representations by back-propagating errors. <em>Cognitive Modeling</em>, <em>5</em>(3), 1.</p>
</dd>
<dt class="label" id="id243"><span class="brackets">Russakovsky et al., 2013</span></dt>
<dd><p>Russakovsky, O., Deng, J., Huang, Z., Berg, A. C., &amp; Fei-Fei, L. (2013). Detecting avocados to zucchinis: what have we done, and where are we going? <em>International Conference on Computer Vision (ICCV)</em>.</p>
</dd>
<dt class="label" id="id430"><span class="brackets">Russakovsky et al., 2015</span></dt>
<dd><p>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., … et al. (2015). ImageNet large scale visual recognition challenge. <em>International Journal of Computer Vision</em>, <em>115</em>(3), 211–252.</p>
</dd>
<dt class="label" id="id244"><span class="brackets">Russell &amp; Norvig, 2016</span></dt>
<dd><p>Russell, S. J., &amp; Norvig, P. (2016). <em>Artificial Intelligence: A Modern Approach</em>. Pearson Education Limited.</p>
</dd>
<dt class="label" id="id409"><span class="brackets">Saharia et al., 2022</span></dt>
<dd><p>Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., … et al. (2022). Photorealistic text-to-image diffusion models with deep language understanding. <em>ArXiv:2205.11487</em>.</p>
</dd>
<dt class="label" id="id443"><span class="brackets">Salinas et al., 2022</span></dt>
<dd><p>Salinas, D., Seeger, M., Klein, A., Perrone, V., Wistuba, M., &amp; Archambeau, C. (2022). Syne Tune: a library for large scale hyperparameter tuning and reproducible research. <em>First Conference on Automated Machine Learning</em>.</p>
</dd>
<dt class="label" id="id390"><span class="brackets">Sanh et al., 2019</span></dt>
<dd><p>Sanh, V., Debut, L., Chaumond, J., &amp; Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. <em>ArXiv:1910.01108</em>.</p>
</dd>
<dt class="label" id="id482"><span class="brackets">Sanh et al., 2021</span></dt>
<dd><p>Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., … et al. (2021). Multitask prompted training enables zero-shot task generalization. <em>ArXiv:2110.08207</em>.</p>
</dd>
<dt class="label" id="id246"><span class="brackets">Santurkar et al., 2018</span></dt>
<dd><p>Santurkar, S., Tsipras, D., Ilyas, A., &amp; Madry, A. (2018). How does batch normalization help optimization? <em>Advances in Neural Information Processing Systems</em> (pp. 2483–2493).</p>
</dd>
<dt class="label" id="id247"><span class="brackets">Sarwar et al., 2001</span></dt>
<dd><p>Sarwar, B. M., Karypis, G., Konstan, J. A., &amp; Riedl, J. (2001). Item-based collaborative filtering recommendation algorithms. <em>Proceedings of 10th International Conference on World Wide Web</em> (pp. 285–295).</p>
</dd>
<dt class="label" id="id488"><span class="brackets">Scao et al., 2022</span></dt>
<dd><p>Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., … et al. (2022). BLOOM: a 176B-parameter open-access multilingual language model. <em>ArXiv:2211.05100</em>.</p>
</dd>
<dt class="label" id="id248"><span class="brackets">Schein et al., 2002</span></dt>
<dd><p>Schein, A. I., Popescul, A., Ungar, L. H., &amp; Pennock, D. M. (2002). Methods and metrics for cold-start recommendations. <em>Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</em> (pp. 253–260).</p>
</dd>
<dt class="label" id="id421"><span class="brackets">Schuhmann et al., 2022</span></dt>
<dd><p>Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., … et al. (2022). LAION-5B: an open large-scale dataset for training next generation image-text models. <em>ArXiv:2210.08402</em>.</p>
</dd>
<dt class="label" id="id252"><span class="brackets">Schuster &amp; Paliwal, 1997</span></dt>
<dd><p>Schuster, M., &amp; Paliwal, K. K. (1997). Bidirectional recurrent neural networks. <em>IEEE Transactions on Signal Processing</em>, <em>45</em>(11), 2673–2681.</p>
</dd>
<dt class="label" id="id250"><span class="brackets">Scholkopf et al., 2001</span></dt>
<dd><p>Schölkopf, B., Herbrich, R., &amp; Smola, A. J. (2001). Helmbold, D. P., &amp; Williamson, B. (Eds.). A generalized representer theorem. <em>Proceedings of the Annual Conference on Computational Learning Theory</em> (pp. 416–426). Springer-Verlag.</p>
</dd>
<dt class="label" id="id249"><span class="brackets">Scholkopf et al., 1996</span></dt>
<dd><p>Schölkopf, B., Burges, C., &amp; Vapnik, V. (1996). Incorporating invariances in support vector learning machines. <em>International Conference on Artificial Neural Networks</em> (pp. 47–52).</p>
</dd>
<dt class="label" id="id251"><span class="brackets">Scholkopf &amp; Smola, 2002</span></dt>
<dd><p>Schölkopf, B., &amp; Smola, A. J. (2002). <em>Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond</em>. MIT Press.</p>
</dd>
<dt class="label" id="id253"><span class="brackets">Sedhain et al., 2015</span></dt>
<dd><p>Sedhain, S., Menon, A. K., Sanner, S., &amp; Xie, L. (2015). Autorec: autoencoders meet collaborative filtering. <em>Proceedings of the 24th International Conference on World Wide Web</em> (pp. 111–112).</p>
</dd>
<dt class="label" id="id254"><span class="brackets">Sennrich et al., 2015</span></dt>
<dd><p>Sennrich, R., Haddow, B., &amp; Birch, A. (2015). Neural machine translation of rare words with subword units. <em>ArXiv:1508.07909</em>.</p>
</dd>
<dt class="label" id="id255"><span class="brackets">Sergeev &amp; DelBalso, 2018</span></dt>
<dd><p>Sergeev, A., &amp; Del Balso, M. (2018). Horovod: fast and easy distributed deep learning in TensorFlow. <em>ArXiv:1802.05799</em>.</p>
</dd>
<dt class="label" id="id256"><span class="brackets">Shannon, 1948</span></dt>
<dd><p>Shannon, C. E. (1948). A mathematical theory of communication. <em>The Bell System Technical Journal</em>, <em>27</em>(3), 379–423.</p>
</dd>
<dt class="label" id="id257"><span class="brackets">Shao et al., 2020</span></dt>
<dd><p>Shao, H., Yao, S., Sun, D., Zhang, A., Liu, S., Liu, D., … Abdelzaher, T. (2020). ControlVAE: controllable variational autoencoder. <em>Proceedings of the 37th International Conference on Machine Learning</em>.</p>
</dd>
<dt class="label" id="id368"><span class="brackets">Shaw et al., 2018</span></dt>
<dd><p>Shaw, P., Uszkoreit, J., &amp; Vaswani, A. (2018). Self-attention with relative position representations. <em>ArXiv:1803.02155</em>.</p>
</dd>
<dt class="label" id="id396"><span class="brackets">Shoeybi et al., 2019</span></dt>
<dd><p>Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., &amp; Catanzaro, B. (2019). Megatron-LM: training multi-billion parameter language models using model parallelism. <em>ArXiv:1909.08053</em>.</p>
</dd>
<dt class="label" id="id258"><span class="brackets">Silver et al., 2016</span></dt>
<dd><p>Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., … et al. (2016). Mastering the game of Go with deep neural networks and tree search. <em>Nature</em>, <em>529</em>(7587), 484.</p>
</dd>
<dt class="label" id="id124"><span class="brackets">Silverman, 1986</span></dt>
<dd><p>Silverman, B. W. (1986). <em>Density Estimation for Statistical and Data Analysis</em>. Chapman and Hall.</p>
</dd>
<dt class="label" id="id259"><span class="brackets">Simard et al., 1998</span></dt>
<dd><p>Simard, P. Y., LeCun, Y. A., Denker, J. S., &amp; Victorri, B. (1998). Transformation invariance in pattern recognition – tangent distance and tangent propagation. <em>Neural Networks: Tricks of the Trade</em> (pp. 239–274). Springer.</p>
</dd>
<dt class="label" id="id260"><span class="brackets">Simonyan &amp; Zisserman, 2014</span></dt>
<dd><p>Simonyan, K., &amp; Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. <em>ArXiv:1409.1556</em>.</p>
</dd>
<dt class="label" id="id352"><span class="brackets">Sindhwani et al., 2015</span></dt>
<dd><p>Sindhwani, V., Sainath, T. N., &amp; Kumar, S. (2015). Structured transforms for small-footprint deep learning. <em>ArXiv:1510.01722</em>.</p>
</dd>
<dt class="label" id="id261"><span class="brackets">Sivic &amp; Zisserman, 2003</span></dt>
<dd><p>Sivic, J., &amp; Zisserman, A. (2003). Video Google: a text retrieval approach to object matching in videos. <em>Proceedings of the IEEE International Conference on Computer Vision</em> (pp. 1470–1470).</p>
</dd>
<dt class="label" id="id397"><span class="brackets">Smith et al., 2022</span></dt>
<dd><p>Smith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhandari, S., Casper, J., … et al. (2022). Using DeepSpeed and Megatron to train Megatron-Turing NLG 530B, a large-scale generative language model. <em>ArXiv:2201.11990</em>.</p>
</dd>
<dt class="label" id="id262"><span class="brackets">Smola &amp; Narayanamurthy, 2010</span></dt>
<dd><p>Smola, A., &amp; Narayanamurthy, S. (2010). An architecture for parallel topic models. <em>Proceedings of the VLDB Endowment</em>, <em>3</em>(1-2), 703–710.</p>
</dd>
<dt class="label" id="id452"><span class="brackets">Snoek et al., 2012</span></dt>
<dd><p>Snoek, J., Larochelle, H., &amp; Adams, R. (2012). Practical Bayesian optimization of machine learning algorithms. <em>Advances in Neural Information Processing Systems 25</em> (pp. 2951–2959).</p>
</dd>
<dt class="label" id="id460"><span class="brackets">Sohl-Dickstein et al., 2015</span></dt>
<dd><p>Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., &amp; Ganguli, S. (2015). Deep unsupervised learning using nonequilibrium thermodynamics. <em>International Conference on Machine Learning</em> (pp. 2256–2265).</p>
</dd>
<dt class="label" id="id462"><span class="brackets">Song &amp; Ermon, 2019</span></dt>
<dd><p>Song, Y., &amp; Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</p>
</dd>
<dt class="label" id="id476"><span class="brackets">Song et al., 2021</span></dt>
<dd><p>Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., &amp; Poole, B. (2021). Score-based generative modeling through stochastic differential equations. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id263"><span class="brackets">Speelpenning, 1980</span></dt>
<dd><p>Speelpenning, B. (1980). <em>Compiling fast partial derivatives of functions given by algorithms</em> (Doctoral dissertation). University of Illinois at Urbana-Champaign.</p>
</dd>
<dt class="label" id="id415"><span class="brackets">Srivastava et al., 2022</span></dt>
<dd><p>Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., … et al. (2022). Beyond the imitation game: quantifying and extrapolating the capabilities of language models. <em>ArXiv:2206.04615</em>.</p>
</dd>
<dt class="label" id="id265"><span class="brackets">Srivastava et al., 2014</span></dt>
<dd><p>Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. <em>Journal of Machine Learning Research</em>, <em>15</em>(1), 1929–1958.</p>
</dd>
<dt class="label" id="id376"><span class="brackets">Srivastava et al., 2015</span></dt>
<dd><p>Srivastava, R. K., Greff, K., &amp; Schmidhuber, J. (2015). Highway networks. <em>ArXiv:1505.00387</em>.</p>
</dd>
<dt class="label" id="id266"><span class="brackets">Strang, 1993</span></dt>
<dd><p>Strang, G. (1993). <em>Introduction to Linear Algebra</em>. Wellesley–Cambridge Press.</p>
</dd>
<dt class="label" id="id267"><span class="brackets">Su &amp; Khoshgoftaar, 2009</span></dt>
<dd><p>Su, X., &amp; Khoshgoftaar, T. M. (2009). A survey of collaborative filtering techniques. <em>Advances in Artificial Intelligence</em>, <em>2009</em>.</p>
</dd>
<dt class="label" id="id268"><span class="brackets">Sukhbaatar et al., 2015</span></dt>
<dd><p>Sukhbaatar, S., Weston, J., &amp; Fergus, R. (2015). End-to-end memory networks. <em>Advances in Neural Information Processing Systems</em> (pp. 2440–2448).</p>
</dd>
<dt class="label" id="id269"><span class="brackets">Sutskever et al., 2013</span></dt>
<dd><p>Sutskever, I., Martens, J., Dahl, G., &amp; Hinton, G. (2013). On the importance of initialization and momentum in deep learning. <em>International Conference on Machine Learning</em> (pp. 1139–1147).</p>
</dd>
<dt class="label" id="id273"><span class="brackets">Sutskever et al., 2014</span></dt>
<dd><p>Sutskever, I., Vinyals, O., &amp; Le, Q. V. (2014). Sequence to sequence learning with neural networks. <em>Advances in Neural Information Processing Systems</em> (pp. 3104–3112).</p>
</dd>
<dt class="label" id="id274"><span class="brackets">Szegedy et al., 2017</span></dt>
<dd><p>Szegedy, C., Ioffe, S., Vanhoucke, V., &amp; Alemi, A. A. (2017). Inception-v4, Inception-ResNet and the impact of residual connections on learning. <em>31st AAAI Conference on Artificial Intelligence</em>.</p>
</dd>
<dt class="label" id="id275"><span class="brackets">Szegedy et al., 2015</span></dt>
<dd><p>Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., … Rabinovich, A. (2015). Going deeper with convolutions. <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 1–9).</p>
</dd>
<dt class="label" id="id276"><span class="brackets">Szegedy et al., 2016</span></dt>
<dd><p>Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., &amp; Wojna, Z. (2016). Rethinking the Inception architecture for computer vision. <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 2818–2826).</p>
</dd>
<dt class="label" id="id277"><span class="brackets">Tallec &amp; Ollivier, 2017</span></dt>
<dd><p>Tallec, C., &amp; Ollivier, Y. (2017). Unbiasing truncated backpropagation through time. <em>ArXiv:1705.08209</em>.</p>
</dd>
<dt class="label" id="id359"><span class="brackets">Tan &amp; Le, 2019</span></dt>
<dd><p>Tan, M., &amp; Le, Q. (2019). EfficientNet: rethinking model scaling for convolutional neural networks. <em>International Conference on Machine Learning</em> (pp. 6105–6114).</p>
</dd>
<dt class="label" id="id278"><span class="brackets">Tang &amp; Wang, 2018</span></dt>
<dd><p>Tang, J., &amp; Wang, K. (2018). Personalized top-n sequential recommendation via convolutional sequence embedding. <em>Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</em> (pp. 565–573).</p>
</dd>
<dt class="label" id="id279"><span class="brackets">Taskar et al., 2004</span></dt>
<dd><p>Taskar, B., Guestrin, C., &amp; Koller, D. (2004). Max-margin Markov networks. <em>Advances in Neural Information Processing Systems</em>, <em>16</em>, 25.</p>
</dd>
<dt class="label" id="id280"><span class="brackets">Tay et al., 2020</span></dt>
<dd><p>Tay, Y., Dehghani, M., Bahri, D., &amp; Metzler, D. (2020). Efficient transformers: a survey. <em>ArXiv:2009.06732</em>.</p>
</dd>
<dt class="label" id="id484"><span class="brackets">Taylor et al., 2022</span></dt>
<dd><p>Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E., … Stojnic, R. (2022). Galactica: a large language model for science. <em>ArXiv:2211.09085</em>.</p>
</dd>
<dt class="label" id="id281"><span class="brackets">Teye et al., 2018</span></dt>
<dd><p>Teye, M., Azizpour, H., &amp; Smith, K. (2018). Bayesian uncertainty estimation for batch normalized deep networks. <em>ArXiv:1802.06455</em>.</p>
</dd>
<dt class="label" id="id342"><span class="brackets">Thomee et al., 2016</span></dt>
<dd><p>Thomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni, K., Poland, D., … Li, L.-J. (2016). Yfcc100m: the new data in multimedia research. <em>Communications of the ACM</em>, <em>59</em>(2), 64–73.</p>
</dd>
<dt class="label" id="id284"><span class="brackets">Tieleman &amp; Hinton, 2012</span></dt>
<dd><p>Tieleman, T., &amp; Hinton, G. (2012). Divide the gradient by a running average of its recent magnitude. <em>COURSERA: Neural Networks for Machine Learning, Lecture 6.5-rmsprop</em>.</p>
</dd>
<dt class="label" id="id374"><span class="brackets">Tikhonov &amp; Arsenin, 1977</span></dt>
<dd><p>Tikhonov, A. N., &amp; Arsenin, V. Y. (1977). <em>Solutions of Ill-Posed Problems</em>. W.H. Winston.</p>
</dd>
<dt class="label" id="id364"><span class="brackets">Tolstikhin et al., 2021</span></dt>
<dd><p>Tolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., … et al. (2021). MLP-mixer: an all-MLP architecture for vision. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>.</p>
</dd>
<dt class="label" id="id285"><span class="brackets">Torralba et al., 2008</span></dt>
<dd><p>Torralba, A., Fergus, R., &amp; Freeman, W. T. (2008). 80 million tiny images: a large data set for nonparametric object and scene recognition. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <em>30</em>(11), 1958–1970.</p>
</dd>
<dt class="label" id="id366"><span class="brackets">Touvron et al., 2021</span></dt>
<dd><p>Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., &amp; Jégou, H. (2021). Training data-efficient image transformers &amp; distillation through attention. <em>International Conference on Machine Learning</em> (pp. 10347–10357).</p>
</dd>
<dt class="label" id="id485"><span class="brackets">Touvron et al., 2023a</span></dt>
<dd><p>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., … et al. (2023a). LLaMA: open and efficient foundation language models. <em>ArXiv:2302.13971</em>.</p>
</dd>
<dt class="label" id="id486"><span class="brackets">Touvron et al., 2023b</span></dt>
<dd><p>Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., … et al. (2023b). LLaMA 2: open foundation and fine-tuned chat models. <em>ArXiv:2307.09288</em>.</p>
</dd>
<dt class="label" id="id288"><span class="brackets">Tsoumakas &amp; Katakis, 2007</span></dt>
<dd><p>Tsoumakas, G., &amp; Katakis, I. (2007). Multi-label classification: an overview. <em>International Journal of Data Warehousing and Mining</em>, <em>3</em>(3), 1–13.</p>
</dd>
<dt class="label" id="id289"><span class="brackets">Turing, 1950</span></dt>
<dd><p>Turing, A. (1950). Computing machinery and intelligence. <em>Mind</em>, <em>59</em>(236), 433.</p>
</dd>
<dt class="label" id="id286"><span class="brackets">Toscher et al., 2009</span></dt>
<dd><p>Töscher, A., Jahrer, M., &amp; Bell, R. M. (2009). <em>The bigchaos solution to the Netflix grand prize</em>.</p>
</dd>
<dt class="label" id="id290"><span class="brackets">Uijlings et al., 2013</span></dt>
<dd><p>Uijlings, J. R., Van De Sande, K. E., Gevers, T., &amp; Smeulders, A. W. (2013). Selective search for object recognition. <em>International Journal of Computer Vision</em>, <em>104</em>(2), 154–171.</p>
</dd>
<dt class="label" id="id424"><span class="brackets">Vapnik, 1995</span></dt>
<dd><p>Vapnik, V. (1995). <em>The Nature of Statistical Learning Theory</em>. New York: Springer.</p>
</dd>
<dt class="label" id="id343"><span class="brackets">Vapnik, 1998</span></dt>
<dd><p>Vapnik, V. (1998). <em>Statistical Learning Theory</em>. New York: John Wiley and Sons.</p>
</dd>
<dt class="label" id="id344"><span class="brackets">Vapnik &amp; Chervonenkis, 1964</span></dt>
<dd><p>Vapnik, V., &amp; Chervonenkis, A. (1964). A note on one class of perceptrons. <em>Automation and Remote Control</em>, <em>25</em>.</p>
</dd>
<dt class="label" id="id345"><span class="brackets">Vapnik &amp; Chervonenkis, 1968</span></dt>
<dd><p>Vapnik, V., &amp; Chervonenkis, A. (1968). Uniform convergence of frequencies of occurence of events to their probabilities. <em>Dokl. Akad. Nauk SSSR</em>, <em>181</em>, 915-918.</p>
</dd>
<dt class="label" id="id346"><span class="brackets">Vapnik &amp; Chervonenkis, 1971</span></dt>
<dd><p>Vapnik, V., &amp; Chervonenkis, A. (1971). On the uniform convergence of relative frequencies of events to their probabilities. <em>Theory Probab. Appl.</em>, <em>16</em>(2), 264-281.</p>
</dd>
<dt class="label" id="id349"><span class="brackets">Vapnik &amp; Chervonenkis, 1981</span></dt>
<dd><p>Vapnik, V., &amp; Chervonenkis, A. (1981). The necessary and sufficient conditions for the uniform convergence of averages to their expected values. <em>Teoriya Veroyatnostei i Ee Primeneniya</em>, <em>26</em>(3), 543-564.</p>
</dd>
<dt class="label" id="id350"><span class="brackets">Vapnik &amp; Chervonenkis, 1991</span></dt>
<dd><p>Vapnik, V., &amp; Chervonenkis, A. (1991). The necessary and sufficient conditions for consistency in the empirical risk minimization method. <em>Pattern Recognition and Image Analysis</em>, <em>1</em>(3), 283-305.</p>
</dd>
<dt class="label" id="id348"><span class="brackets">Vapnik &amp; Chervonenkis, 1974</span></dt>
<dd><p>Vapnik, V. N., &amp; Chervonenkis, A. Y. (1974). Ordered risk minimization. <em>Automation and Remote Control</em>, <em>35</em>, 1226–1235, 1403–1412.</p>
</dd>
<dt class="label" id="id291"><span class="brackets">Vapnik, 1992</span></dt>
<dd><p>Vapnik, V. (1992). Principles of risk minimization for learning theory. <em>Advances in Neural Information Processing Systems</em> (pp. 831–838).</p>
</dd>
<dt class="label" id="id339"><span class="brackets">Vapnik et al., 1994</span></dt>
<dd><p>Vapnik, V., Levin, E., &amp; Le Cun, Y. (1994). Measuring the VC-dimension of a learning machine. <em>Neural Computation</em>, <em>6</em>(5), 851–876.</p>
</dd>
<dt class="label" id="id302"><span class="brackets">Vaswani et al., 2017</span></dt>
<dd><p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention is all you need. <em>Advances in Neural Information Processing Systems</em> (pp. 5998–6008).</p>
</dd>
<dt class="label" id="id303"><span class="brackets">Wahba, 1990</span></dt>
<dd><p>Wahba, G. (1990). <em>Spline Models for Observational Data</em>. SIAM.</p>
</dd>
<dt class="label" id="id304"><span class="brackets">Waibel et al., 1989</span></dt>
<dd><p>Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., &amp; Lang, K. J. (1989). Phoneme recognition using time-delay neural networks. <em>IEEE Transactions on Acoustics, Speech, and Signal Processing</em>, <em>37</em>(3), 328–339.</p>
</dd>
<dt class="label" id="id470"><span class="brackets">Wang et al., 2022</span></dt>
<dd><p>Wang, H., Zhang, A., Zheng, S., Shi, X., Li, M., &amp; Wang, Z. (2022). Removing batch normalization boosts adversarial training. <em>International Conference on Machine Learning</em> (pp. 23433–23445).</p>
</dd>
<dt class="label" id="id306"><span class="brackets">Wang et al., 2018</span></dt>
<dd><p>Wang, L., Li, M., Liberty, E., &amp; Smola, A. J. (2018). Optimal message scheduling for aggregation. <em>Networks</em>, <em>2</em>(3), 2–3.</p>
</dd>
<dt class="label" id="id384"><span class="brackets">Wang et al., 2019</span></dt>
<dd><p>Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F., &amp; Chao, L. S. (2019). Learning deep transformer models for machine translation. <em>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em> (pp. 1810–1822).</p>
</dd>
<dt class="label" id="id473"><span class="brackets">Wang et al., 2023</span></dt>
<dd><p>Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., &amp; Zhou, D. (2023). Self-consistency improves chain of thought reasoning in language models. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id305"><span class="brackets">Wang et al., 2016</span></dt>
<dd><p>Wang, Y., Davidson, A., Pan, Y., Wu, Y., Riffel, A., &amp; Owens, J. D. (2016). Gunrock: a high-performance graph processing library on the GPU. <em>ACM SIGPLAN Notices</em> (p. 11).</p>
</dd>
<dt class="label" id="id307"><span class="brackets">Warstadt et al., 2019</span></dt>
<dd><p>Warstadt, A., Singh, A., &amp; Bowman, S. R. (2019). Neural network acceptability judgments. <em>Transactions of the Association for Computational Linguistics</em>, <em>7</em>, 625–641.</p>
</dd>
<dt class="label" id="id308"><span class="brackets">Wasserman, 2013</span></dt>
<dd><p>Wasserman, L. (2013). <em>All of Statistics: A Concise Course in Statistical Inference</em>. Springer.</p>
</dd>
<dt class="label" id="id309"><span class="brackets">Watkins &amp; Dayan, 1992</span></dt>
<dd><p>Watkins, C. J., &amp; Dayan, P. (1992). Q-learning. <em>Machine Learning</em>, <em>8</em>(3–4), 279–292.</p>
</dd>
<dt class="label" id="id310"><span class="brackets">Watson, 1964</span></dt>
<dd><p>Watson, G. S. (1964). Smooth regression analysis. <em>Sankhyā: The Indian Journal of Statistics, Series A</em>, pp. 359–372.</p>
</dd>
<dt class="label" id="id481"><span class="brackets">Wei et al., 2021</span></dt>
<dd><p>Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., … Le, Q. V. (2021). Finetuned language models are zero-shot learners. <em>ArXiv:2109.01652</em>.</p>
</dd>
<dt class="label" id="id404"><span class="brackets">Wei et al., 2022a</span></dt>
<dd><p>Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., … et al. (2022). Emergent abilities of large language models. <em>ArXiv:2206.07682</em>.</p>
</dd>
<dt class="label" id="id468"><span class="brackets">Wei et al., 2022b</span></dt>
<dd><p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., &amp; Zhou, D. (2022). Chain of thought prompting elicits reasoning in large language models. <em>ArXiv:2201.11903</em>.</p>
</dd>
<dt class="label" id="id311"><span class="brackets">Welling &amp; Teh, 2011</span></dt>
<dd><p>Welling, M., &amp; Teh, Y. W. (2011). Bayesian learning via stochastic gradient Langevin dynamics. <em>Proceedings of the 28th International Conference on Machine Learning (ICML-11)</em> (pp. 681–688).</p>
</dd>
<dt class="label" id="id312"><span class="brackets">Wengert, 1964</span></dt>
<dd><p>Wengert, R. E. (1964). A simple automatic derivative evaluation program. <em>Communications of the ACM</em>, <em>7</em>(8), 463–464.</p>
</dd>
<dt class="label" id="id313"><span class="brackets">Werbos, 1990</span></dt>
<dd><p>Werbos, P. J. (1990). Backpropagation through time: what it does and how to do it. <em>Proceedings of the IEEE</em>, <em>78</em>(10), 1550–1560.</p>
</dd>
<dt class="label" id="id314"><span class="brackets">Wigner, 1958</span></dt>
<dd><p>Wigner, E. P. (1958). On the distribution of the roots of certain symmetric matrices. <em>Ann. Math.</em> (pp. 325–327).</p>
</dd>
<dt class="label" id="id385"><span class="brackets">Wilson &amp; Izmailov, 2020</span></dt>
<dd><p>Wilson, A. G., &amp; Izmailov, P. (2020). Bayesian deep learning and a probabilistic perspective of generalization. <em>Advances in Neural Information Processing Systems</em>, <em>33</em>, 4697–4708.</p>
</dd>
<dt class="label" id="id449"><span class="brackets">Wistuba et al., 2019</span></dt>
<dd><p>Wistuba, M., Rawat, A., &amp; Pedapati, T. (2019). A survey on neural architecture search. <em>ArXiv:1905.01392 [cs.LG]</em>.</p>
</dd>
<dt class="label" id="id433"><span class="brackets">Wistuba et al., 2018</span></dt>
<dd><p>Wistuba, M., Schilling, N., &amp; Schmidt-Thieme, L. (2018). Scalable Gaussian process-based transfer surrogates for hyperparameter optimization. <em>Machine Learning</em>, <em>108</em>, 43–78.</p>
</dd>
<dt class="label" id="id458"><span class="brackets">Wolpert &amp; Macready, 1995</span></dt>
<dd><p>Wolpert, D. H., &amp; Macready, W. G. (1995). <em>No free lunch theorems for search</em>. Technical Report SFI-TR-95-02-010, Santa Fe Institute.</p>
</dd>
<dt class="label" id="id316"><span class="brackets">Wood et al., 2011</span></dt>
<dd><p>Wood, F., Gasthaus, J., Archambeau, C., James, L., &amp; Teh, Y. W. (2011). The sequence memoizer. <em>Communications of the ACM</em>, <em>54</em>(2), 91–98.</p>
</dd>
<dt class="label" id="id419"><span class="brackets">Wu et al., 2018</span></dt>
<dd><p>Wu, B., Wan, A., Yue, X., Jin, P., Zhao, S., Golmant, N., … Keutzer, K. (2018). Shift: a zero flop, zero parameter alternative to spatial convolutions. <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 9127–9135).</p>
</dd>
<dt class="label" id="id318"><span class="brackets">Wu et al., 2016</span></dt>
<dd><p>Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., … et al. (2016). Google's neural machine translation system: bridging the gap between human and machine translation. <em>ArXiv:1609.08144</em>.</p>
</dd>
<dt class="label" id="id320"><span class="brackets">Xiao et al., 2017</span></dt>
<dd><p>Xiao, H., Rasul, K., &amp; Vollgraf, R. (2017). Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. <em>ArXiv:1708.07747</em>.</p>
</dd>
<dt class="label" id="id319"><span class="brackets">Xiao et al., 2018</span></dt>
<dd><p>Xiao, L., Bahri, Y., Sohl-Dickstein, J., Schoenholz, S., &amp; Pennington, J. (2018). Dynamical isometry and a mean field theory of CNNs: how to train 10,000-layer vanilla convolutional neural networks. <em>International Conference on Machine Learning</em> (pp. 5393–5402).</p>
</dd>
<dt class="label" id="id321"><span class="brackets">Xie et al., 2017</span></dt>
<dd><p>Xie, S., Girshick, R., Dollár, P., Tu, Z., &amp; He, K. (2017). Aggregated residual transformations for deep neural networks. <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (pp. 1492–1500).</p>
</dd>
<dt class="label" id="id382"><span class="brackets">Xiong et al., 2020</span></dt>
<dd><p>Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., … Liu, T. (2020). On layer normalization in the transformer architecture. <em>International Conference on Machine Learning</em> (pp. 10524–10533).</p>
</dd>
<dt class="label" id="id322"><span class="brackets">Xiong et al., 2018</span></dt>
<dd><p>Xiong, W., Wu, L., Alleva, F., Droppo, J., Huang, X., &amp; Stolcke, A. (2018). The Microsoft 2017 conversational speech recognition system. <em>2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> (pp. 5934–5938).</p>
</dd>
<dt class="label" id="id323"><span class="brackets">Yamaguchi et al., 1990</span></dt>
<dd><p>Yamaguchi, K., Sakamoto, K., Akabane, T., &amp; Fujimoto, Y. (1990). A neural network for speaker-independent isolated word recognition. <em>First International Conference on Spoken Language Processing</em>.</p>
</dd>
<dt class="label" id="id272"><span class="brackets">Yang et al., 2016</span></dt>
<dd><p>Yang, Z., Hu, Z., Deng, Y., Dyer, C., &amp; Smola, A. (2016). Neural machine translation with recurrent attention modeling. <em>ArXiv:1607.05108</em>.</p>
</dd>
<dt class="label" id="id324"><span class="brackets">Yang et al., 2015</span></dt>
<dd><p>Yang, Z., Moczulski, M., Denil, M., De Freitas, N., Smola, A., Song, L., &amp; Wang, Z. (2015). Deep fried convnets. <em>Proceedings of the IEEE International Conference on Computer Vision</em> (pp. 1476–1483).</p>
</dd>
<dt class="label" id="id325"><span class="brackets">Ye et al., 2011</span></dt>
<dd><p>Ye, M., Yin, P., Lee, W.-C., &amp; Lee, D.-L. (2011). Exploiting geographical influence for collaborative point-of-interest recommendation. <em>Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval</em> (pp. 325–334).</p>
</dd>
<dt class="label" id="id326"><span class="brackets">You et al., 2017</span></dt>
<dd><p>You, Y., Gitman, I., &amp; Ginsburg, B. (2017). Large batch training of convolutional networks. <em>ArXiv:1708.03888</em>.</p>
</dd>
<dt class="label" id="id414"><span class="brackets">Yu et al., 2022</span></dt>
<dd><p>Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., … Wu, Y. (2022). Scaling autoregressive models for content-rich text-to-image generation. <em>ArXiv:2206.10789</em>.</p>
</dd>
<dt class="label" id="id328"><span class="brackets">Zaheer et al., 2018</span></dt>
<dd><p>Zaheer, M., Reddi, S., Sachan, D., Kale, S., &amp; Kumar, S. (2018). Adaptive methods for nonconvex optimization. <em>Advances in Neural Information Processing Systems</em> (pp. 9793–9803).</p>
</dd>
<dt class="label" id="id329"><span class="brackets">Zeiler, 2012</span></dt>
<dd><p>Zeiler, M. D. (2012). ADADELTA: an adaptive learning rate method. <em>ArXiv:1212.5701</em>.</p>
</dd>
<dt class="label" id="id330"><span class="brackets">Zeiler &amp; Fergus, 2013</span></dt>
<dd><p>Zeiler, M. D., &amp; Fergus, R. (2013). Stochastic pooling for regularization of deep convolutional neural networks. <em>ArXiv:1301.3557</em>.</p>
</dd>
<dt class="label" id="id333"><span class="brackets">Zhang et al., 2021a</span></dt>
<dd><p>Zhang, A., Tay, Y., Zhang, S., Chan, A., Luu, A. T., Hui, S. C., &amp; Fu, J. (2021). Beyond fully-connected layers with quaternions: parameterization of hypercomplex multiplications with 1/n parameters. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id372"><span class="brackets">Zhang et al., 2021b</span></dt>
<dd><p>Zhang, C., Bengio, S., Hardt, M., Recht, B., &amp; Vinyals, O. (2021). Understanding deep learning (still) requires rethinking generalization. <em>Communications of the ACM</em>, <em>64</em>(3), 107–115.</p>
</dd>
<dt class="label" id="id334"><span class="brackets">Zhang et al., 2019</span></dt>
<dd><p>Zhang, S., Yao, L., Sun, A., &amp; Tay, Y. (2019). Deep learning based recommender system: a survey and new perspectives. <em>ACM Computing Surveys</em>, <em>52</em>(1), 5.</p>
</dd>
<dt class="label" id="id400"><span class="brackets">Zhang et al., 2022</span></dt>
<dd><p>Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., … et al. (2022). OPT: open pre-trained transformer language models. <em>ArXiv:2205.01068</em>.</p>
</dd>
<dt class="label" id="id335"><span class="brackets">Zhang et al., 1988</span></dt>
<dd><p>Zhang, W., Tanida, J., Itoh, K., &amp; Ichioka, Y. (1988). Shift-invariant pattern recognition neural network and its optical architecture. <em>Proceedings of Annual Conference of the Japan Society of Applied Physics</em>.</p>
</dd>
<dt class="label" id="id332"><span class="brackets">Zhang et al., 2021c</span></dt>
<dd><p>Zhang, Y., Sun, P., Jiang, Y., Yu, D., Yuan, Z., Luo, P., … Wang, X. (2021). ByteTrack: multi-object tracking by associating every detection box. <em>ArXiv:2110.06864</em>.</p>
</dd>
<dt class="label" id="id469"><span class="brackets">Zhang et al., 2023a</span></dt>
<dd><p>Zhang, Z., Zhang, A., Li, M., &amp; Smola, A. (2023). Automatic chain of thought prompting in large language models. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id471"><span class="brackets">Zhang et al., 2023b</span></dt>
<dd><p>Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., &amp; Smola, A. (2023). Multimodal chain-of-thought reasoning in language models. <em>ArXiv:2302.00923</em>.</p>
</dd>
<dt class="label" id="id336"><span class="brackets">Zhao et al., 2019</span></dt>
<dd><p>Zhao, Z.-Q., Zheng, P., Xu, S.-t., &amp; Wu, X. (2019). Object detection with deep learning: a review. <em>IEEE Transactions on Neural Networks and Learning Systems</em>, <em>30</em>(11), 3212–3232.</p>
</dd>
<dt class="label" id="id474"><span class="brackets">Zhou et al., 2023</span></dt>
<dd><p>Zhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang, X., … Chi, E. (2023). Least-to-most prompting enables complex reasoning in large language models. <em>International Conference on Learning Representations</em>.</p>
</dd>
<dt class="label" id="id338"><span class="brackets">Zhu et al., 2017</span></dt>
<dd><p>Zhu, J.-Y., Park, T., Isola, P., &amp; Efros, A. A. (2017). Unpaired image-to-image translation using cycle-consistent adversarial networks. <em>Proceedings of the IEEE International Conference on Computer Vision</em> (pp. 2223–2232).</p>
</dd>
<dt class="label" id="id337"><span class="brackets">Zhu et al., 2015</span></dt>
<dd><p>Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., &amp; Fidler, S. (2015). Aligning books and movies: towards story-like visual explanations by watching movies and reading books. <em>Proceedings of the IEEE International Conference on Computer Vision</em> (pp. 19–27).</p>
</dd>
<dt class="label" id="id360"><span class="brackets">Zoph &amp; Le, 2016</span></dt>
<dd><p>Zoph, B., &amp; Le, Q. V. (2016). Neural architecture search with reinforcement learning. <em>ArXiv:1611.01578</em>.</p>
</dd>
</dl>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="../chapter_appendix-tools-for-deep-learning/d2l.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>23.8. The d2l API Document</div>
         </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>