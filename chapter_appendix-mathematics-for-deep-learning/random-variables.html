<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>22.6. Random Variables &#8212; Dive into Deep Learning 1.0.3 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="22.7. Maximum Likelihood" href="maximum-likelihood.html" />
    <link rel="prev" title="22.5. Integral Calculus" href="integral-calculus.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">22. </span>Appendix: Mathematics for Deep Learning</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">22.6. </span>Random Variables</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_appendix-mathematics-for-deep-learning/random-variables.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="http://preview.d2l.ai/d2l-en/master">
                  <i class="fas fa-book"></i>
                  Preview Version
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai/d2l-en.pdf">
                  <i class="fas fa-file-pdf"></i>
                  PyTorch
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai/d2l-en-mxnet.pdf">
                  <i class="fas fa-file-pdf"></i>
                  MXNet
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai/d2l-en.zip">
                  <i class="fab fa-python"></i>
                  Notebooks
              </a>
          
              <a  class="mdl-navigation__link" href="https://courses.d2l.ai">
                  <i class="fas fa-user-graduate"></i>
                  Courses
              </a>
          
              <a  class="mdl-navigation__link" href="https://github.com/d2l-ai/d2l-en">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://zh.d2l.ai">
                  <i class="fas fa-external-link-alt"></i>
                  中文版
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="Dive into Deep Learning"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index.html">Notation</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. Preliminaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html">2.1. Data Manipulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html">2.2. Data Preprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html">2.3. Linear Algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html">2.4. Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html">2.5. Automatic Differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html">2.6. Probability and Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html">2.7. Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-regression/index.html">3. Linear Neural Networks for Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression.html">3.1. Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/oo-design.html">3.2. Object-Oriented Design for Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/synthetic-regression-data.html">3.3. Synthetic Regression Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression-scratch.html">3.4. Linear Regression Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression-concise.html">3.5. Concise Implementation of Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/generalization.html">3.6. Generalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/weight-decay.html">3.7. Weight Decay</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-classification/index.html">4. Linear Neural Networks for Classification</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression.html">4.1. Softmax Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/image-classification-dataset.html">4.2. The Image Classification Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/classification.html">4.3. The Base Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression-scratch.html">4.4. Softmax Regression Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression-concise.html">4.5. Concise Implementation of Softmax Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/generalization-classification.html">4.6. Generalization in Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/environment-and-distribution-shift.html">4.7. Environment and Distribution Shift</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multilayer-perceptrons/index.html">5. Multilayer Perceptrons</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp.html">5.1. Multilayer Perceptrons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-implementation.html">5.2. Implementation of Multilayer Perceptrons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/backprop.html">5.3. Forward Propagation, Backward Propagation, and Computational Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html">5.4. Numerical Stability and Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/generalization-deep.html">5.5. Generalization in Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/dropout.html">5.6. Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/kaggle-house-price.html">5.7. Predicting House Prices on Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_builders-guide/index.html">6. Builders’ Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/model-construction.html">6.1. Layers and Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/parameters.html">6.2. Parameter Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/init-param.html">6.3. Parameter Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/lazy-init.html">6.4. Lazy Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/custom-layer.html">6.5. Custom Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/read-write.html">6.6. File I/O</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/use-gpu.html">6.7. GPUs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index.html">7. Convolutional Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv.html">7.1. From Fully Connected Layers to Convolutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer.html">7.2. Convolutions for Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides.html">7.3. Padding and Stride</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels.html">7.4. Multiple Input and Multiple Output Channels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling.html">7.5. Pooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet.html">7.6. Convolutional Neural Networks (LeNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index.html">8. Modern Convolutional Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet.html">8.1. Deep Convolutional Neural Networks (AlexNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/vgg.html">8.2. Networks Using Blocks (VGG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/nin.html">8.3. Network in Network (NiN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet.html">8.4. Multi-Branch Networks (GoogLeNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm.html">8.5. Batch Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet.html">8.6. Residual Networks (ResNet) and ResNeXt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet.html">8.7. Densely Connected Networks (DenseNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/cnn-design.html">8.8. Designing Convolution Network Architectures</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index.html">9. Recurrent Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/sequence.html">9.1. Working with Sequences</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/text-sequence.html">9.2. Converting Raw Text into Sequence Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/language-model.html">9.3. Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn.html">9.4. Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch.html">9.5. Recurrent Neural Network Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-concise.html">9.6. Concise Implementation of Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/bptt.html">9.7. Backpropagation Through Time</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index.html">10. Modern Recurrent Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm.html">10.1. Long Short-Term Memory (LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru.html">10.2. Gated Recurrent Units (GRU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn.html">10.3. Deep Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/bi-rnn.html">10.4. Bidirectional Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset.html">10.5. Machine Translation and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder.html">10.6. The Encoder–Decoder Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq.html">10.7. Sequence-to-Sequence Learning for Machine Translation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search.html">10.8. Beam Search</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/index.html">11. Attention Mechanisms and Transformers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/queries-keys-values.html">11.1. Queries, Keys, and Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/attention-pooling.html">11.2. Attention Pooling by Similarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html">11.3. Attention Scoring Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/bahdanau-attention.html">11.4. The Bahdanau Attention Mechanism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/multihead-attention.html">11.5. Multi-Head Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html">11.6. Self-Attention and Positional Encoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/transformer.html">11.7. The Transformer Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/vision-transformer.html">11.8. Transformers for Vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html">11.9. Large-Scale Pretraining with Transformers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_optimization/index.html">12. Optimization Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro.html">12.1. Optimization and Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/convexity.html">12.2. Convexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/gd.html">12.3. Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/sgd.html">12.4. Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/minibatch-sgd.html">12.5. Minibatch Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/momentum.html">12.6. Momentum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adagrad.html">12.7. Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/rmsprop.html">12.8. RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adadelta.html">12.9. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adam.html">12.10. Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/lr-scheduler.html">12.11. Learning Rate Scheduling</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational-performance/index.html">13. Computational Performance</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hybridize.html">13.1. Compilers and Interpreters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/async-computation.html">13.2. Asynchronous Computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/auto-parallelism.html">13.3. Automatic Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hardware.html">13.4. Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus.html">13.5. Training on Multiple GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus-concise.html">13.6. Concise Implementation for Multiple GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/parameterserver.html">13.7. Parameter Servers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index.html">14. Computer Vision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation.html">14.1. Image Augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning.html">14.2. Fine-Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box.html">14.3. Object Detection and Bounding Boxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor.html">14.4. Anchor Boxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection.html">14.5. Multiscale Object Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset.html">14.6. The Object Detection Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd.html">14.7. Single Shot Multibox Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn.html">14.8. Region-based CNNs (R-CNNs)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset.html">14.9. Semantic Segmentation and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv.html">14.10. Transposed Convolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn.html">14.11. Fully Convolutional Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style.html">14.12. Neural Style Transfer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10.html">14.13. Image Classification (CIFAR-10) on Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog.html">14.14. Dog Breed Identification (ImageNet Dogs) on Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index.html">15. Natural Language Processing: Pretraining</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec.html">15.1. Word Embedding (word2vec)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training.html">15.2. Approximate Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html">15.3. The Dataset for Pretraining Word Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html">15.4. Pretraining word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove.html">15.5. Word Embedding with Global Vectors (GloVe)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding.html">15.6. Subword Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy.html">15.7. Word Similarity and Analogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert.html">15.8. Bidirectional Encoder Representations from Transformers (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset.html">15.9. The Dataset for Pretraining BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining.html">15.10. Pretraining BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index.html">16. Natural Language Processing: Applications</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html">16.1. Sentiment Analysis and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html">16.2. Sentiment Analysis: Using Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html">16.3. Sentiment Analysis: Using Convolutional Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html">16.4. Natural Language Inference and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html">16.5. Natural Language Inference: Using Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert.html">16.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html">16.7. Natural Language Inference: Fine-Tuning BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement-learning/index.html">17. Reinforcement Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/mdp.html">17.1. Markov Decision Process (MDP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/value-iter.html">17.2. Value Iteration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/qlearning.html">17.3. Q-Learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_gaussian-processes/index.html">18. Gaussian Processes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-intro.html">18.1. Introduction to Gaussian Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-priors.html">18.2. Gaussian Process Priors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-inference.html">18.3. Gaussian Process Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_hyperparameter-optimization/index.html">19. Hyperparameter Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/hyperopt-intro.html">19.1. What Is Hyperparameter Optimization?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/hyperopt-api.html">19.2. Hyperparameter Optimization API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/rs-async.html">19.3. Asynchronous Random Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/sh-intro.html">19.4. Multi-Fidelity Hyperparameter Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/sh-async.html">19.5. Asynchronous Successive Halving</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_generative-adversarial-networks/index.html">20. Generative Adversarial Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/gan.html">20.1. Generative Adversarial Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/dcgan.html">20.2. Deep Convolutional Generative Adversarial Networks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender-systems/index.html">21. Recommender Systems</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/recsys-intro.html">21.1. Overview of Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/movielens.html">21.2. The MovieLens Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/mf.html">21.3. Matrix Factorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/autorec.html">21.4. AutoRec: Rating Prediction with Autoencoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ranking.html">21.5. Personalized Ranking for Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/neumf.html">21.6. Neural Collaborative Filtering for Personalized Ranking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/seqrec.html">21.7. Sequence-Aware Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ctr.html">21.8. Feature-Rich Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/fm.html">21.9. Factorization Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/deepfm.html">21.10. Deep Factorization Machines</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">22. Appendix: Mathematics for Deep Learning</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="geometry-linear-algebraic-ops.html">22.1. Geometry and Linear Algebraic Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="eigendecomposition.html">22.2. Eigendecompositions</a></li>
<li class="toctree-l2"><a class="reference internal" href="single-variable-calculus.html">22.3. Single Variable Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="multivariable-calculus.html">22.4. Multivariable Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="integral-calculus.html">22.5. Integral Calculus</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">22.6. Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="maximum-likelihood.html">22.7. Maximum Likelihood</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html">22.8. Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="naive-bayes.html">22.9. Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="statistics.html">22.10. Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="information-theory.html">22.11. Information Theory</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index.html">23. Appendix: Tools for Deep Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/jupyter.html">23.1. Using Jupyter Notebooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/sagemaker.html">23.2. Using Amazon SageMaker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/aws.html">23.3. Using AWS EC2 Instances</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/colab.html">23.4. Using Google Colab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html">23.5. Selecting Servers and GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/contributing.html">23.6. Contributing to This Book</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/utils.html">23.7. Utility Functions and Classes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/d2l.html">23.8. The <code class="docutils literal notranslate"><span class="pre">d2l</span></code> API Document</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">References</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="Dive into Deep Learning"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index.html">Notation</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. Preliminaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html">2.1. Data Manipulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html">2.2. Data Preprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html">2.3. Linear Algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html">2.4. Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html">2.5. Automatic Differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html">2.6. Probability and Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html">2.7. Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-regression/index.html">3. Linear Neural Networks for Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression.html">3.1. Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/oo-design.html">3.2. Object-Oriented Design for Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/synthetic-regression-data.html">3.3. Synthetic Regression Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression-scratch.html">3.4. Linear Regression Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression-concise.html">3.5. Concise Implementation of Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/generalization.html">3.6. Generalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/weight-decay.html">3.7. Weight Decay</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-classification/index.html">4. Linear Neural Networks for Classification</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression.html">4.1. Softmax Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/image-classification-dataset.html">4.2. The Image Classification Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/classification.html">4.3. The Base Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression-scratch.html">4.4. Softmax Regression Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression-concise.html">4.5. Concise Implementation of Softmax Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/generalization-classification.html">4.6. Generalization in Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/environment-and-distribution-shift.html">4.7. Environment and Distribution Shift</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multilayer-perceptrons/index.html">5. Multilayer Perceptrons</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp.html">5.1. Multilayer Perceptrons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-implementation.html">5.2. Implementation of Multilayer Perceptrons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/backprop.html">5.3. Forward Propagation, Backward Propagation, and Computational Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html">5.4. Numerical Stability and Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/generalization-deep.html">5.5. Generalization in Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/dropout.html">5.6. Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/kaggle-house-price.html">5.7. Predicting House Prices on Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_builders-guide/index.html">6. Builders’ Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/model-construction.html">6.1. Layers and Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/parameters.html">6.2. Parameter Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/init-param.html">6.3. Parameter Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/lazy-init.html">6.4. Lazy Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/custom-layer.html">6.5. Custom Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/read-write.html">6.6. File I/O</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/use-gpu.html">6.7. GPUs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index.html">7. Convolutional Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv.html">7.1. From Fully Connected Layers to Convolutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer.html">7.2. Convolutions for Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides.html">7.3. Padding and Stride</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels.html">7.4. Multiple Input and Multiple Output Channels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling.html">7.5. Pooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet.html">7.6. Convolutional Neural Networks (LeNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index.html">8. Modern Convolutional Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet.html">8.1. Deep Convolutional Neural Networks (AlexNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/vgg.html">8.2. Networks Using Blocks (VGG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/nin.html">8.3. Network in Network (NiN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet.html">8.4. Multi-Branch Networks (GoogLeNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm.html">8.5. Batch Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet.html">8.6. Residual Networks (ResNet) and ResNeXt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet.html">8.7. Densely Connected Networks (DenseNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/cnn-design.html">8.8. Designing Convolution Network Architectures</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index.html">9. Recurrent Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/sequence.html">9.1. Working with Sequences</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/text-sequence.html">9.2. Converting Raw Text into Sequence Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/language-model.html">9.3. Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn.html">9.4. Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch.html">9.5. Recurrent Neural Network Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-concise.html">9.6. Concise Implementation of Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/bptt.html">9.7. Backpropagation Through Time</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index.html">10. Modern Recurrent Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm.html">10.1. Long Short-Term Memory (LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru.html">10.2. Gated Recurrent Units (GRU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn.html">10.3. Deep Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/bi-rnn.html">10.4. Bidirectional Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset.html">10.5. Machine Translation and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder.html">10.6. The Encoder–Decoder Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq.html">10.7. Sequence-to-Sequence Learning for Machine Translation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search.html">10.8. Beam Search</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/index.html">11. Attention Mechanisms and Transformers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/queries-keys-values.html">11.1. Queries, Keys, and Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/attention-pooling.html">11.2. Attention Pooling by Similarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html">11.3. Attention Scoring Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/bahdanau-attention.html">11.4. The Bahdanau Attention Mechanism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/multihead-attention.html">11.5. Multi-Head Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html">11.6. Self-Attention and Positional Encoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/transformer.html">11.7. The Transformer Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/vision-transformer.html">11.8. Transformers for Vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html">11.9. Large-Scale Pretraining with Transformers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_optimization/index.html">12. Optimization Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro.html">12.1. Optimization and Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/convexity.html">12.2. Convexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/gd.html">12.3. Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/sgd.html">12.4. Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/minibatch-sgd.html">12.5. Minibatch Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/momentum.html">12.6. Momentum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adagrad.html">12.7. Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/rmsprop.html">12.8. RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adadelta.html">12.9. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adam.html">12.10. Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/lr-scheduler.html">12.11. Learning Rate Scheduling</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational-performance/index.html">13. Computational Performance</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hybridize.html">13.1. Compilers and Interpreters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/async-computation.html">13.2. Asynchronous Computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/auto-parallelism.html">13.3. Automatic Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hardware.html">13.4. Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus.html">13.5. Training on Multiple GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus-concise.html">13.6. Concise Implementation for Multiple GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/parameterserver.html">13.7. Parameter Servers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index.html">14. Computer Vision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation.html">14.1. Image Augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning.html">14.2. Fine-Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box.html">14.3. Object Detection and Bounding Boxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor.html">14.4. Anchor Boxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection.html">14.5. Multiscale Object Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset.html">14.6. The Object Detection Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd.html">14.7. Single Shot Multibox Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn.html">14.8. Region-based CNNs (R-CNNs)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset.html">14.9. Semantic Segmentation and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv.html">14.10. Transposed Convolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn.html">14.11. Fully Convolutional Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style.html">14.12. Neural Style Transfer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10.html">14.13. Image Classification (CIFAR-10) on Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog.html">14.14. Dog Breed Identification (ImageNet Dogs) on Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index.html">15. Natural Language Processing: Pretraining</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec.html">15.1. Word Embedding (word2vec)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training.html">15.2. Approximate Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html">15.3. The Dataset for Pretraining Word Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html">15.4. Pretraining word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove.html">15.5. Word Embedding with Global Vectors (GloVe)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding.html">15.6. Subword Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy.html">15.7. Word Similarity and Analogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert.html">15.8. Bidirectional Encoder Representations from Transformers (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset.html">15.9. The Dataset for Pretraining BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining.html">15.10. Pretraining BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index.html">16. Natural Language Processing: Applications</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html">16.1. Sentiment Analysis and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html">16.2. Sentiment Analysis: Using Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html">16.3. Sentiment Analysis: Using Convolutional Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html">16.4. Natural Language Inference and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html">16.5. Natural Language Inference: Using Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert.html">16.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html">16.7. Natural Language Inference: Fine-Tuning BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement-learning/index.html">17. Reinforcement Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/mdp.html">17.1. Markov Decision Process (MDP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/value-iter.html">17.2. Value Iteration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/qlearning.html">17.3. Q-Learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_gaussian-processes/index.html">18. Gaussian Processes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-intro.html">18.1. Introduction to Gaussian Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-priors.html">18.2. Gaussian Process Priors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-inference.html">18.3. Gaussian Process Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_hyperparameter-optimization/index.html">19. Hyperparameter Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/hyperopt-intro.html">19.1. What Is Hyperparameter Optimization?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/hyperopt-api.html">19.2. Hyperparameter Optimization API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/rs-async.html">19.3. Asynchronous Random Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/sh-intro.html">19.4. Multi-Fidelity Hyperparameter Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/sh-async.html">19.5. Asynchronous Successive Halving</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_generative-adversarial-networks/index.html">20. Generative Adversarial Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/gan.html">20.1. Generative Adversarial Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/dcgan.html">20.2. Deep Convolutional Generative Adversarial Networks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender-systems/index.html">21. Recommender Systems</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/recsys-intro.html">21.1. Overview of Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/movielens.html">21.2. The MovieLens Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/mf.html">21.3. Matrix Factorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/autorec.html">21.4. AutoRec: Rating Prediction with Autoencoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ranking.html">21.5. Personalized Ranking for Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/neumf.html">21.6. Neural Collaborative Filtering for Personalized Ranking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/seqrec.html">21.7. Sequence-Aware Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ctr.html">21.8. Feature-Rich Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/fm.html">21.9. Factorization Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/deepfm.html">21.10. Deep Factorization Machines</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">22. Appendix: Mathematics for Deep Learning</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="geometry-linear-algebraic-ops.html">22.1. Geometry and Linear Algebraic Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="eigendecomposition.html">22.2. Eigendecompositions</a></li>
<li class="toctree-l2"><a class="reference internal" href="single-variable-calculus.html">22.3. Single Variable Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="multivariable-calculus.html">22.4. Multivariable Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="integral-calculus.html">22.5. Integral Calculus</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">22.6. Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="maximum-likelihood.html">22.7. Maximum Likelihood</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html">22.8. Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="naive-bayes.html">22.9. Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="statistics.html">22.10. Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="information-theory.html">22.11. Information Theory</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index.html">23. Appendix: Tools for Deep Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/jupyter.html">23.1. Using Jupyter Notebooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/sagemaker.html">23.2. Using Amazon SageMaker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/aws.html">23.3. Using AWS EC2 Instances</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/colab.html">23.4. Using Google Colab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html">23.5. Selecting Servers and GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/contributing.html">23.6. Contributing to This Book</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/utils.html">23.7. Utility Functions and Classes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/d2l.html">23.8. The <code class="docutils literal notranslate"><span class="pre">d2l</span></code> API Document</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">References</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="random-variables">
<span id="sec-random-variables"></span><h1><span class="section-number">22.6. </span>Random Variables<a class="headerlink" href="#random-variables" title="Permalink to this heading">¶</a><div class="d2l-tabs" style="float:right"><div class="d2l-tabs__tab"><a href="https://colab.research.google.com/github/d2l-ai/d2l-pytorch-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/random-variables.ipynb" onclick="captureOutboundLink('https://colab.research.google.com/github/d2l-ai/d2l-pytorch-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/random-variables.ipynb'); return false;"> <button style="float:right", id="Colab_[pytorch]" class="mdl-button mdl-js-button mdl-button--primary mdl-js-ripple-effect"> <i class=" fas fa-external-link-alt"></i> Colab [pytorch] </button></a><div class="mdl-tooltip" data-mdl-for="Colab_[pytorch]"> Open the notebook in Colab</div></div><div class="d2l-tabs__tab"><a href="https://colab.research.google.com/github/d2l-ai/d2l-en-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/random-variables.ipynb" onclick="captureOutboundLink('https://colab.research.google.com/github/d2l-ai/d2l-en-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/random-variables.ipynb'); return false;"> <button style="float:right", id="Colab_[mxnet]" class="mdl-button mdl-js-button mdl-button--primary mdl-js-ripple-effect"> <i class=" fas fa-external-link-alt"></i> Colab [mxnet] </button></a><div class="mdl-tooltip" data-mdl-for="Colab_[mxnet]"> Open the notebook in Colab</div></div><div class="d2l-tabs__tab"><a href="https://colab.research.google.com/github/d2l-ai/d2l-jax-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/random-variables.ipynb" onclick="captureOutboundLink('https://colab.research.google.com/github/d2l-ai/d2l-jax-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/random-variables.ipynb'); return false;"> <button style="float:right", id="Colab_[jax]" class="mdl-button mdl-js-button mdl-button--primary mdl-js-ripple-effect"> <i class=" fas fa-external-link-alt"></i> Colab [jax] </button></a><div class="mdl-tooltip" data-mdl-for="Colab_[jax]"> Open the notebook in Colab</div></div><div class="d2l-tabs__tab"><a href="https://colab.research.google.com/github/d2l-ai/d2l-tensorflow-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/random-variables.ipynb" onclick="captureOutboundLink('https://colab.research.google.com/github/d2l-ai/d2l-tensorflow-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/random-variables.ipynb'); return false;"> <button style="float:right", id="Colab_[tensorflow]" class="mdl-button mdl-js-button mdl-button--primary mdl-js-ripple-effect"> <i class=" fas fa-external-link-alt"></i> Colab [tensorflow] </button></a><div class="mdl-tooltip" data-mdl-for="Colab_[tensorflow]"> Open the notebook in Colab</div></div></div></h1>
<p>In <a class="reference internal" href="../chapter_preliminaries/probability.html#sec-prob"><span class="std std-numref">Section 2.6</span></a> we saw the basics of how to work with discrete
random variables, which in our case refer to those random variables
which take either a finite set of possible values, or the integers. In
this section, we develop the theory of <em>continuous random variables</em>,
which are random variables which can take on any real value.</p>
<div class="section" id="continuous-random-variables">
<h2><span class="section-number">22.6.1. </span>Continuous Random Variables<a class="headerlink" href="#continuous-random-variables" title="Permalink to this heading">¶</a></h2>
<p>Continuous random variables are a significantly more subtle topic than
discrete random variables. A fair analogy to make is that the technical
jump is comparable to the jump between adding lists of numbers and
integrating functions. As such, we will need to take some time to
develop the theory.</p>
<div class="section" id="from-discrete-to-continuous">
<h3><span class="section-number">22.6.1.1. </span>From Discrete to Continuous<a class="headerlink" href="#from-discrete-to-continuous" title="Permalink to this heading">¶</a></h3>
<p>To understand the additional technical challenges encountered when
working with continuous random variables, let’s perform a thought
experiment. Suppose that we are throwing a dart at the dart board, and
we want to know the probability that it hits exactly
<span class="math notranslate nohighlight">\(2 \textrm{cm}\)</span> from the center of the board.</p>
<p>To start with, we imagine measuring a single digit of accuracy, that is
to say with bins for <span class="math notranslate nohighlight">\(0 \textrm{cm}\)</span>, <span class="math notranslate nohighlight">\(1 \textrm{cm}\)</span>,
<span class="math notranslate nohighlight">\(2 \textrm{cm}\)</span>, and so on. We throw say <span class="math notranslate nohighlight">\(100\)</span> darts at the
dart board, and if <span class="math notranslate nohighlight">\(20\)</span> of them fall into the bin for
<span class="math notranslate nohighlight">\(2\textrm{cm}\)</span> we conclude that <span class="math notranslate nohighlight">\(20\%\)</span> of the darts we throw
hit the board <span class="math notranslate nohighlight">\(2 \textrm{cm}\)</span> away from the center.</p>
<p>However, when we look closer, this does not match our question! We
wanted exact equality, whereas these bins hold all that fell between say
<span class="math notranslate nohighlight">\(1.5\textrm{cm}\)</span> and <span class="math notranslate nohighlight">\(2.5\textrm{cm}\)</span>.</p>
<p>Undeterred, we continue further. We measure even more precisely, say
<span class="math notranslate nohighlight">\(1.9\textrm{cm}\)</span>, <span class="math notranslate nohighlight">\(2.0\textrm{cm}\)</span>, <span class="math notranslate nohighlight">\(2.1\textrm{cm}\)</span>,
and now see that perhaps <span class="math notranslate nohighlight">\(3\)</span> of the <span class="math notranslate nohighlight">\(100\)</span> darts hit the
board in the <span class="math notranslate nohighlight">\(2.0\textrm{cm}\)</span> bucket. Thus we conclude the
probability is <span class="math notranslate nohighlight">\(3\%\)</span>.</p>
<p>However, this does not solve anything! We have just pushed the issue
down one digit further. Let’s abstract a bit. Imagine we know the
probability that the first <span class="math notranslate nohighlight">\(k\)</span> digits match with
<span class="math notranslate nohighlight">\(2.00000\ldots\)</span> and we want to know the probability it matches for
the first <span class="math notranslate nohighlight">\(k+1\)</span> digits. It is fairly reasonable to assume that the
<span class="math notranslate nohighlight">\({k+1}^{\textrm{th}}\)</span> digit is essentially a random choice from
the set <span class="math notranslate nohighlight">\(\{0, 1, 2, \ldots, 9\}\)</span>. At least, we cannot conceive of
a physically meaningful process which would force the number of
micrometers away form the center to prefer to end in a <span class="math notranslate nohighlight">\(7\)</span> vs a
<span class="math notranslate nohighlight">\(3\)</span>.</p>
<p>What this means is that in essence each additional digit of accuracy we
require should decrease probability of matching by a factor of
<span class="math notranslate nohighlight">\(10\)</span>. Or put another way, we would expect that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-0">
<span class="eqno">(22.6.1)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-0" title="Permalink to this equation">¶</a></span>\[P(\textrm{distance is}\; 2.00\ldots, \;\textrm{to}\; k \;\textrm{digits} ) \approx p\cdot10^{-k}.\]</div>
<p>The value <span class="math notranslate nohighlight">\(p\)</span> essentially encodes what happens with the first few
digits, and the <span class="math notranslate nohighlight">\(10^{-k}\)</span> handles the rest.</p>
<p>Notice that if we know the position accurate to <span class="math notranslate nohighlight">\(k=4\)</span> digits after
the decimal, that means we know the value falls within the interval say
<span class="math notranslate nohighlight">\([1.99995,2.00005]\)</span> which is an interval of length
<span class="math notranslate nohighlight">\(2.00005-1.99995 = 10^{-4}\)</span>. Thus, if we call the length of this
interval <span class="math notranslate nohighlight">\(\epsilon\)</span>, we can say</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-1">
<span class="eqno">(22.6.2)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-1" title="Permalink to this equation">¶</a></span>\[P(\textrm{distance is in an}\; \epsilon\textrm{-sized interval around}\; 2 ) \approx \epsilon \cdot p.\]</div>
<p>Let’s take this one final step further. We have been thinking about the
point <span class="math notranslate nohighlight">\(2\)</span> the entire time, but never thinking about other points.
Nothing is different there fundamentally, but it is the case that the
value <span class="math notranslate nohighlight">\(p\)</span> will likely be different. We would at least hope that a
dart thrower was more likely to hit a point near the center, like
<span class="math notranslate nohighlight">\(2\textrm{cm}\)</span> rather than <span class="math notranslate nohighlight">\(20\textrm{cm}\)</span>. Thus, the value
<span class="math notranslate nohighlight">\(p\)</span> is not fixed, but rather should depend on the point <span class="math notranslate nohighlight">\(x\)</span>.
This tells us that we should expect</p>
<div class="math notranslate nohighlight" id="equation-eq-pdf-deriv">
<span class="eqno">(22.6.3)<a class="headerlink" href="#equation-eq-pdf-deriv" title="Permalink to this equation">¶</a></span>\[P(\textrm{distance is in an}\; \epsilon \textrm{-sized interval around}\; x ) \approx \epsilon \cdot p(x).\]</div>
<p>Indeed, <a class="reference internal" href="#equation-eq-pdf-deriv">(22.6.3)</a> precisely defines the <em>probability
density function</em>. It is a function <span class="math notranslate nohighlight">\(p(x)\)</span> which encodes the
relative probability of hitting near one point vs. another. Let’s
visualize what such a function might look like.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-1-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-1-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-1-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-1-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">torch</span> <span class="k">as</span> <span class="n">d2l</span>

<span class="n">torch</span><span class="o">.</span><span class="n">pi</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">acos</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span>  <span class="c1"># Define pi in torch</span>

<span class="c1"># Plot the probability density function for some random variable</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.2</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span> <span class="o">+</span> \
    <span class="mf">0.8</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;Density&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-1-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">np</span><span class="p">,</span> <span class="n">npx</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">mxnet</span> <span class="k">as</span> <span class="n">d2l</span>

<span class="n">npx</span><span class="o">.</span><span class="n">set_np</span><span class="p">()</span>

<span class="c1"># Plot the probability density function for some random variable</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">+</span> \
    <span class="mf">0.8</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;Density&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-1-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">d2l</span>

<span class="n">tf</span><span class="o">.</span><span class="n">pi</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">acos</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span>  <span class="c1"># Define pi in TensorFlow</span>

<span class="c1"># Plot the probability density function for some random variable</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.2</span><span class="o">*</span><span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span> <span class="o">+</span> \
    <span class="mf">0.8</span><span class="o">*</span><span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;Density&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div></div><p>The locations where the function value is large indicates regions where
we are more likely to find the random value. The low portions are areas
where we are unlikely to find the random value.</p>
</div>
<div class="section" id="probability-density-functions">
<h3><span class="section-number">22.6.1.2. </span>Probability Density Functions<a class="headerlink" href="#probability-density-functions" title="Permalink to this heading">¶</a></h3>
<p>Let’s now investigate this further. We have already seen what a
probability density function is intuitively for a random variable
<span class="math notranslate nohighlight">\(X\)</span>, namely the density function is a function <span class="math notranslate nohighlight">\(p(x)\)</span> so
that</p>
<div class="math notranslate nohighlight" id="equation-eq-pdf-def">
<span class="eqno">(22.6.4)<a class="headerlink" href="#equation-eq-pdf-def" title="Permalink to this equation">¶</a></span>\[P(X \; \textrm{is in an}\; \epsilon \textrm{-sized interval around}\; x ) \approx \epsilon \cdot p(x).\]</div>
<p>But what does this imply for the properties of <span class="math notranslate nohighlight">\(p(x)\)</span>?</p>
<p>First, probabilities are never negative, thus we should expect that
<span class="math notranslate nohighlight">\(p(x) \ge 0\)</span> as well.</p>
<p>Second, let’s imagine that we slice up the <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> into an
infinite number of slices which are <span class="math notranslate nohighlight">\(\epsilon\)</span> wide, say with
slices <span class="math notranslate nohighlight">\((\epsilon\cdot i, \epsilon \cdot (i+1)]\)</span>. For each of
these, we know from <a class="reference internal" href="#equation-eq-pdf-def">(22.6.4)</a> the probability is
approximately</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-2">
<span class="eqno">(22.6.5)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-2" title="Permalink to this equation">¶</a></span>\[P(X \; \textrm{is in an}\; \epsilon\textrm{-sized interval around}\; x ) \approx \epsilon \cdot p(\epsilon \cdot i),\]</div>
<p>so summed over all of them it should be</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-3">
<span class="eqno">(22.6.6)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-3" title="Permalink to this equation">¶</a></span>\[P(X\in\mathbb{R}) \approx \sum_i \epsilon \cdot p(\epsilon\cdot i).\]</div>
<p>This is nothing more than the approximation of an integral discussed in
<a class="reference internal" href="integral-calculus.html#sec-integral-calculus"><span class="std std-numref">Section 22.5</span></a>, thus we can say that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-4">
<span class="eqno">(22.6.7)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-4" title="Permalink to this equation">¶</a></span>\[P(X\in\mathbb{R}) = \int_{-\infty}^{\infty} p(x) \; dx.\]</div>
<p>We know that <span class="math notranslate nohighlight">\(P(X\in\mathbb{R}) = 1\)</span>, since the random variable
must take on <em>some</em> number, we can conclude that for any density</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-5">
<span class="eqno">(22.6.8)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-5" title="Permalink to this equation">¶</a></span>\[\int_{-\infty}^{\infty} p(x) \; dx = 1.\]</div>
<p>Indeed, digging into this further shows that for any <span class="math notranslate nohighlight">\(a\)</span>, and
<span class="math notranslate nohighlight">\(b\)</span>, we see that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-6">
<span class="eqno">(22.6.9)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-6" title="Permalink to this equation">¶</a></span>\[P(X\in(a, b]) = \int _ {a}^{b} p(x) \; dx.\]</div>
<p>We may approximate this in code by using the same discrete approximation
methods as before. In this case we can approximate the probability of
falling in the blue region.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-3-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-3-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-3-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-3-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Approximate probability using numerical integration</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.2</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span> <span class="o">+</span>\
    <span class="mf">0.8</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">300</span><span class="p">:</span><span class="mi">800</span><span class="p">],</span> <span class="n">p</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">300</span><span class="p">:</span><span class="mi">800</span><span class="p">])</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="sa">f</span><span class="s1">&#39;approximate Probability: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">epsilon</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="mi">300</span><span class="p">:</span><span class="mi">800</span><span class="p">])</span><span class="si">}</span><span class="s1">&#39;</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-3-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Approximate probability using numerical integration</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">+</span> \
    <span class="mf">0.8</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">300</span><span class="p">:</span><span class="mi">800</span><span class="p">],</span> <span class="n">p</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">300</span><span class="p">:</span><span class="mi">800</span><span class="p">])</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="sa">f</span><span class="s1">&#39;approximate Probability: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">epsilon</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="mi">300</span><span class="p">:</span><span class="mi">800</span><span class="p">])</span><span class="si">}</span><span class="s1">&#39;</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-3-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Approximate probability using numerical integration</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.2</span><span class="o">*</span><span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span> <span class="o">+</span>\
    <span class="mf">0.8</span><span class="o">*</span><span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">300</span><span class="p">:</span><span class="mi">800</span><span class="p">],</span> <span class="n">p</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">300</span><span class="p">:</span><span class="mi">800</span><span class="p">])</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="sa">f</span><span class="s1">&#39;approximate Probability: </span><span class="si">{</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">epsilon</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="mi">300</span><span class="p">:</span><span class="mi">800</span><span class="p">])</span><span class="si">}</span><span class="s1">&#39;</span>
</pre></div>
</div>
</div></div><p>It turns out that these two properties describe exactly the space of
possible probability density functions (or <em>p.d.f.</em>’s for the commonly
encountered abbreviation). They are non-negative functions
<span class="math notranslate nohighlight">\(p(x) \ge 0\)</span> such that</p>
<div class="math notranslate nohighlight" id="equation-eq-pdf-int-one">
<span class="eqno">(22.6.10)<a class="headerlink" href="#equation-eq-pdf-int-one" title="Permalink to this equation">¶</a></span>\[\int_{-\infty}^{\infty} p(x) \; dx = 1.\]</div>
<p>We interpret this function by using integration to obtain the
probability our random variable is in a specific interval:</p>
<div class="math notranslate nohighlight" id="equation-eq-pdf-int-int">
<span class="eqno">(22.6.11)<a class="headerlink" href="#equation-eq-pdf-int-int" title="Permalink to this equation">¶</a></span>\[P(X\in(a, b]) = \int _ {a}^{b} p(x) \; dx.\]</div>
<p>In <a class="reference internal" href="distributions.html#sec-distributions"><span class="std std-numref">Section 22.8</span></a> we will see a number of common
distributions, but let’s continue working in the abstract.</p>
</div>
<div class="section" id="cumulative-distribution-functions">
<h3><span class="section-number">22.6.1.3. </span>Cumulative Distribution Functions<a class="headerlink" href="#cumulative-distribution-functions" title="Permalink to this heading">¶</a></h3>
<p>In the previous section, we saw the notion of the p.d.f. In practice,
this is a commonly encountered method to discuss continuous random
variables, but it has one significant pitfall: that the values of the
p.d.f. are not themselves probabilities, but rather a function that we
must integrate to yield probabilities. There is nothing wrong with a
density being larger than <span class="math notranslate nohighlight">\(10\)</span>, as long as it is not larger than
<span class="math notranslate nohighlight">\(10\)</span> for more than an interval of length <span class="math notranslate nohighlight">\(1/10\)</span>. This can be
counter-intuitive, so people often also think in terms of the
<em>cumulative distribution function</em>, or c.d.f., which <em>is</em> a probability.</p>
<p>In particular, by using <a class="reference internal" href="#equation-eq-pdf-int-int">(22.6.11)</a>, we define the c.d.f.
for a random variable <span class="math notranslate nohighlight">\(X\)</span> with density <span class="math notranslate nohighlight">\(p(x)\)</span> by</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-7">
<span class="eqno">(22.6.12)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-7" title="Permalink to this equation">¶</a></span>\[F(x) = \int _ {-\infty}^{x} p(x) \; dx = P(X \le x).\]</div>
<p>Let’s observe a few properties.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(F(x) \rightarrow 0\)</span> as <span class="math notranslate nohighlight">\(x\rightarrow -\infty\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(F(x) \rightarrow 1\)</span> as <span class="math notranslate nohighlight">\(x\rightarrow \infty\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(F(x)\)</span> is non-decreasing
(<span class="math notranslate nohighlight">\(y &gt; x \implies F(y) \ge F(x)\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(F(x)\)</span> is continuous (has no jumps) if <span class="math notranslate nohighlight">\(X\)</span> is a
continuous random variable.</p></li>
</ul>
<p>With the fourth bullet point, note that this would not be true if
<span class="math notranslate nohighlight">\(X\)</span> were discrete, say taking the values <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>
both with probability <span class="math notranslate nohighlight">\(1/2\)</span>. In that case</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-8">
<span class="eqno">(22.6.13)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-8" title="Permalink to this equation">¶</a></span>\[\begin{split}F(x) = \begin{cases}
0 &amp; x &lt; 0, \\
\frac{1}{2} &amp; x &lt; 1, \\
1 &amp; x \ge 1.
\end{cases}\end{split}\]</div>
<p>In this example, we see one of the benefits of working with the c.d.f.,
the ability to deal with continuous or discrete random variables in the
same framework, or indeed mixtures of the two (flip a coin: if heads
return the roll of a die, if tails return the distance of a dart throw
from the center of a dart board).</p>
</div>
<div class="section" id="means">
<h3><span class="section-number">22.6.1.4. </span>Means<a class="headerlink" href="#means" title="Permalink to this heading">¶</a></h3>
<p>Suppose that we are dealing with a random variables <span class="math notranslate nohighlight">\(X\)</span>. The
distribution itself can be hard to interpret. It is often useful to be
able to summarize the behavior of a random variable concisely. Numbers
that help us capture the behavior of a random variable are called
<em>summary statistics</em>. The most commonly encountered ones are the <em>mean</em>,
the <em>variance</em>, and the <em>standard deviation</em>.</p>
<p>The <em>mean</em> encodes the average value of a random variable. If we have a
discrete random variable <span class="math notranslate nohighlight">\(X\)</span>, which takes the values <span class="math notranslate nohighlight">\(x_i\)</span>
with probabilities <span class="math notranslate nohighlight">\(p_i\)</span>, then the mean is given by the weighted
average: sum the values times the probability that the random variable
takes on that value:</p>
<div class="math notranslate nohighlight" id="equation-eq-exp-def">
<span class="eqno">(22.6.14)<a class="headerlink" href="#equation-eq-exp-def" title="Permalink to this equation">¶</a></span>\[\mu_X = E[X] = \sum_i x_i p_i.\]</div>
<p>The way we should interpret the mean (albeit with caution) is that it
tells us essentially where the random variable tends to be located.</p>
<p>As a minimalistic example that we will examine throughout this section,
let’s take <span class="math notranslate nohighlight">\(X\)</span> to be the random variable which takes the value
<span class="math notranslate nohighlight">\(a-2\)</span> with probability <span class="math notranslate nohighlight">\(p\)</span>, <span class="math notranslate nohighlight">\(a+2\)</span> with probability
<span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(a\)</span> with probability <span class="math notranslate nohighlight">\(1-2p\)</span>. We can compute
using <a class="reference internal" href="#equation-eq-exp-def">(22.6.14)</a> that, for any possible choice of <span class="math notranslate nohighlight">\(a\)</span>
and <span class="math notranslate nohighlight">\(p\)</span>, the mean is</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-9">
<span class="eqno">(22.6.15)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-9" title="Permalink to this equation">¶</a></span>\[\mu_X = E[X] = \sum_i x_i p_i = (a-2)p + a(1-2p) + (a+2)p = a.\]</div>
<p>Thus we see that the mean is <span class="math notranslate nohighlight">\(a\)</span>. This matches the intuition since
<span class="math notranslate nohighlight">\(a\)</span> is the location around which we centered our random variable.</p>
<p>Because they are helpful, let’s summarize a few properties.</p>
<ul class="simple">
<li><p>For any random variable <span class="math notranslate nohighlight">\(X\)</span> and numbers <span class="math notranslate nohighlight">\(a\)</span> and
<span class="math notranslate nohighlight">\(b\)</span>, we have that <span class="math notranslate nohighlight">\(\mu_{aX+b} = a\mu_X + b\)</span>.</p></li>
<li><p>If we have two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, we have
<span class="math notranslate nohighlight">\(\mu_{X+Y} = \mu_X+\mu_Y\)</span>.</p></li>
</ul>
<p>Means are useful for understanding the average behavior of a random
variable, however the mean is not sufficient to even have a full
intuitive understanding. Making a profit of <span class="math notranslate nohighlight">\(\$10 \pm \$1\)</span> per
sale is very different from making <span class="math notranslate nohighlight">\(\$10 \pm \$15\)</span> per sale
despite having the same average value. The second one has a much larger
degree of fluctuation, and thus represents a much larger risk. Thus, to
understand the behavior of a random variable, we will need at minimum
one more measure: some measure of how widely a random variable
fluctuates.</p>
</div>
<div class="section" id="variances">
<h3><span class="section-number">22.6.1.5. </span>Variances<a class="headerlink" href="#variances" title="Permalink to this heading">¶</a></h3>
<p>This leads us to consider the <em>variance</em> of a random variable. This is a
quantitative measure of how far a random variable deviates from the
mean. Consider the expression <span class="math notranslate nohighlight">\(X - \mu_X\)</span>. This is the deviation
of the random variable from its mean. This value can be positive or
negative, so we need to do something to make it positive so that we are
measuring the magnitude of the deviation.</p>
<p>A reasonable thing to try is to look at <span class="math notranslate nohighlight">\(\left|X-\mu_X\right|\)</span>,
and indeed this leads to a useful quantity called the <em>mean absolute
deviation</em>, however due to connections with other areas of mathematics
and statistics, people often use a different solution.</p>
<p>In particular, they look at <span class="math notranslate nohighlight">\((X-\mu_X)^2.\)</span> If we look at the
typical size of this quantity by taking the mean, we arrive at the
variance</p>
<div class="math notranslate nohighlight" id="equation-eq-var-def">
<span class="eqno">(22.6.16)<a class="headerlink" href="#equation-eq-var-def" title="Permalink to this equation">¶</a></span>\[\sigma_X^2 = \textrm{Var}(X) = E\left[(X-\mu_X)^2\right] = E[X^2] - \mu_X^2.\]</div>
<p>The last equality in <a class="reference internal" href="#equation-eq-var-def">(22.6.16)</a> holds by expanding out the
definition in the middle, and applying the properties of expectation.</p>
<p>Let’s look at our example where <span class="math notranslate nohighlight">\(X\)</span> is the random variable which
takes the value <span class="math notranslate nohighlight">\(a-2\)</span> with probability <span class="math notranslate nohighlight">\(p\)</span>, <span class="math notranslate nohighlight">\(a+2\)</span> with
probability <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(a\)</span> with probability <span class="math notranslate nohighlight">\(1-2p\)</span>. In
this case <span class="math notranslate nohighlight">\(\mu_X = a\)</span>, so all we need to compute is
<span class="math notranslate nohighlight">\(E\left[X^2\right]\)</span>. This can readily be done:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-10">
<span class="eqno">(22.6.17)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-10" title="Permalink to this equation">¶</a></span>\[E\left[X^2\right] = (a-2)^2p + a^2(1-2p) + (a+2)^2p = a^2 + 8p.\]</div>
<p>Thus, we see that by <a class="reference internal" href="#equation-eq-var-def">(22.6.16)</a> our variance is</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-11">
<span class="eqno">(22.6.18)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-11" title="Permalink to this equation">¶</a></span>\[\sigma_X^2 = \textrm{Var}(X) = E[X^2] - \mu_X^2 = a^2 + 8p - a^2 = 8p.\]</div>
<p>This result again makes sense. The largest <span class="math notranslate nohighlight">\(p\)</span> can be is
<span class="math notranslate nohighlight">\(1/2\)</span> which corresponds to picking <span class="math notranslate nohighlight">\(a-2\)</span> or <span class="math notranslate nohighlight">\(a+2\)</span> with
a coin flip. The variance of this being <span class="math notranslate nohighlight">\(4\)</span> corresponds to the
fact that both <span class="math notranslate nohighlight">\(a-2\)</span> and <span class="math notranslate nohighlight">\(a+2\)</span> are <span class="math notranslate nohighlight">\(2\)</span> units away from
the mean, and <span class="math notranslate nohighlight">\(2^2 = 4\)</span>. On the other end of the spectrum, if
<span class="math notranslate nohighlight">\(p=0\)</span>, this random variable always takes the value <span class="math notranslate nohighlight">\(0\)</span> and
so it has no variance at all.</p>
<p>We will list a few properties of variance below:</p>
<ul class="simple">
<li><p>For any random variable <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(\textrm{Var}(X) \ge 0\)</span>,
with <span class="math notranslate nohighlight">\(\textrm{Var}(X) = 0\)</span> if and only if <span class="math notranslate nohighlight">\(X\)</span> is a
constant.</p></li>
<li><p>For any random variable <span class="math notranslate nohighlight">\(X\)</span> and numbers <span class="math notranslate nohighlight">\(a\)</span> and
<span class="math notranslate nohighlight">\(b\)</span>, we have that
<span class="math notranslate nohighlight">\(\textrm{Var}(aX+b) = a^2\textrm{Var}(X)\)</span>.</p></li>
<li><p>If we have two <em>independent</em> random variables <span class="math notranslate nohighlight">\(X\)</span> and
<span class="math notranslate nohighlight">\(Y\)</span>, we have
<span class="math notranslate nohighlight">\(\textrm{Var}(X+Y) = \textrm{Var}(X) + \textrm{Var}(Y)\)</span>.</p></li>
</ul>
<p>When interpreting these values, there can be a bit of a hiccup. In
particular, let’s try imagining what happens if we keep track of units
through this computation. Suppose that we are working with the star
rating assigned to a product on the web page. Then <span class="math notranslate nohighlight">\(a\)</span>,
<span class="math notranslate nohighlight">\(a-2\)</span>, and <span class="math notranslate nohighlight">\(a+2\)</span> are all measured in units of stars.
Similarly, the mean <span class="math notranslate nohighlight">\(\mu_X\)</span> is then also measured in stars (being
a weighted average). However, if we get to the variance, we immediately
encounter an issue, which is we want to look at <span class="math notranslate nohighlight">\((X-\mu_X)^2\)</span>,
which is in units of <em>squared stars</em>. This means that the variance
itself is not comparable to the original measurements. To make it
interpretable, we will need to return to our original units.</p>
</div>
<div class="section" id="standard-deviations">
<h3><span class="section-number">22.6.1.6. </span>Standard Deviations<a class="headerlink" href="#standard-deviations" title="Permalink to this heading">¶</a></h3>
<p>This summary statistics can always be deduced from the variance by
taking the square root! Thus we define the <em>standard deviation</em> to be</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-12">
<span class="eqno">(22.6.19)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-12" title="Permalink to this equation">¶</a></span>\[\sigma_X = \sqrt{\textrm{Var}(X)}.\]</div>
<p>In our example, this means we now have the standard deviation is
<span class="math notranslate nohighlight">\(\sigma_X = 2\sqrt{2p}\)</span>. If we are dealing with units of stars for
our review example, <span class="math notranslate nohighlight">\(\sigma_X\)</span> is again in units of stars.</p>
<p>The properties we had for the variance can be restated for the standard
deviation.</p>
<ul class="simple">
<li><p>For any random variable <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(\sigma_{X} \ge 0\)</span>.</p></li>
<li><p>For any random variable <span class="math notranslate nohighlight">\(X\)</span> and numbers <span class="math notranslate nohighlight">\(a\)</span> and
<span class="math notranslate nohighlight">\(b\)</span>, we have that <span class="math notranslate nohighlight">\(\sigma_{aX+b} = |a|\sigma_{X}\)</span></p></li>
<li><p>If we have two <em>independent</em> random variables <span class="math notranslate nohighlight">\(X\)</span> and
<span class="math notranslate nohighlight">\(Y\)</span>, we have
<span class="math notranslate nohighlight">\(\sigma_{X+Y} = \sqrt{\sigma_{X}^2 + \sigma_{Y}^2}\)</span>.</p></li>
</ul>
<p>It is natural at this moment to ask, “If the standard deviation is in
the units of our original random variable, does it represent something
we can draw with regards to that random variable?” The answer is a
resounding yes! Indeed much like the mean told us the typical location
of our random variable, the standard deviation gives the typical range
of variation of that random variable. We can make this rigorous with
what is known as Chebyshev’s inequality:</p>
<div class="math notranslate nohighlight" id="equation-eq-chebyshev">
<span class="eqno">(22.6.20)<a class="headerlink" href="#equation-eq-chebyshev" title="Permalink to this equation">¶</a></span>\[P\left(X \not\in [\mu_X - \alpha\sigma_X, \mu_X + \alpha\sigma_X]\right) \le \frac{1}{\alpha^2}.\]</div>
<p>Or to state it verbally in the case of <span class="math notranslate nohighlight">\(\alpha=10\)</span>, <span class="math notranslate nohighlight">\(99\%\)</span>
of the samples from any random variable fall within <span class="math notranslate nohighlight">\(10\)</span> standard
deviations of the mean. This gives an immediate interpretation to our
standard summary statistics.</p>
<p>To see how this statement is rather subtle, let’s take a look at our
running example again where <span class="math notranslate nohighlight">\(X\)</span> is the random variable which takes
the value <span class="math notranslate nohighlight">\(a-2\)</span> with probability <span class="math notranslate nohighlight">\(p\)</span>, <span class="math notranslate nohighlight">\(a+2\)</span> with
probability <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(a\)</span> with probability <span class="math notranslate nohighlight">\(1-2p\)</span>. We
saw that the mean was <span class="math notranslate nohighlight">\(a\)</span> and the standard deviation was
<span class="math notranslate nohighlight">\(2\sqrt{2p}\)</span>. This means, if we take Chebyshev’s inequality
<a class="reference internal" href="#equation-eq-chebyshev">(22.6.20)</a> with <span class="math notranslate nohighlight">\(\alpha = 2\)</span>, we see that the
expression is</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-13">
<span class="eqno">(22.6.21)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-13" title="Permalink to this equation">¶</a></span>\[P\left(X \not\in [a - 4\sqrt{2p}, a + 4\sqrt{2p}]\right) \le \frac{1}{4}.\]</div>
<p>This means that <span class="math notranslate nohighlight">\(75\%\)</span> of the time, this random variable will fall
within this interval for any value of <span class="math notranslate nohighlight">\(p\)</span>. Now, notice that as
<span class="math notranslate nohighlight">\(p \rightarrow 0\)</span>, this interval also converges to the single
point <span class="math notranslate nohighlight">\(a\)</span>. But we know that our random variable takes the values
<span class="math notranslate nohighlight">\(a-2, a\)</span>, and <span class="math notranslate nohighlight">\(a+2\)</span> only so eventually we can be certain
<span class="math notranslate nohighlight">\(a-2\)</span> and <span class="math notranslate nohighlight">\(a+2\)</span> will fall outside the interval! The question
is, at what <span class="math notranslate nohighlight">\(p\)</span> does that happen. So we want to solve: for what
<span class="math notranslate nohighlight">\(p\)</span> does <span class="math notranslate nohighlight">\(a+4\sqrt{2p} = a+2\)</span>, which is solved when
<span class="math notranslate nohighlight">\(p=1/8\)</span>, which is <em>exactly</em> the first <span class="math notranslate nohighlight">\(p\)</span> where it could
possibly happen without violating our claim that no more than
<span class="math notranslate nohighlight">\(1/4\)</span> of samples from the distribution would fall outside the
interval (<span class="math notranslate nohighlight">\(1/8\)</span> to the left, and <span class="math notranslate nohighlight">\(1/8\)</span> to the right).</p>
<p>Let’s visualize this. We will show the probability of getting the three
values as three vertical bars with height proportional to the
probability. The interval will be drawn as a horizontal line in the
middle. The first plot shows what happens for <span class="math notranslate nohighlight">\(p &gt; 1/8\)</span> where the
interval safely contains all points.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-5-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-5-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-5-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-5-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a helper to plot these figures</span>
<span class="k">def</span> <span class="nf">plot_chebyshev</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">stem</span><span class="p">([</span><span class="n">a</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="o">+</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">],</span> <span class="n">use_line_collection</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;p.m.f.&#39;</span><span class="p">)</span>

    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">a</span> <span class="o">-</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">p</span><span class="p">),</span>
                   <span class="n">a</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">p</span><span class="p">),</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">p</span><span class="p">),</span> <span class="mf">0.53</span><span class="p">,</span> <span class="mf">0.47</span><span class="p">,</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">p</span><span class="p">),</span> <span class="mf">0.53</span><span class="p">,</span> <span class="mf">0.47</span><span class="p">,</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;p = </span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Plot interval when p &gt; 1/8</span>
<span class="n">plot_chebyshev</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-5-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a helper to plot these figures</span>
<span class="k">def</span> <span class="nf">plot_chebyshev</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">stem</span><span class="p">([</span><span class="n">a</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="o">+</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">],</span> <span class="n">use_line_collection</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;p.m.f.&#39;</span><span class="p">)</span>

    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">a</span> <span class="o">-</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">p</span><span class="p">),</span>
                   <span class="n">a</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">p</span><span class="p">),</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">p</span><span class="p">),</span> <span class="mf">0.53</span><span class="p">,</span> <span class="mf">0.47</span><span class="p">,</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">p</span><span class="p">),</span> <span class="mf">0.53</span><span class="p">,</span> <span class="mf">0.47</span><span class="p">,</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;p = </span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Plot interval when p &gt; 1/8</span>
<span class="n">plot_chebyshev</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-5-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a helper to plot these figures</span>
<span class="k">def</span> <span class="nf">plot_chebyshev</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">stem</span><span class="p">([</span><span class="n">a</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="o">+</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span><span class="p">],</span> <span class="n">use_line_collection</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;p.m.f.&#39;</span><span class="p">)</span>

    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">a</span> <span class="o">-</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">p</span><span class="p">),</span>
                   <span class="n">a</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">p</span><span class="p">),</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">p</span><span class="p">),</span> <span class="mf">0.53</span><span class="p">,</span> <span class="mf">0.47</span><span class="p">,</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">p</span><span class="p">),</span> <span class="mf">0.53</span><span class="p">,</span> <span class="mf">0.47</span><span class="p">,</span> <span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;p = </span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s1">.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Plot interval when p &gt; 1/8</span>
<span class="n">plot_chebyshev</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>
</pre></div>
</div>
</div></div><p>The second shows that at <span class="math notranslate nohighlight">\(p = 1/8\)</span>, the interval exactly touches
the two points. This shows that the inequality is <em>sharp</em>, since no
smaller interval could be taken while keeping the inequality true.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-7-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-7-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-7-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-7-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot interval when p = 1/8</span>
<span class="n">plot_chebyshev</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.125</span><span class="p">))</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-7-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot interval when p = 1/8</span>
<span class="n">plot_chebyshev</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.125</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-7-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot interval when p = 1/8</span>
<span class="n">plot_chebyshev</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.125</span><span class="p">))</span>
</pre></div>
</div>
</div></div><p>The third shows that for <span class="math notranslate nohighlight">\(p &lt; 1/8\)</span> the interval only contains the
center. This does not invalidate the inequality since we only needed to
ensure that no more than <span class="math notranslate nohighlight">\(1/4\)</span> of the probability falls outside
the interval, which means that once <span class="math notranslate nohighlight">\(p &lt; 1/8\)</span>, the two points at
<span class="math notranslate nohighlight">\(a-2\)</span> and <span class="math notranslate nohighlight">\(a+2\)</span> can be discarded.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-9-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-9-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-9-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-9-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot interval when p &lt; 1/8</span>
<span class="n">plot_chebyshev</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.05</span><span class="p">))</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-9-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot interval when p &lt; 1/8</span>
<span class="n">plot_chebyshev</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-9-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot interval when p &lt; 1/8</span>
<span class="n">plot_chebyshev</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.05</span><span class="p">))</span>
</pre></div>
</div>
</div></div></div>
<div class="section" id="means-and-variances-in-the-continuum">
<h3><span class="section-number">22.6.1.7. </span>Means and Variances in the Continuum<a class="headerlink" href="#means-and-variances-in-the-continuum" title="Permalink to this heading">¶</a></h3>
<p>This has all been in terms of discrete random variables, but the case of
continuous random variables is similar. To intuitively understand how
this works, imagine that we split the real number line into intervals of
length <span class="math notranslate nohighlight">\(\epsilon\)</span> given by <span class="math notranslate nohighlight">\((\epsilon i, \epsilon (i+1)]\)</span>.
Once we do this, our continuous random variable has been made discrete
and we can use <a class="reference internal" href="#equation-eq-exp-def">(22.6.14)</a> say that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-14">
<span class="eqno">(22.6.22)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-14" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\mu_X &amp; \approx \sum_{i} (\epsilon i)P(X \in (\epsilon i, \epsilon (i+1)]) \\
&amp; \approx \sum_{i} (\epsilon i)p_X(\epsilon i)\epsilon, \\
\end{aligned}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(p_X\)</span> is the density of <span class="math notranslate nohighlight">\(X\)</span>. This is an approximation
to the integral of <span class="math notranslate nohighlight">\(xp_X(x)\)</span>, so we can conclude that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-15">
<span class="eqno">(22.6.23)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-15" title="Permalink to this equation">¶</a></span>\[\mu_X = \int_{-\infty}^\infty xp_X(x) \; dx.\]</div>
<p>Similarly, using <a class="reference internal" href="#equation-eq-var-def">(22.6.16)</a> the variance can be written as</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-16">
<span class="eqno">(22.6.24)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-16" title="Permalink to this equation">¶</a></span>\[\sigma^2_X = E[X^2] - \mu_X^2 = \int_{-\infty}^\infty x^2p_X(x) \; dx - \left(\int_{-\infty}^\infty xp_X(x) \; dx\right)^2.\]</div>
<p>Everything stated above about the mean, the variance, and the standard
deviation still applies in this case. For instance, if we consider the
random variable with density</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-17">
<span class="eqno">(22.6.25)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-17" title="Permalink to this equation">¶</a></span>\[\begin{split}p(x) = \begin{cases}
1 &amp; x \in [0,1], \\
0 &amp; \textrm{otherwise}.
\end{cases}\end{split}\]</div>
<p>we can compute</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-18">
<span class="eqno">(22.6.26)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-18" title="Permalink to this equation">¶</a></span>\[\mu_X = \int_{-\infty}^\infty xp(x) \; dx = \int_0^1 x \; dx = \frac{1}{2}.\]</div>
<p>and</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-19">
<span class="eqno">(22.6.27)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-19" title="Permalink to this equation">¶</a></span>\[\sigma_X^2 = \int_{-\infty}^\infty x^2p(x) \; dx - \left(\frac{1}{2}\right)^2 = \frac{1}{3} - \frac{1}{4} = \frac{1}{12}.\]</div>
<p>As a warning, let’s examine one more example, known as the <em>Cauchy
distribution</em>. This is the distribution with p.d.f. given by</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-20">
<span class="eqno">(22.6.28)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-20" title="Permalink to this equation">¶</a></span>\[p(x) = \frac{1}{1+x^2}.\]</div>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-11-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-11-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-11-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-11-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the Cauchy distribution p.d.f.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;p.d.f.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-11-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the Cauchy distribution p.d.f.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;p.d.f.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-11-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the Cauchy distribution p.d.f.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;p.d.f.&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div></div><p>This function looks innocent, and indeed consulting a table of integrals
will show it has area one under it, and thus it defines a continuous
random variable.</p>
<p>To see what goes astray, let’s try to compute the variance of this. This
would involve using <a class="reference internal" href="#equation-eq-var-def">(22.6.16)</a> computing</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-21">
<span class="eqno">(22.6.29)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-21" title="Permalink to this equation">¶</a></span>\[\int_{-\infty}^\infty \frac{x^2}{1+x^2}\; dx.\]</div>
<p>The function on the inside looks like this:</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-13-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-13-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-13-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-13-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the integrand needed to compute the variance</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;integrand&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-13-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the integrand needed to compute the variance</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;integrand&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-13-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the integrand needed to compute the variance</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;integrand&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div></div><p>This function clearly has infinite area under it since it is essentially
the constant one with a small dip near zero, and indeed we could show
that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-22">
<span class="eqno">(22.6.30)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-22" title="Permalink to this equation">¶</a></span>\[\int_{-\infty}^\infty \frac{x^2}{1+x^2}\; dx = \infty.\]</div>
<p>This means it does not have a well-defined finite variance.</p>
<p>However, looking deeper shows an even more disturbing result. Let’s try
to compute the mean using <a class="reference internal" href="#equation-eq-exp-def">(22.6.14)</a>. Using the change of
variables formula, we see</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-23">
<span class="eqno">(22.6.31)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-23" title="Permalink to this equation">¶</a></span>\[\mu_X = \int_{-\infty}^{\infty} \frac{x}{1+x^2} \; dx = \frac{1}{2}\int_1^\infty \frac{1}{u} \; du.\]</div>
<p>The integral inside is the definition of the logarithm, so this is in
essence <span class="math notranslate nohighlight">\(\log(\infty) = \infty\)</span>, so there is no well-defined
average value either!</p>
<p>Machine learning scientists define their models so that we most often do
not need to deal with these issues, and will in the vast majority of
cases deal with random variables with well-defined means and variances.
However, every so often random variables with <em>heavy tails</em> (that is
those random variables where the probabilities of getting large values
are large enough to make things like the mean or variance undefined) are
helpful in modeling physical systems, thus it is worth knowing that they
exist.</p>
</div>
<div class="section" id="joint-density-functions">
<h3><span class="section-number">22.6.1.8. </span>Joint Density Functions<a class="headerlink" href="#joint-density-functions" title="Permalink to this heading">¶</a></h3>
<p>The above work all assumes we are working with a single real valued
random variable. But what if we are dealing with two or more potentially
highly correlated random variables? This circumstance is the norm in
machine learning: imagine random variables like <span class="math notranslate nohighlight">\(R_{i, j}\)</span> which
encode the red value of the pixel at the <span class="math notranslate nohighlight">\((i, j)\)</span> coordinate in an
image, or <span class="math notranslate nohighlight">\(P_t\)</span> which is a random variable given by a stock price
at time <span class="math notranslate nohighlight">\(t\)</span>. Nearby pixels tend to have similar color, and nearby
times tend to have similar prices. We cannot treat them as separate
random variables, and expect to create a successful model (we will see
in <a class="reference internal" href="naive-bayes.html#sec-naive-bayes"><span class="std std-numref">Section 22.9</span></a> a model that under-performs due to such
an assumption). We need to develop the mathematical language to handle
these correlated continuous random variables.</p>
<p>Thankfully, with the multiple integrals in
<a class="reference internal" href="integral-calculus.html#sec-integral-calculus"><span class="std std-numref">Section 22.5</span></a> we can develop such a language.
Suppose that we have, for simplicity, two random variables <span class="math notranslate nohighlight">\(X, Y\)</span>
which can be correlated. Then, similar to the case of a single variable,
we can ask the question:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-24">
<span class="eqno">(22.6.32)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-24" title="Permalink to this equation">¶</a></span>\[P(X \;\textrm{is in an}\; \epsilon \textrm{-sized interval around}\; x \; \textrm{and} \;Y \;\textrm{is in an}\; \epsilon \textrm{-sized interval around}\; y ).\]</div>
<p>Similar reasoning to the single variable case shows that this should be
approximately</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-25">
<span class="eqno">(22.6.33)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-25" title="Permalink to this equation">¶</a></span>\[P(X \;\textrm{is in an}\; \epsilon \textrm{-sized interval around}\; x \; \textrm{and} \;Y \;\textrm{is in an}\; \epsilon \textrm{-sized interval around}\; y ) \approx \epsilon^{2}p(x, y),\]</div>
<p>for some function <span class="math notranslate nohighlight">\(p(x, y)\)</span>. This is referred to as the joint
density of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. Similar properties are true for this
as we saw in the single variable case. Namely:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p(x, y) \ge 0\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\int _ {\mathbb{R}^2} p(x, y) \;dx \;dy = 1\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(P((X, Y) \in \mathcal{D}) = \int _ {\mathcal{D}} p(x, y) \;dx \;dy\)</span>.</p></li>
</ul>
<p>In this way, we can deal with multiple, potentially correlated random
variables. If we wish to work with more than two random variables, we
can extend the multivariate density to as many coordinates as desired by
considering <span class="math notranslate nohighlight">\(p(\mathbf{x}) = p(x_1, \ldots, x_n)\)</span>. The same
properties of being non-negative, and having total integral of one still
hold.</p>
</div>
<div class="section" id="marginal-distributions">
<h3><span class="section-number">22.6.1.9. </span>Marginal Distributions<a class="headerlink" href="#marginal-distributions" title="Permalink to this heading">¶</a></h3>
<p>When dealing with multiple variables, we oftentimes want to be able to
ignore the relationships and ask, “how is this one variable
distributed?” Such a distribution is called a <em>marginal distribution</em>.</p>
<p>To be concrete, let’s suppose that we have two random variables
<span class="math notranslate nohighlight">\(X, Y\)</span> with joint density given by <span class="math notranslate nohighlight">\(p _ {X, Y}(x, y)\)</span>. We
will be using the subscript to indicate what random variables the
density is for. The question of finding the marginal distribution is
taking this function, and using it to find <span class="math notranslate nohighlight">\(p _ X(x)\)</span>.</p>
<p>As with most things, it is best to return to the intuitive picture to
figure out what should be true. Recall that the density is the function
<span class="math notranslate nohighlight">\(p _ X\)</span> so that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-26">
<span class="eqno">(22.6.34)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-26" title="Permalink to this equation">¶</a></span>\[P(X \in [x, x+\epsilon]) \approx \epsilon \cdot p _ X(x).\]</div>
<p>There is no mention of <span class="math notranslate nohighlight">\(Y\)</span>, but if all we are given is
<span class="math notranslate nohighlight">\(p _{X, Y}\)</span>, we need to include <span class="math notranslate nohighlight">\(Y\)</span> somehow. We can first
observe that this is the same as</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-27">
<span class="eqno">(22.6.35)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-27" title="Permalink to this equation">¶</a></span>\[P(X \in [x, x+\epsilon] \textrm{, and } Y \in \mathbb{R}) \approx \epsilon \cdot p _ X(x).\]</div>
<p>Our density does not directly tell us about what happens in this case,
we need to split into small intervals in <span class="math notranslate nohighlight">\(y\)</span> as well, so we can
write this as</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-28">
<span class="eqno">(22.6.36)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-28" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\epsilon \cdot p _ X(x) &amp; \approx \sum _ {i} P(X \in [x, x+\epsilon] \textrm{, and } Y \in [\epsilon \cdot i, \epsilon \cdot (i+1)]) \\
&amp; \approx \sum _ {i} \epsilon^{2} p _ {X, Y}(x, \epsilon\cdot i).
\end{aligned}\end{split}\]</div>
<div class="figure align-default" id="id1">
<span id="fig-marginal"></span><img alt="../_images/marginal.svg" src="../_images/marginal.svg" /><p class="caption"><span class="caption-number">Fig. 22.6.1 </span><span class="caption-text">By summing along the columns of our array of probabilities, we are
able to obtain the marginal distribution for just the random variable
represented along the <span class="math notranslate nohighlight">\(\mathit{x}\)</span>-axis.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p>This tells us to add up the value of the density along a series of
squares in a line as is shown in <a class="reference internal" href="#fig-marginal"><span class="std std-numref">Fig. 22.6.1</span></a>. Indeed, after
canceling one factor of epsilon from both sides, and recognizing the sum
on the right is the integral over <span class="math notranslate nohighlight">\(y\)</span>, we can conclude that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-29">
<span class="eqno">(22.6.37)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-29" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
 p _ X(x) &amp;  \approx \sum _ {i} \epsilon p _ {X, Y}(x, \epsilon\cdot i) \\
 &amp; \approx \int_{-\infty}^\infty p_{X, Y}(x, y) \; dy.
\end{aligned}\end{split}\]</div>
<p>Thus we see</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-30">
<span class="eqno">(22.6.38)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-30" title="Permalink to this equation">¶</a></span>\[p _ X(x) = \int_{-\infty}^\infty p_{X, Y}(x, y) \; dy.\]</div>
<p>This tells us that to get a marginal distribution, we integrate over the
variables we do not care about. This process is often referred to as
<em>integrating out</em> or <em>marginalized out</em> the unneeded variables.</p>
</div>
<div class="section" id="covariance">
<h3><span class="section-number">22.6.1.10. </span>Covariance<a class="headerlink" href="#covariance" title="Permalink to this heading">¶</a></h3>
<p>When dealing with multiple random variables, there is one additional
summary statistic which is helpful to know: the <em>covariance</em>. This
measures the degree that two random variable fluctuate together.</p>
<p>Suppose that we have two random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, to
begin with, let’s suppose they are discrete, taking on values
<span class="math notranslate nohighlight">\((x_i, y_j)\)</span> with probability <span class="math notranslate nohighlight">\(p_{ij}\)</span>. In this case, the
covariance is defined as</p>
<div class="math notranslate nohighlight" id="equation-eq-cov-def">
<span class="eqno">(22.6.39)<a class="headerlink" href="#equation-eq-cov-def" title="Permalink to this equation">¶</a></span>\[\sigma_{XY} = \textrm{Cov}(X, Y) = \sum_{i, j} (x_i - \mu_X) (y_j-\mu_Y) p_{ij}. = E[XY] - E[X]E[Y].\]</div>
<p>To think about this intuitively: consider the following pair of random
variables. Suppose that <span class="math notranslate nohighlight">\(X\)</span> takes the values <span class="math notranslate nohighlight">\(1\)</span> and
<span class="math notranslate nohighlight">\(3\)</span>, and <span class="math notranslate nohighlight">\(Y\)</span> takes the values <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(3\)</span>.
Suppose that we have the following probabilities</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-31">
<span class="eqno">(22.6.40)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-31" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
P(X = 1 \; \textrm{and} \; Y = -1) &amp; = \frac{p}{2}, \\
P(X = 1 \; \textrm{and} \; Y = 3) &amp; = \frac{1-p}{2}, \\
P(X = 3 \; \textrm{and} \; Y = -1) &amp; = \frac{1-p}{2}, \\
P(X = 3 \; \textrm{and} \; Y = 3) &amp; = \frac{p}{2},
\end{aligned}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(p\)</span> is a parameter in <span class="math notranslate nohighlight">\([0,1]\)</span> we get to pick. Notice
that if <span class="math notranslate nohighlight">\(p=1\)</span> then they are both always their minimum or maximum
values simultaneously, and if <span class="math notranslate nohighlight">\(p=0\)</span> they are guaranteed to take
their flipped values simultaneously (one is large when the other is
small and vice versa). If <span class="math notranslate nohighlight">\(p=1/2\)</span>, then the four possibilities are
all equally likely, and neither should be related. Let’s compute the
covariance. First, note <span class="math notranslate nohighlight">\(\mu_X = 2\)</span> and <span class="math notranslate nohighlight">\(\mu_Y = 1\)</span>, so we
may compute using <a class="reference internal" href="#equation-eq-cov-def">(22.6.39)</a>:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-32">
<span class="eqno">(22.6.41)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-32" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
\textrm{Cov}(X, Y) &amp; = \sum_{i, j} (x_i - \mu_X) (y_j-\mu_Y) p_{ij} \\
&amp; = (1-2)(-1-1)\frac{p}{2} + (1-2)(3-1)\frac{1-p}{2} + (3-2)(-1-1)\frac{1-p}{2} + (3-2)(3-1)\frac{p}{2} \\
&amp; = 4p-2.
\end{aligned}\end{split}\]</div>
<p>When <span class="math notranslate nohighlight">\(p=1\)</span> (the case where they are both maximally positive or
negative at the same time) has a covariance of <span class="math notranslate nohighlight">\(2\)</span>. When
<span class="math notranslate nohighlight">\(p=0\)</span> (the case where they are flipped) the covariance is
<span class="math notranslate nohighlight">\(-2\)</span>. Finally, when <span class="math notranslate nohighlight">\(p=1/2\)</span> (the case where they are
unrelated), the covariance is <span class="math notranslate nohighlight">\(0\)</span>. Thus we see that the covariance
measures how these two random variables are related.</p>
<p>A quick note on the covariance is that it only measures these linear
relationships. More complex relationships like <span class="math notranslate nohighlight">\(X = Y^2\)</span> where
<span class="math notranslate nohighlight">\(Y\)</span> is randomly chosen from <span class="math notranslate nohighlight">\(\{-2, -1, 0, 1, 2\}\)</span> with equal
probability can be missed. Indeed a quick computation shows that these
random variables have covariance zero, despite one being a deterministic
function of the other.</p>
<p>For continuous random variables, much the same story holds. At this
point, we are pretty comfortable with doing the transition between
discrete and continuous, so we will provide the continuous analogue of
<a class="reference internal" href="#equation-eq-cov-def">(22.6.39)</a> without any derivation.</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-33">
<span class="eqno">(22.6.42)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-33" title="Permalink to this equation">¶</a></span>\[\sigma_{XY} = \int_{\mathbb{R}^2} (x-\mu_X)(y-\mu_Y)p(x, y) \;dx \;dy.\]</div>
<p>For visualization, let’s take a look at a collection of random variables
with tunable covariance.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-15-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-15-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-15-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-15-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot a few random variables adjustable covariance</span>
<span class="n">covs</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">]</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">500</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">covs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">X</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">500</span><span class="p">)</span>

    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">Y</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cov = </span><span class="si">{</span><span class="n">covs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-15-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot a few random variables adjustable covariance</span>
<span class="n">covs</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">]</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">covs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">500</span><span class="p">))</span>

    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">Y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cov = </span><span class="si">{</span><span class="n">covs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-15-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot a few random variables adjustable covariance</span>
<span class="n">covs</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">]</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="mi">500</span><span class="p">,</span> <span class="p">))</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">covs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">X</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="mi">500</span><span class="p">,</span> <span class="p">))</span>

    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">Y</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cov = </span><span class="si">{</span><span class="n">covs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div></div><p>Let’s see some properties of covariances:</p>
<ul class="simple">
<li><p>For any random variable <span class="math notranslate nohighlight">\(X\)</span>,
<span class="math notranslate nohighlight">\(\textrm{Cov}(X, X) = \textrm{Var}(X)\)</span>.</p></li>
<li><p>For any random variables <span class="math notranslate nohighlight">\(X, Y\)</span> and numbers <span class="math notranslate nohighlight">\(a\)</span> and
<span class="math notranslate nohighlight">\(b\)</span>,
<span class="math notranslate nohighlight">\(\textrm{Cov}(aX+b, Y) = \textrm{Cov}(X, aY+b) = a\textrm{Cov}(X, Y)\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent then
<span class="math notranslate nohighlight">\(\textrm{Cov}(X, Y) = 0\)</span>.</p></li>
</ul>
<p>In addition, we can use the covariance to expand a relationship we saw
before. Recall that is <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are two independent
random variables then</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-34">
<span class="eqno">(22.6.43)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-34" title="Permalink to this equation">¶</a></span>\[\textrm{Var}(X+Y) = \textrm{Var}(X) + \textrm{Var}(Y).\]</div>
<p>With knowledge of covariances, we can expand this relationship. Indeed,
some algebra can show that in general,</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-35">
<span class="eqno">(22.6.44)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-35" title="Permalink to this equation">¶</a></span>\[\textrm{Var}(X+Y) = \textrm{Var}(X) + \textrm{Var}(Y) + 2\textrm{Cov}(X, Y).\]</div>
<p>This allows us to generalize the variance summation rule for correlated
random variables.</p>
</div>
<div class="section" id="correlation">
<h3><span class="section-number">22.6.1.11. </span>Correlation<a class="headerlink" href="#correlation" title="Permalink to this heading">¶</a></h3>
<p>As we did in the case of means and variances, let’s now consider units.
If <span class="math notranslate nohighlight">\(X\)</span> is measured in one unit (say inches), and <span class="math notranslate nohighlight">\(Y\)</span> is
measured in another (say dollars), the covariance is measured in the
product of these two units
<span class="math notranslate nohighlight">\(\textrm{inches} \times \textrm{dollars}\)</span>. These units can be hard
to interpret. What we will often want in this case is a unit-less
measurement of relatedness. Indeed, often we do not care about exact
quantitative correlation, but rather ask if the correlation is in the
same direction, and how strong the relationship is.</p>
<p>To see what makes sense, let’s perform a thought experiment. Suppose
that we convert our random variables in inches and dollars to be in
inches and cents. In this case the random variable <span class="math notranslate nohighlight">\(Y\)</span> is
multiplied by <span class="math notranslate nohighlight">\(100\)</span>. If we work through the definition, this means
that <span class="math notranslate nohighlight">\(\textrm{Cov}(X, Y)\)</span> will be multiplied by <span class="math notranslate nohighlight">\(100\)</span>. Thus
we see that in this case a change of units change the covariance by a
factor of <span class="math notranslate nohighlight">\(100\)</span>. Thus, to find our unit-invariant measure of
correlation, we will need to divide by something else that also gets
scaled by <span class="math notranslate nohighlight">\(100\)</span>. Indeed we have a clear candidate, the standard
deviation! Indeed if we define the <em>correlation coefficient</em> to be</p>
<div class="math notranslate nohighlight" id="equation-eq-cor-def">
<span class="eqno">(22.6.45)<a class="headerlink" href="#equation-eq-cor-def" title="Permalink to this equation">¶</a></span>\[\rho(X, Y) = \frac{\textrm{Cov}(X, Y)}{\sigma_{X}\sigma_{Y}},\]</div>
<p>we see that this is a unit-less value. A little mathematics can show
that this number is between <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(1\)</span> with <span class="math notranslate nohighlight">\(1\)</span>
meaning maximally positively correlated, whereas <span class="math notranslate nohighlight">\(-1\)</span> means
maximally negatively correlated.</p>
<p>Returning to our explicit discrete example above, we can see that
<span class="math notranslate nohighlight">\(\sigma_X = 1\)</span> and <span class="math notranslate nohighlight">\(\sigma_Y = 2\)</span>, so we can compute the
correlation between the two random variables using <a class="reference internal" href="#equation-eq-cor-def">(22.6.45)</a>
to see that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-36">
<span class="eqno">(22.6.46)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-36" title="Permalink to this equation">¶</a></span>\[\rho(X, Y) = \frac{4p-2}{1\cdot 2} = 2p-1.\]</div>
<p>This now ranges between <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(1\)</span> with the expected
behavior of <span class="math notranslate nohighlight">\(1\)</span> meaning most correlated, and <span class="math notranslate nohighlight">\(-1\)</span> meaning
minimally correlated.</p>
<p>As another example, consider <span class="math notranslate nohighlight">\(X\)</span> as any random variable, and
<span class="math notranslate nohighlight">\(Y=aX+b\)</span> as any linear deterministic function of <span class="math notranslate nohighlight">\(X\)</span>. Then,
one can compute that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-37">
<span class="eqno">(22.6.47)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-37" title="Permalink to this equation">¶</a></span>\[\sigma_{Y} = \sigma_{aX+b} = |a|\sigma_{X},\]</div>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-38">
<span class="eqno">(22.6.48)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-38" title="Permalink to this equation">¶</a></span>\[\textrm{Cov}(X, Y) = \textrm{Cov}(X, aX+b) = a\textrm{Cov}(X, X) = a\textrm{Var}(X),\]</div>
<p>and thus by <a class="reference internal" href="#equation-eq-cor-def">(22.6.45)</a> that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-39">
<span class="eqno">(22.6.49)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-39" title="Permalink to this equation">¶</a></span>\[\rho(X, Y) = \frac{a\textrm{Var}(X)}{|a|\sigma_{X}^2} = \frac{a}{|a|} = \textrm{sign}(a).\]</div>
<p>Thus we see that the correlation is <span class="math notranslate nohighlight">\(+1\)</span> for any <span class="math notranslate nohighlight">\(a &gt; 0\)</span>,
and <span class="math notranslate nohighlight">\(-1\)</span> for any <span class="math notranslate nohighlight">\(a &lt; 0\)</span> illustrating that correlation
measures the degree and directionality the two random variables are
related, not the scale that the variation takes.</p>
<p>Let’s again plot a collection of random variables with tunable
correlation.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-17-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-17-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-17-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-17-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot a few random variables adjustable correlations</span>
<span class="n">cors</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">500</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">cors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span>
                                 <span class="n">cors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">500</span><span class="p">)</span>

    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">Y</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cor = </span><span class="si">{</span><span class="n">cors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-17-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot a few random variables adjustable correlations</span>
<span class="n">cors</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">cors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">cors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>

    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">Y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cor = </span><span class="si">{</span><span class="n">cors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-17-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot a few random variables adjustable correlations</span>
<span class="n">cors</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="mi">500</span><span class="p">,</span> <span class="p">))</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">cors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">1.</span><span class="p">)</span> <span class="o">-</span>
                                 <span class="n">cors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="mi">500</span><span class="p">,</span> <span class="p">))</span>

    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">Y</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Y&#39;</span><span class="p">)</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;cor = </span><span class="si">{</span><span class="n">cors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div></div><p>Let’s list a few properties of the correlation below.</p>
<ul class="simple">
<li><p>For any random variable <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(\rho(X, X) = 1\)</span>.</p></li>
<li><p>For any random variables <span class="math notranslate nohighlight">\(X, Y\)</span> and numbers <span class="math notranslate nohighlight">\(a\)</span> and
<span class="math notranslate nohighlight">\(b\)</span>, <span class="math notranslate nohighlight">\(\rho(aX+b, Y) = \rho(X, aY+b) = \rho(X, Y)\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent with non-zero variance
then <span class="math notranslate nohighlight">\(\rho(X, Y) = 0\)</span>.</p></li>
</ul>
<p>As a final note, you may feel like some of these formulae are familiar.
Indeed, if we expand everything out assuming that
<span class="math notranslate nohighlight">\(\mu_X = \mu_Y = 0\)</span>, we see that this is</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-40">
<span class="eqno">(22.6.50)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-40" title="Permalink to this equation">¶</a></span>\[\rho(X, Y) = \frac{\sum_{i, j} x_iy_ip_{ij}}{\sqrt{\sum_{i, j}x_i^2 p_{ij}}\sqrt{\sum_{i, j}y_j^2 p_{ij}}}.\]</div>
<p>This looks like a sum of a product of terms divided by the square root
of sums of terms. This is exactly the formula for the cosine of the
angle between two vectors <span class="math notranslate nohighlight">\(\mathbf{v}, \mathbf{w}\)</span> with the
different coordinates weighted by <span class="math notranslate nohighlight">\(p_{ij}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-random-variables-41">
<span class="eqno">(22.6.51)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-random-variables-41" title="Permalink to this equation">¶</a></span>\[\cos(\theta) = \frac{\mathbf{v}\cdot \mathbf{w}}{\|\mathbf{v}\|\|\mathbf{w}\|} = \frac{\sum_{i} v_iw_i}{\sqrt{\sum_{i}v_i^2}\sqrt{\sum_{i}w_i^2}}.\]</div>
<p>Indeed if we think of norms as being related to standard deviations, and
correlations as being cosines of angles, much of the intuition we have
from geometry can be applied to thinking about random variables.</p>
</div>
</div>
<div class="section" id="summary">
<h2><span class="section-number">22.6.2. </span>Summary<a class="headerlink" href="#summary" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>Continuous random variables are random variables that can take on a
continuum of values. They have some technical difficulties that make
them more challenging to work with compared to discrete random
variables.</p></li>
<li><p>The probability density function allows us to work with continuous
random variables by giving a function where the area under the curve
on some interval gives the probability of finding a sample point in
that interval.</p></li>
<li><p>The cumulative distribution function is the probability of observing
the random variable to be less than a given threshold. It can provide
a useful alternate viewpoint which unifies discrete and continuous
variables.</p></li>
<li><p>The mean is the average value of a random variable.</p></li>
<li><p>The variance is the expected square of the difference between the
random variable and its mean.</p></li>
<li><p>The standard deviation is the square root of the variance. It can be
thought of as measuring the range of values the random variable may
take.</p></li>
<li><p>Chebyshev’s inequality allows us to make this intuition rigorous by
giving an explicit interval that contains the random variable most of
the time.</p></li>
<li><p>Joint densities allow us to work with correlated random variables. We
may marginalize joint densities by integrating over unwanted random
variables to get the distribution of the desired random variable.</p></li>
<li><p>The covariance and correlation coefficient provide a way to measure
any linear relationship between two correlated random variables.</p></li>
</ul>
</div>
<div class="section" id="exercises">
<h2><span class="section-number">22.6.3. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this heading">¶</a></h2>
<ol class="arabic simple">
<li><p>Suppose that we have the random variable with density given by
<span class="math notranslate nohighlight">\(p(x) = \frac{1}{x^2}\)</span> for <span class="math notranslate nohighlight">\(x \ge 1\)</span> and <span class="math notranslate nohighlight">\(p(x) = 0\)</span>
otherwise. What is <span class="math notranslate nohighlight">\(P(X &gt; 2)\)</span>?</p></li>
<li><p>The Laplace distribution is a random variable whose density is given
by <span class="math notranslate nohighlight">\(p(x = \frac{1}{2}e^{-|x|}\)</span>. What is the mean and the
standard deviation of this function? As a hint,
<span class="math notranslate nohighlight">\(\int_0^\infty xe^{-x} \; dx = 1\)</span> and
<span class="math notranslate nohighlight">\(\int_0^\infty x^2e^{-x} \; dx = 2\)</span>.</p></li>
<li><p>I walk up to you on the street and say “I have a random variable with
mean <span class="math notranslate nohighlight">\(1\)</span>, standard deviation <span class="math notranslate nohighlight">\(2\)</span>, and I observed
<span class="math notranslate nohighlight">\(25\%\)</span> of my samples taking a value larger than <span class="math notranslate nohighlight">\(9\)</span>.” Do
you believe me? Why or why not?</p></li>
<li><p>Suppose that you have two random variables <span class="math notranslate nohighlight">\(X, Y\)</span>, with joint
density given by <span class="math notranslate nohighlight">\(p_{XY}(x, y) = 4xy\)</span> for
<span class="math notranslate nohighlight">\(x, y \in [0,1]\)</span> and <span class="math notranslate nohighlight">\(p_{XY}(x, y) = 0\)</span> otherwise. What
is the covariance of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>?</p></li>
</ol>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar text"><a href="#pytorch-19-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-19-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-19-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-19-0"><p><a class="reference external" href="https://discuss.d2l.ai/t/1094">Discussions</a></p>
</div><div class="mdl-tabs__panel " id="mxnet-19-1"><p><a class="reference external" href="https://discuss.d2l.ai/t/415">Discussions</a></p>
</div><div class="mdl-tabs__panel " id="tensorflow-19-2"><p><a class="reference external" href="https://discuss.d2l.ai/t/1095">Discussions</a></p>
</div></div></div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">22.6. Random Variables</a><ul>
<li><a class="reference internal" href="#continuous-random-variables">22.6.1. Continuous Random Variables</a><ul>
<li><a class="reference internal" href="#from-discrete-to-continuous">22.6.1.1. From Discrete to Continuous</a></li>
<li><a class="reference internal" href="#probability-density-functions">22.6.1.2. Probability Density Functions</a></li>
<li><a class="reference internal" href="#cumulative-distribution-functions">22.6.1.3. Cumulative Distribution Functions</a></li>
<li><a class="reference internal" href="#means">22.6.1.4. Means</a></li>
<li><a class="reference internal" href="#variances">22.6.1.5. Variances</a></li>
<li><a class="reference internal" href="#standard-deviations">22.6.1.6. Standard Deviations</a></li>
<li><a class="reference internal" href="#means-and-variances-in-the-continuum">22.6.1.7. Means and Variances in the Continuum</a></li>
<li><a class="reference internal" href="#joint-density-functions">22.6.1.8. Joint Density Functions</a></li>
<li><a class="reference internal" href="#marginal-distributions">22.6.1.9. Marginal Distributions</a></li>
<li><a class="reference internal" href="#covariance">22.6.1.10. Covariance</a></li>
<li><a class="reference internal" href="#correlation">22.6.1.11. Correlation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#summary">22.6.2. Summary</a></li>
<li><a class="reference internal" href="#exercises">22.6.3. Exercises</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="integral-calculus.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>22.5. Integral Calculus</div>
         </div>
     </a>
     <a id="button-next" href="maximum-likelihood.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>22.7. Maximum Likelihood</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>