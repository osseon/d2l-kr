<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>22.5. Integral Calculus &#8212; Dive into Deep Learning 1.0.3 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="22.6. Random Variables" href="random-variables.html" />
    <link rel="prev" title="22.4. Multivariable Calculus" href="multivariable-calculus.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">22. </span>Appendix: Mathematics for Deep Learning</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">22.5. </span>Integral Calculus</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_appendix-mathematics-for-deep-learning/integral-calculus.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="http://preview.d2l.ai/d2l-en/master">
                  <i class="fas fa-book"></i>
                  Preview Version
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai/d2l-en.pdf">
                  <i class="fas fa-file-pdf"></i>
                  PyTorch
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai/d2l-en-mxnet.pdf">
                  <i class="fas fa-file-pdf"></i>
                  MXNet
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai/d2l-en.zip">
                  <i class="fab fa-python"></i>
                  Notebooks
              </a>
          
              <a  class="mdl-navigation__link" href="https://courses.d2l.ai">
                  <i class="fas fa-user-graduate"></i>
                  Courses
              </a>
          
              <a  class="mdl-navigation__link" href="https://github.com/d2l-ai/d2l-en">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://zh.d2l.ai">
                  <i class="fas fa-external-link-alt"></i>
                  中文版
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="Dive into Deep Learning"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index.html">Notation</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. Preliminaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html">2.1. Data Manipulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html">2.2. Data Preprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html">2.3. Linear Algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html">2.4. Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html">2.5. Automatic Differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html">2.6. Probability and Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html">2.7. Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-regression/index.html">3. Linear Neural Networks for Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression.html">3.1. Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/oo-design.html">3.2. Object-Oriented Design for Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/synthetic-regression-data.html">3.3. Synthetic Regression Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression-scratch.html">3.4. Linear Regression Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression-concise.html">3.5. Concise Implementation of Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/generalization.html">3.6. Generalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/weight-decay.html">3.7. Weight Decay</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-classification/index.html">4. Linear Neural Networks for Classification</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression.html">4.1. Softmax Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/image-classification-dataset.html">4.2. The Image Classification Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/classification.html">4.3. The Base Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression-scratch.html">4.4. Softmax Regression Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression-concise.html">4.5. Concise Implementation of Softmax Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/generalization-classification.html">4.6. Generalization in Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/environment-and-distribution-shift.html">4.7. Environment and Distribution Shift</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multilayer-perceptrons/index.html">5. Multilayer Perceptrons</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp.html">5.1. Multilayer Perceptrons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-implementation.html">5.2. Implementation of Multilayer Perceptrons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/backprop.html">5.3. Forward Propagation, Backward Propagation, and Computational Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html">5.4. Numerical Stability and Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/generalization-deep.html">5.5. Generalization in Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/dropout.html">5.6. Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/kaggle-house-price.html">5.7. Predicting House Prices on Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_builders-guide/index.html">6. Builders’ Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/model-construction.html">6.1. Layers and Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/parameters.html">6.2. Parameter Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/init-param.html">6.3. Parameter Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/lazy-init.html">6.4. Lazy Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/custom-layer.html">6.5. Custom Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/read-write.html">6.6. File I/O</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/use-gpu.html">6.7. GPUs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index.html">7. Convolutional Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv.html">7.1. From Fully Connected Layers to Convolutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer.html">7.2. Convolutions for Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides.html">7.3. Padding and Stride</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels.html">7.4. Multiple Input and Multiple Output Channels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling.html">7.5. Pooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet.html">7.6. Convolutional Neural Networks (LeNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index.html">8. Modern Convolutional Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet.html">8.1. Deep Convolutional Neural Networks (AlexNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/vgg.html">8.2. Networks Using Blocks (VGG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/nin.html">8.3. Network in Network (NiN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet.html">8.4. Multi-Branch Networks (GoogLeNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm.html">8.5. Batch Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet.html">8.6. Residual Networks (ResNet) and ResNeXt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet.html">8.7. Densely Connected Networks (DenseNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/cnn-design.html">8.8. Designing Convolution Network Architectures</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index.html">9. Recurrent Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/sequence.html">9.1. Working with Sequences</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/text-sequence.html">9.2. Converting Raw Text into Sequence Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/language-model.html">9.3. Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn.html">9.4. Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch.html">9.5. Recurrent Neural Network Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-concise.html">9.6. Concise Implementation of Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/bptt.html">9.7. Backpropagation Through Time</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index.html">10. Modern Recurrent Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm.html">10.1. Long Short-Term Memory (LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru.html">10.2. Gated Recurrent Units (GRU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn.html">10.3. Deep Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/bi-rnn.html">10.4. Bidirectional Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset.html">10.5. Machine Translation and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder.html">10.6. The Encoder–Decoder Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq.html">10.7. Sequence-to-Sequence Learning for Machine Translation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search.html">10.8. Beam Search</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/index.html">11. Attention Mechanisms and Transformers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/queries-keys-values.html">11.1. Queries, Keys, and Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/attention-pooling.html">11.2. Attention Pooling by Similarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html">11.3. Attention Scoring Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/bahdanau-attention.html">11.4. The Bahdanau Attention Mechanism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/multihead-attention.html">11.5. Multi-Head Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html">11.6. Self-Attention and Positional Encoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/transformer.html">11.7. The Transformer Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/vision-transformer.html">11.8. Transformers for Vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html">11.9. Large-Scale Pretraining with Transformers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_optimization/index.html">12. Optimization Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro.html">12.1. Optimization and Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/convexity.html">12.2. Convexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/gd.html">12.3. Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/sgd.html">12.4. Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/minibatch-sgd.html">12.5. Minibatch Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/momentum.html">12.6. Momentum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adagrad.html">12.7. Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/rmsprop.html">12.8. RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adadelta.html">12.9. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adam.html">12.10. Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/lr-scheduler.html">12.11. Learning Rate Scheduling</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational-performance/index.html">13. Computational Performance</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hybridize.html">13.1. Compilers and Interpreters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/async-computation.html">13.2. Asynchronous Computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/auto-parallelism.html">13.3. Automatic Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hardware.html">13.4. Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus.html">13.5. Training on Multiple GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus-concise.html">13.6. Concise Implementation for Multiple GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/parameterserver.html">13.7. Parameter Servers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index.html">14. Computer Vision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation.html">14.1. Image Augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning.html">14.2. Fine-Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box.html">14.3. Object Detection and Bounding Boxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor.html">14.4. Anchor Boxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection.html">14.5. Multiscale Object Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset.html">14.6. The Object Detection Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd.html">14.7. Single Shot Multibox Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn.html">14.8. Region-based CNNs (R-CNNs)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset.html">14.9. Semantic Segmentation and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv.html">14.10. Transposed Convolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn.html">14.11. Fully Convolutional Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style.html">14.12. Neural Style Transfer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10.html">14.13. Image Classification (CIFAR-10) on Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog.html">14.14. Dog Breed Identification (ImageNet Dogs) on Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index.html">15. Natural Language Processing: Pretraining</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec.html">15.1. Word Embedding (word2vec)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training.html">15.2. Approximate Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html">15.3. The Dataset for Pretraining Word Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html">15.4. Pretraining word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove.html">15.5. Word Embedding with Global Vectors (GloVe)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding.html">15.6. Subword Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy.html">15.7. Word Similarity and Analogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert.html">15.8. Bidirectional Encoder Representations from Transformers (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset.html">15.9. The Dataset for Pretraining BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining.html">15.10. Pretraining BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index.html">16. Natural Language Processing: Applications</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html">16.1. Sentiment Analysis and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html">16.2. Sentiment Analysis: Using Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html">16.3. Sentiment Analysis: Using Convolutional Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html">16.4. Natural Language Inference and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html">16.5. Natural Language Inference: Using Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert.html">16.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html">16.7. Natural Language Inference: Fine-Tuning BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement-learning/index.html">17. Reinforcement Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/mdp.html">17.1. Markov Decision Process (MDP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/value-iter.html">17.2. Value Iteration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/qlearning.html">17.3. Q-Learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_gaussian-processes/index.html">18. Gaussian Processes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-intro.html">18.1. Introduction to Gaussian Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-priors.html">18.2. Gaussian Process Priors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-inference.html">18.3. Gaussian Process Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_hyperparameter-optimization/index.html">19. Hyperparameter Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/hyperopt-intro.html">19.1. What Is Hyperparameter Optimization?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/hyperopt-api.html">19.2. Hyperparameter Optimization API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/rs-async.html">19.3. Asynchronous Random Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/sh-intro.html">19.4. Multi-Fidelity Hyperparameter Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/sh-async.html">19.5. Asynchronous Successive Halving</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_generative-adversarial-networks/index.html">20. Generative Adversarial Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/gan.html">20.1. Generative Adversarial Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/dcgan.html">20.2. Deep Convolutional Generative Adversarial Networks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender-systems/index.html">21. Recommender Systems</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/recsys-intro.html">21.1. Overview of Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/movielens.html">21.2. The MovieLens Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/mf.html">21.3. Matrix Factorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/autorec.html">21.4. AutoRec: Rating Prediction with Autoencoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ranking.html">21.5. Personalized Ranking for Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/neumf.html">21.6. Neural Collaborative Filtering for Personalized Ranking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/seqrec.html">21.7. Sequence-Aware Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ctr.html">21.8. Feature-Rich Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/fm.html">21.9. Factorization Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/deepfm.html">21.10. Deep Factorization Machines</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">22. Appendix: Mathematics for Deep Learning</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="geometry-linear-algebraic-ops.html">22.1. Geometry and Linear Algebraic Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="eigendecomposition.html">22.2. Eigendecompositions</a></li>
<li class="toctree-l2"><a class="reference internal" href="single-variable-calculus.html">22.3. Single Variable Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="multivariable-calculus.html">22.4. Multivariable Calculus</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">22.5. Integral Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="random-variables.html">22.6. Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="maximum-likelihood.html">22.7. Maximum Likelihood</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html">22.8. Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="naive-bayes.html">22.9. Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="statistics.html">22.10. Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="information-theory.html">22.11. Information Theory</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index.html">23. Appendix: Tools for Deep Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/jupyter.html">23.1. Using Jupyter Notebooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/sagemaker.html">23.2. Using Amazon SageMaker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/aws.html">23.3. Using AWS EC2 Instances</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/colab.html">23.4. Using Google Colab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html">23.5. Selecting Servers and GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/contributing.html">23.6. Contributing to This Book</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/utils.html">23.7. Utility Functions and Classes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/d2l.html">23.8. The <code class="docutils literal notranslate"><span class="pre">d2l</span></code> API Document</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">References</a></li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="Dive into Deep Learning"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index.html">Notation</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. Preliminaries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html">2.1. Data Manipulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html">2.2. Data Preprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html">2.3. Linear Algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html">2.4. Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html">2.5. Automatic Differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html">2.6. Probability and Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html">2.7. Documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-regression/index.html">3. Linear Neural Networks for Regression</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression.html">3.1. Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/oo-design.html">3.2. Object-Oriented Design for Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/synthetic-regression-data.html">3.3. Synthetic Regression Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression-scratch.html">3.4. Linear Regression Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/linear-regression-concise.html">3.5. Concise Implementation of Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/generalization.html">3.6. Generalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-regression/weight-decay.html">3.7. Weight Decay</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-classification/index.html">4. Linear Neural Networks for Classification</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression.html">4.1. Softmax Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/image-classification-dataset.html">4.2. The Image Classification Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/classification.html">4.3. The Base Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression-scratch.html">4.4. Softmax Regression Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/softmax-regression-concise.html">4.5. Concise Implementation of Softmax Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/generalization-classification.html">4.6. Generalization in Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-classification/environment-and-distribution-shift.html">4.7. Environment and Distribution Shift</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multilayer-perceptrons/index.html">5. Multilayer Perceptrons</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp.html">5.1. Multilayer Perceptrons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-implementation.html">5.2. Implementation of Multilayer Perceptrons</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/backprop.html">5.3. Forward Propagation, Backward Propagation, and Computational Graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html">5.4. Numerical Stability and Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/generalization-deep.html">5.5. Generalization in Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/dropout.html">5.6. Dropout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/kaggle-house-price.html">5.7. Predicting House Prices on Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_builders-guide/index.html">6. Builders’ Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/model-construction.html">6.1. Layers and Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/parameters.html">6.2. Parameter Management</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/init-param.html">6.3. Parameter Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/lazy-init.html">6.4. Lazy Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/custom-layer.html">6.5. Custom Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/read-write.html">6.6. File I/O</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_builders-guide/use-gpu.html">6.7. GPUs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index.html">7. Convolutional Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv.html">7.1. From Fully Connected Layers to Convolutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer.html">7.2. Convolutions for Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides.html">7.3. Padding and Stride</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels.html">7.4. Multiple Input and Multiple Output Channels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling.html">7.5. Pooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet.html">7.6. Convolutional Neural Networks (LeNet)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index.html">8. Modern Convolutional Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet.html">8.1. Deep Convolutional Neural Networks (AlexNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/vgg.html">8.2. Networks Using Blocks (VGG)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/nin.html">8.3. Network in Network (NiN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet.html">8.4. Multi-Branch Networks (GoogLeNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm.html">8.5. Batch Normalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet.html">8.6. Residual Networks (ResNet) and ResNeXt</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet.html">8.7. Densely Connected Networks (DenseNet)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/cnn-design.html">8.8. Designing Convolution Network Architectures</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index.html">9. Recurrent Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/sequence.html">9.1. Working with Sequences</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/text-sequence.html">9.2. Converting Raw Text into Sequence Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/language-model.html">9.3. Language Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn.html">9.4. Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch.html">9.5. Recurrent Neural Network Implementation from Scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-concise.html">9.6. Concise Implementation of Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/bptt.html">9.7. Backpropagation Through Time</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index.html">10. Modern Recurrent Neural Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm.html">10.1. Long Short-Term Memory (LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru.html">10.2. Gated Recurrent Units (GRU)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn.html">10.3. Deep Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/bi-rnn.html">10.4. Bidirectional Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset.html">10.5. Machine Translation and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder.html">10.6. The Encoder–Decoder Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq.html">10.7. Sequence-to-Sequence Learning for Machine Translation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search.html">10.8. Beam Search</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/index.html">11. Attention Mechanisms and Transformers</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/queries-keys-values.html">11.1. Queries, Keys, and Values</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/attention-pooling.html">11.2. Attention Pooling by Similarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html">11.3. Attention Scoring Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/bahdanau-attention.html">11.4. The Bahdanau Attention Mechanism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/multihead-attention.html">11.5. Multi-Head Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html">11.6. Self-Attention and Positional Encoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/transformer.html">11.7. The Transformer Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/vision-transformer.html">11.8. Transformers for Vision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html">11.9. Large-Scale Pretraining with Transformers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_optimization/index.html">12. Optimization Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/optimization-intro.html">12.1. Optimization and Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/convexity.html">12.2. Convexity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/gd.html">12.3. Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/sgd.html">12.4. Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/minibatch-sgd.html">12.5. Minibatch Stochastic Gradient Descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/momentum.html">12.6. Momentum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adagrad.html">12.7. Adagrad</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/rmsprop.html">12.8. RMSProp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adadelta.html">12.9. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/adam.html">12.10. Adam</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_optimization/lr-scheduler.html">12.11. Learning Rate Scheduling</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational-performance/index.html">13. Computational Performance</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hybridize.html">13.1. Compilers and Interpreters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/async-computation.html">13.2. Asynchronous Computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/auto-parallelism.html">13.3. Automatic Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/hardware.html">13.4. Hardware</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus.html">13.5. Training on Multiple GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/multiple-gpus-concise.html">13.6. Concise Implementation for Multiple GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computational-performance/parameterserver.html">13.7. Parameter Servers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index.html">14. Computer Vision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation.html">14.1. Image Augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning.html">14.2. Fine-Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box.html">14.3. Object Detection and Bounding Boxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor.html">14.4. Anchor Boxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection.html">14.5. Multiscale Object Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset.html">14.6. The Object Detection Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd.html">14.7. Single Shot Multibox Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn.html">14.8. Region-based CNNs (R-CNNs)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset.html">14.9. Semantic Segmentation and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv.html">14.10. Transposed Convolution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn.html">14.11. Fully Convolutional Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style.html">14.12. Neural Style Transfer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10.html">14.13. Image Classification (CIFAR-10) on Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog.html">14.14. Dog Breed Identification (ImageNet Dogs) on Kaggle</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index.html">15. Natural Language Processing: Pretraining</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec.html">15.1. Word Embedding (word2vec)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training.html">15.2. Approximate Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html">15.3. The Dataset for Pretraining Word Embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html">15.4. Pretraining word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove.html">15.5. Word Embedding with Global Vectors (GloVe)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding.html">15.6. Subword Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy.html">15.7. Word Similarity and Analogy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert.html">15.8. Bidirectional Encoder Representations from Transformers (BERT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset.html">15.9. The Dataset for Pretraining BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining.html">15.10. Pretraining BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index.html">16. Natural Language Processing: Applications</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html">16.1. Sentiment Analysis and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html">16.2. Sentiment Analysis: Using Recurrent Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html">16.3. Sentiment Analysis: Using Convolutional Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html">16.4. Natural Language Inference and the Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html">16.5. Natural Language Inference: Using Attention</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert.html">16.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html">16.7. Natural Language Inference: Fine-Tuning BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_reinforcement-learning/index.html">17. Reinforcement Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/mdp.html">17.1. Markov Decision Process (MDP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/value-iter.html">17.2. Value Iteration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_reinforcement-learning/qlearning.html">17.3. Q-Learning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_gaussian-processes/index.html">18. Gaussian Processes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-intro.html">18.1. Introduction to Gaussian Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-priors.html">18.2. Gaussian Process Priors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_gaussian-processes/gp-inference.html">18.3. Gaussian Process Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_hyperparameter-optimization/index.html">19. Hyperparameter Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/hyperopt-intro.html">19.1. What Is Hyperparameter Optimization?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/hyperopt-api.html">19.2. Hyperparameter Optimization API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/rs-async.html">19.3. Asynchronous Random Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/sh-intro.html">19.4. Multi-Fidelity Hyperparameter Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_hyperparameter-optimization/sh-async.html">19.5. Asynchronous Successive Halving</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_generative-adversarial-networks/index.html">20. Generative Adversarial Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/gan.html">20.1. Generative Adversarial Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_generative-adversarial-networks/dcgan.html">20.2. Deep Convolutional Generative Adversarial Networks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recommender-systems/index.html">21. Recommender Systems</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/recsys-intro.html">21.1. Overview of Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/movielens.html">21.2. The MovieLens Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/mf.html">21.3. Matrix Factorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/autorec.html">21.4. AutoRec: Rating Prediction with Autoencoders</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ranking.html">21.5. Personalized Ranking for Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/neumf.html">21.6. Neural Collaborative Filtering for Personalized Ranking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/seqrec.html">21.7. Sequence-Aware Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/ctr.html">21.8. Feature-Rich Recommender Systems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/fm.html">21.9. Factorization Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recommender-systems/deepfm.html">21.10. Deep Factorization Machines</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">22. Appendix: Mathematics for Deep Learning</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="geometry-linear-algebraic-ops.html">22.1. Geometry and Linear Algebraic Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="eigendecomposition.html">22.2. Eigendecompositions</a></li>
<li class="toctree-l2"><a class="reference internal" href="single-variable-calculus.html">22.3. Single Variable Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="multivariable-calculus.html">22.4. Multivariable Calculus</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">22.5. Integral Calculus</a></li>
<li class="toctree-l2"><a class="reference internal" href="random-variables.html">22.6. Random Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="maximum-likelihood.html">22.7. Maximum Likelihood</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributions.html">22.8. Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="naive-bayes.html">22.9. Naive Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="statistics.html">22.10. Statistics</a></li>
<li class="toctree-l2"><a class="reference internal" href="information-theory.html">22.11. Information Theory</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index.html">23. Appendix: Tools for Deep Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/jupyter.html">23.1. Using Jupyter Notebooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/sagemaker.html">23.2. Using Amazon SageMaker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/aws.html">23.3. Using AWS EC2 Instances</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/colab.html">23.4. Using Google Colab</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html">23.5. Selecting Servers and GPUs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/contributing.html">23.6. Contributing to This Book</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/utils.html">23.7. Utility Functions and Classes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/d2l.html">23.8. The <code class="docutils literal notranslate"><span class="pre">d2l</span></code> API Document</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_references/zreferences.html">References</a></li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="integral-calculus">
<span id="sec-integral-calculus"></span><h1><span class="section-number">22.5. </span>Integral Calculus<a class="headerlink" href="#integral-calculus" title="Permalink to this heading">¶</a><div class="d2l-tabs" style="float:right"><div class="d2l-tabs__tab"><a href="https://colab.research.google.com/github/d2l-ai/d2l-pytorch-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/integral-calculus.ipynb" onclick="captureOutboundLink('https://colab.research.google.com/github/d2l-ai/d2l-pytorch-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/integral-calculus.ipynb'); return false;"> <button style="float:right", id="Colab_[pytorch]" class="mdl-button mdl-js-button mdl-button--primary mdl-js-ripple-effect"> <i class=" fas fa-external-link-alt"></i> Colab [pytorch] </button></a><div class="mdl-tooltip" data-mdl-for="Colab_[pytorch]"> Open the notebook in Colab</div></div><div class="d2l-tabs__tab"><a href="https://colab.research.google.com/github/d2l-ai/d2l-en-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/integral-calculus.ipynb" onclick="captureOutboundLink('https://colab.research.google.com/github/d2l-ai/d2l-en-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/integral-calculus.ipynb'); return false;"> <button style="float:right", id="Colab_[mxnet]" class="mdl-button mdl-js-button mdl-button--primary mdl-js-ripple-effect"> <i class=" fas fa-external-link-alt"></i> Colab [mxnet] </button></a><div class="mdl-tooltip" data-mdl-for="Colab_[mxnet]"> Open the notebook in Colab</div></div><div class="d2l-tabs__tab"><a href="https://colab.research.google.com/github/d2l-ai/d2l-jax-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/integral-calculus.ipynb" onclick="captureOutboundLink('https://colab.research.google.com/github/d2l-ai/d2l-jax-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/integral-calculus.ipynb'); return false;"> <button style="float:right", id="Colab_[jax]" class="mdl-button mdl-js-button mdl-button--primary mdl-js-ripple-effect"> <i class=" fas fa-external-link-alt"></i> Colab [jax] </button></a><div class="mdl-tooltip" data-mdl-for="Colab_[jax]"> Open the notebook in Colab</div></div><div class="d2l-tabs__tab"><a href="https://colab.research.google.com/github/d2l-ai/d2l-tensorflow-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/integral-calculus.ipynb" onclick="captureOutboundLink('https://colab.research.google.com/github/d2l-ai/d2l-tensorflow-colab/blob/master/chapter_appendix-mathematics-for-deep-learning/integral-calculus.ipynb'); return false;"> <button style="float:right", id="Colab_[tensorflow]" class="mdl-button mdl-js-button mdl-button--primary mdl-js-ripple-effect"> <i class=" fas fa-external-link-alt"></i> Colab [tensorflow] </button></a><div class="mdl-tooltip" data-mdl-for="Colab_[tensorflow]"> Open the notebook in Colab</div></div></div></h1>
<p>Differentiation only makes up half of the content of a traditional
calculus education. The other pillar, integration, starts out seeming a
rather disjoint question, “What is the area underneath this curve?”
While seemingly unrelated, integration is tightly intertwined with the
differentiation via what is known as the <em>fundamental theorem of
calculus</em>.</p>
<p>At the level of machine learning we discuss in this book, we will not
need a deep understanding of integration. However, we will provide a
brief introduction to lay the groundwork for any further applications we
will encounter later on.</p>
<div class="section" id="geometric-interpretation">
<h2><span class="section-number">22.5.1. </span>Geometric Interpretation<a class="headerlink" href="#geometric-interpretation" title="Permalink to this heading">¶</a></h2>
<p>Suppose that we have a function <span class="math notranslate nohighlight">\(f(x)\)</span>. For simplicity, let’s
assume that <span class="math notranslate nohighlight">\(f(x)\)</span> is non-negative (never takes a value less than
zero). What we want to try and understand is: what is the area contained
between <span class="math notranslate nohighlight">\(f(x)\)</span> and the <span class="math notranslate nohighlight">\(x\)</span>-axis?</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-1-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-1-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-1-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-1-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits</span> <span class="kn">import</span> <span class="n">mplot3d</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">torch</span> <span class="k">as</span> <span class="n">d2l</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">f</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-1-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits</span> <span class="kn">import</span> <span class="n">mplot3d</span>
<span class="kn">from</span> <span class="nn">mxnet</span> <span class="kn">import</span> <span class="n">np</span><span class="p">,</span> <span class="n">npx</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">mxnet</span> <span class="k">as</span> <span class="n">d2l</span>

<span class="n">npx</span><span class="o">.</span><span class="n">set_np</span><span class="p">()</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">f</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-1-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits</span> <span class="kn">import</span> <span class="n">mplot3d</span>
<span class="kn">from</span> <span class="nn">d2l</span> <span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">d2l</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">f</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div></div><p>In most cases, this area will be infinite or undefined (consider the
area under <span class="math notranslate nohighlight">\(f(x) = x^{2}\)</span>), so people will often talk about the
area between a pair of ends, say <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-3-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-3-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-3-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-3-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">50</span><span class="p">:</span><span class="mi">250</span><span class="p">],</span> <span class="n">f</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">50</span><span class="p">:</span><span class="mi">250</span><span class="p">])</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-3-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">50</span><span class="p">:</span><span class="mi">250</span><span class="p">],</span> <span class="n">f</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">50</span><span class="p">:</span><span class="mi">250</span><span class="p">])</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-3-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">50</span><span class="p">:</span><span class="mi">250</span><span class="p">],</span> <span class="n">f</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">50</span><span class="p">:</span><span class="mi">250</span><span class="p">])</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div></div><p>We will denote this area by the integral symbol below:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-0">
<span class="eqno">(22.5.1)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-0" title="Permalink to this equation">¶</a></span>\[\textrm{Area}(\mathcal{A}) = \int_a^b f(x) \;dx.\]</div>
<p>The inner variable is a dummy variable, much like the index of a sum in
a <span class="math notranslate nohighlight">\(\sum\)</span>, and so this can be equivalently written with any inner
value we like:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-1">
<span class="eqno">(22.5.2)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-1" title="Permalink to this equation">¶</a></span>\[\int_a^b f(x) \;dx = \int_a^b f(z) \;dz.\]</div>
<p>There is a traditional way to try and understand how we might try to
approximate such integrals: we can imagine taking the region in-between
<span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> and chopping it into <span class="math notranslate nohighlight">\(N\)</span> vertical slices.
If <span class="math notranslate nohighlight">\(N\)</span> is large, we can approximate the area of each slice by a
rectangle, and then add up the areas to get the total area under the
curve. Let’s take a look at an example doing this in code. We will see
how to get the true value in a later section.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-5-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-5-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-5-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-5-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">a</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">approx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">epsilon</span><span class="o">*</span><span class="n">f</span><span class="p">)</span>
<span class="n">true</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">5.</span><span class="p">]))</span> <span class="o">/</span> <span class="mi">2</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;edge&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="sa">f</span><span class="s1">&#39;approximation: </span><span class="si">{</span><span class="n">approx</span><span class="si">}</span><span class="s1">, truth: </span><span class="si">{</span><span class="n">true</span><span class="si">}</span><span class="s1">&#39;</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-5-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">a</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">approx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">epsilon</span><span class="o">*</span><span class="n">f</span><span class="p">)</span>
<span class="n">true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">f</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">width</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;edge&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="sa">f</span><span class="s1">&#39;approximation: </span><span class="si">{</span><span class="n">approx</span><span class="si">}</span><span class="s1">, truth: </span><span class="si">{</span><span class="n">true</span><span class="si">}</span><span class="s1">&#39;</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-5-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">a</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">approx</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">epsilon</span><span class="o">*</span><span class="n">f</span><span class="p">)</span>
<span class="n">true</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mf">5.</span><span class="p">]))</span> <span class="o">/</span> <span class="mi">2</span>

<span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;edge&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="sa">f</span><span class="s1">&#39;approximation: </span><span class="si">{</span><span class="n">approx</span><span class="si">}</span><span class="s1">, truth: </span><span class="si">{</span><span class="n">true</span><span class="si">}</span><span class="s1">&#39;</span>
</pre></div>
</div>
</div></div><p>The issue is that while it can be done numerically, we can do this
approach analytically for only the simplest functions like</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-2">
<span class="eqno">(22.5.3)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-2" title="Permalink to this equation">¶</a></span>\[\int_a^b x \;dx.\]</div>
<p>Anything somewhat more complex like our example from the code above</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-3">
<span class="eqno">(22.5.4)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-3" title="Permalink to this equation">¶</a></span>\[\int_a^b \frac{x}{1+x^{2}} \;dx.\]</div>
<p>is beyond what we can solve with such a direct method.</p>
<p>We will instead take a different approach. We will work intuitively with
the notion of the area, and learn the main computational tool used to
find integrals: the <em>fundamental theorem of calculus</em>. This will be the
basis for our study of integration.</p>
</div>
<div class="section" id="the-fundamental-theorem-of-calculus">
<h2><span class="section-number">22.5.2. </span>The Fundamental Theorem of Calculus<a class="headerlink" href="#the-fundamental-theorem-of-calculus" title="Permalink to this heading">¶</a></h2>
<p>To dive deeper into the theory of integration, let’s introduce a
function</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-4">
<span class="eqno">(22.5.5)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-4" title="Permalink to this equation">¶</a></span>\[F(x) = \int_0^x f(y) dy.\]</div>
<p>This function measures the area between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(x\)</span>
depending on how we change <span class="math notranslate nohighlight">\(x\)</span>. Notice that this is everything we
need since</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-5">
<span class="eqno">(22.5.6)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-5" title="Permalink to this equation">¶</a></span>\[\int_a^b f(x) \;dx = F(b) - F(a).\]</div>
<p>This is a mathematical encoding of the fact that we can measure the area
out to the far end-point and then subtract off the area to the near end
point as indicated in <a class="reference internal" href="#fig-area-subtract"><span class="std std-numref">Fig. 22.5.1</span></a>.</p>
<div class="figure align-default" id="id1">
<span id="fig-area-subtract"></span><img alt="../_images/sub-area.svg" src="../_images/sub-area.svg" /><p class="caption"><span class="caption-number">Fig. 22.5.1 </span><span class="caption-text">Visualizing why we may reduce the problem of computing the area under
a curve between two points to computing the area to the left of a
point.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p>Thus, we can figure out what the integral over any interval is by
figuring out what <span class="math notranslate nohighlight">\(F(x)\)</span> is.</p>
<p>To do so, let’s consider an experiment. As we often do in calculus,
let’s imagine what happens when we shift the value by a tiny bit. From
the comment above, we know that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-6">
<span class="eqno">(22.5.7)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-6" title="Permalink to this equation">¶</a></span>\[F(x+\epsilon) - F(x) = \int_x^{x+\epsilon} f(y) \; dy.\]</div>
<p>This tells us that the function changes by the area under a tiny sliver
of a function.</p>
<p>This is the point at which we make an approximation. If we look at a
tiny sliver of area like this, it looks like this area is close to the
rectangular area with height the value of <span class="math notranslate nohighlight">\(f(x)\)</span> and the base
width <span class="math notranslate nohighlight">\(\epsilon\)</span>. Indeed, one can show that as
<span class="math notranslate nohighlight">\(\epsilon \rightarrow 0\)</span> this approximation becomes better and
better. Thus we can conclude:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-7">
<span class="eqno">(22.5.8)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-7" title="Permalink to this equation">¶</a></span>\[F(x+\epsilon) - F(x) \approx \epsilon f(x).\]</div>
<p>However, we can now notice: this is exactly the pattern we expect if we
were computing the derivative of <span class="math notranslate nohighlight">\(F\)</span>! Thus we see the following
rather surprising fact:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-8">
<span class="eqno">(22.5.9)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-8" title="Permalink to this equation">¶</a></span>\[\frac{dF}{dx}(x) = f(x).\]</div>
<p>This is the <em>fundamental theorem of calculus</em>. We may write it in
expanded form as</p>
<div class="math notranslate nohighlight" id="equation-eq-ftc">
<span class="eqno">(22.5.10)<a class="headerlink" href="#equation-eq-ftc" title="Permalink to this equation">¶</a></span>\[\frac{d}{dx}\int_0^x  f(y) \; dy = f(x).\]</div>
<p>It takes the concept of finding areas (<em>a priori</em> rather hard), and
reduces it to a statement derivatives (something much more completely
understood). One last comment that we must make is that this does not
tell us exactly what <span class="math notranslate nohighlight">\(F(x)\)</span> is. Indeed <span class="math notranslate nohighlight">\(F(x) + C\)</span> for any
<span class="math notranslate nohighlight">\(C\)</span> has the same derivative. This is a fact-of-life in the theory
of integration. Thankfully, notice that when working with definite
integrals, the constants drop out, and thus are irrelevant to the
outcome.</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-9">
<span class="eqno">(22.5.11)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-9" title="Permalink to this equation">¶</a></span>\[\int_a^b f(x) \; dx = (F(b) + C) - (F(a) + C) = F(b) - F(a).\]</div>
<p>This may seem like abstract non-sense, but let’s take a moment to
appreciate that it has given us a whole new perspective on computing
integrals. Our goal is no-longer to do some sort of chop-and-sum process
to try and recover the area, rather we need only find a function whose
derivative is the function we have! This is incredible since we can now
list many rather difficult integrals by just reversing the table from
<a class="reference internal" href="single-variable-calculus.html#sec-derivative-table"><span class="std std-numref">Section 22.3.2</span></a>. For instance, we know that the
derivative of <span class="math notranslate nohighlight">\(x^{n}\)</span> is <span class="math notranslate nohighlight">\(nx^{n-1}\)</span>. Thus, we can say using
the fundamental theorem <a class="reference internal" href="#equation-eq-ftc">(22.5.10)</a> that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-10">
<span class="eqno">(22.5.12)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-10" title="Permalink to this equation">¶</a></span>\[\int_0^{x} ny^{n-1} \; dy = x^n - 0^n = x^n.\]</div>
<p>Similarly, we know that the derivative of <span class="math notranslate nohighlight">\(e^{x}\)</span> is itself, so
that means</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-11">
<span class="eqno">(22.5.13)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-11" title="Permalink to this equation">¶</a></span>\[\int_0^{x} e^{x} \; dx = e^{x} - e^{0} = e^x - 1.\]</div>
<p>In this way, we can develop the entire theory of integration leveraging
ideas from differential calculus freely. Every integration rule derives
from this one fact.</p>
</div>
<div class="section" id="change-of-variables">
<span id="subsec-integral-example"></span><h2><span class="section-number">22.5.3. </span>Change of Variables<a class="headerlink" href="#change-of-variables" title="Permalink to this heading">¶</a></h2>
<p>Just as with differentiation, there are a number of rules which make the
computation of integrals more tractable. In fact, every rule of
differential calculus (like the product rule, sum rule, and chain rule)
has a corresponding rule for integral calculus (integration by parts,
linearity of integration, and the change of variables formula
respectively). In this section, we will dive into what is arguably the
most important from the list: the change of variables formula.</p>
<p>First, suppose that we have a function which is itself an integral:</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-12">
<span class="eqno">(22.5.14)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-12" title="Permalink to this equation">¶</a></span>\[F(x) = \int_0^x f(y) \; dy.\]</div>
<p>Let’s suppose that we want to know how this function looks when we
compose it with another to obtain <span class="math notranslate nohighlight">\(F(u(x))\)</span>. By the chain rule, we
know</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-13">
<span class="eqno">(22.5.15)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-13" title="Permalink to this equation">¶</a></span>\[\frac{d}{dx}F(u(x)) = \frac{dF}{du}(u(x))\cdot \frac{du}{dx}.\]</div>
<p>We can turn this into a statement about integration by using the
fundamental theorem <a class="reference internal" href="#equation-eq-ftc">(22.5.10)</a> as above. This gives</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-14">
<span class="eqno">(22.5.16)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-14" title="Permalink to this equation">¶</a></span>\[F(u(x)) - F(u(0)) = \int_0^x \frac{dF}{du}(u(y))\cdot \frac{du}{dy} \;dy.\]</div>
<p>Recalling that <span class="math notranslate nohighlight">\(F\)</span> is itself an integral gives that the left hand
side may be rewritten to be</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-15">
<span class="eqno">(22.5.17)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-15" title="Permalink to this equation">¶</a></span>\[\int_{u(0)}^{u(x)} f(y) \; dy = \int_0^x \frac{dF}{du}(u(y))\cdot \frac{du}{dy} \;dy.\]</div>
<p>Similarly, recalling that <span class="math notranslate nohighlight">\(F\)</span> is an integral allows us to
recognize that <span class="math notranslate nohighlight">\(\frac{dF}{dx} = f\)</span> using the fundamental theorem
<a class="reference internal" href="#equation-eq-ftc">(22.5.10)</a>, and thus we may conclude</p>
<div class="math notranslate nohighlight" id="equation-eq-change-var">
<span class="eqno">(22.5.18)<a class="headerlink" href="#equation-eq-change-var" title="Permalink to this equation">¶</a></span>\[\int_{u(0)}^{u(x)} f(y) \; dy = \int_0^x f(u(y))\cdot \frac{du}{dy} \;dy.\]</div>
<p>This is the <em>change of variables</em> formula.</p>
<p>For a more intuitive derivation, consider what happens when we take an
integral of <span class="math notranslate nohighlight">\(f(u(x))\)</span> between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(x+\epsilon\)</span>.
For a small <span class="math notranslate nohighlight">\(\epsilon\)</span>, this integral is approximately
<span class="math notranslate nohighlight">\(\epsilon f(u(x))\)</span>, the area of the associated rectangle. Now,
let’s compare this with the integral of <span class="math notranslate nohighlight">\(f(y)\)</span> from <span class="math notranslate nohighlight">\(u(x)\)</span>
to <span class="math notranslate nohighlight">\(u(x+\epsilon)\)</span>. We know that
<span class="math notranslate nohighlight">\(u(x+\epsilon) \approx u(x) + \epsilon \frac{du}{dx}(x)\)</span>, so the
area of this rectangle is approximately
<span class="math notranslate nohighlight">\(\epsilon \frac{du}{dx}(x)f(u(x))\)</span>. Thus, to make the area of
these two rectangles to agree, we need to multiply the first one by
<span class="math notranslate nohighlight">\(\frac{du}{dx}(x)\)</span> as is illustrated in
<a class="reference internal" href="#fig-rect-transform"><span class="std std-numref">Fig. 22.5.2</span></a>.</p>
<div class="figure align-default" id="id2">
<span id="fig-rect-transform"></span><img alt="../_images/rect-trans.svg" src="../_images/rect-trans.svg" /><p class="caption"><span class="caption-number">Fig. 22.5.2 </span><span class="caption-text">Visualizing the transformation of a single thin rectangle under the
change of variables.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<p>This tells us that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-16">
<span class="eqno">(22.5.19)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-16" title="Permalink to this equation">¶</a></span>\[\int_x^{x+\epsilon} f(u(y))\frac{du}{dy}(y)\;dy = \int_{u(x)}^{u(x+\epsilon)} f(y) \; dy.\]</div>
<p>This is the change of variables formula expressed for a single small
rectangle.</p>
<p>If <span class="math notranslate nohighlight">\(u(x)\)</span> and <span class="math notranslate nohighlight">\(f(x)\)</span> are properly chosen, this can allow for
the computation of incredibly complex integrals. For instance, if we
even chose <span class="math notranslate nohighlight">\(f(y) = 1\)</span> and <span class="math notranslate nohighlight">\(u(x) = e^{-x^{2}}\)</span> (which means
<span class="math notranslate nohighlight">\(\frac{du}{dx}(x) = -2xe^{-x^{2}}\)</span>), this can show for instance
that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-17">
<span class="eqno">(22.5.20)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-17" title="Permalink to this equation">¶</a></span>\[e^{-1} - 1 = \int_{e^{-0}}^{e^{-1}} 1 \; dy = -2\int_0^{1} ye^{-y^2}\;dy,\]</div>
<p>and thus by rearranging that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-18">
<span class="eqno">(22.5.21)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-18" title="Permalink to this equation">¶</a></span>\[\int_0^{1} ye^{-y^2}\; dy = \frac{1-e^{-1}}{2}.\]</div>
</div>
<div class="section" id="a-comment-on-sign-conventions">
<h2><span class="section-number">22.5.4. </span>A Comment on Sign Conventions<a class="headerlink" href="#a-comment-on-sign-conventions" title="Permalink to this heading">¶</a></h2>
<p>Keen-eyed readers will observe something strange about the computations
above. Namely, computations like</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-19">
<span class="eqno">(22.5.22)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-19" title="Permalink to this equation">¶</a></span>\[\int_{e^{-0}}^{e^{-1}} 1 \; dy = e^{-1} -1 &lt; 0,\]</div>
<p>can produce negative numbers. When thinking about areas, it can be
strange to see a negative value, and so it is worth digging into what
the convention is.</p>
<p>Mathematicians take the notion of signed areas. This manifests itself in
two ways. First, if we consider a function <span class="math notranslate nohighlight">\(f(x)\)</span> which is
sometimes less than zero, then the area will also be negative. So for
instance</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-20">
<span class="eqno">(22.5.23)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-20" title="Permalink to this equation">¶</a></span>\[\int_0^{1} (-1)\;dx = -1.\]</div>
<p>Similarly, integrals which progress from right to left, rather than left
to right are also taken to be negative areas</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-21">
<span class="eqno">(22.5.24)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-21" title="Permalink to this equation">¶</a></span>\[\int_0^{-1} 1\; dx = -1.\]</div>
<p>The standard area (from left to right of a positive function) is always
positive. Anything obtained by flipping it (say flipping over the
<span class="math notranslate nohighlight">\(x\)</span>-axis to get the integral of a negative number, or flipping
over the <span class="math notranslate nohighlight">\(y\)</span>-axis to get an integral in the wrong order) will
produce a negative area. And indeed, flipping twice will give a pair of
negative signs that cancel out to have positive area</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-22">
<span class="eqno">(22.5.25)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-22" title="Permalink to this equation">¶</a></span>\[\int_0^{-1} (-1)\;dx =  1.\]</div>
<p>If this discussion sounds familiar, it is! In
<a class="reference internal" href="geometry-linear-algebraic-ops.html#sec-geometry-linear-algebraic-ops"><span class="std std-numref">Section 22.1</span></a> we discussed how the
determinant represented the signed area in much the same way.</p>
</div>
<div class="section" id="multiple-integrals">
<h2><span class="section-number">22.5.5. </span>Multiple Integrals<a class="headerlink" href="#multiple-integrals" title="Permalink to this heading">¶</a></h2>
<p>In some cases, we will need to work in higher dimensions. For instance,
suppose that we have a function of two variables, like <span class="math notranslate nohighlight">\(f(x, y)\)</span>
and we want to know the volume under <span class="math notranslate nohighlight">\(f\)</span> when <span class="math notranslate nohighlight">\(x\)</span> ranges
over <span class="math notranslate nohighlight">\([a, b]\)</span> and <span class="math notranslate nohighlight">\(y\)</span> ranges over <span class="math notranslate nohighlight">\([c, d]\)</span>.</p>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar code"><a href="#pytorch-7-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-7-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-7-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-7-0"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Construct grid and compute function</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">101</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">101</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Plot function</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_wireframe</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="mi">12</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="mxnet-7-1"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Construct grid and compute function</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">101</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">101</span><span class="p">),</span>
                   <span class="n">indexing</span><span class="o">=</span><span class="s1">&#39;ij&#39;</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Plot function</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_wireframe</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(),</span> <span class="n">z</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="mi">12</span>
</pre></div>
</div>
</div><div class="mdl-tabs__panel " id="tensorflow-7-2"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Construct grid and compute function</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mi">101</span><span class="p">),</span> <span class="n">tf</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mi">101</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">-</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Plot function</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_wireframe</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="mi">12</span>
</pre></div>
</div>
</div></div><p>We write this as</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-23">
<span class="eqno">(22.5.26)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-23" title="Permalink to this equation">¶</a></span>\[\int_{[a, b]\times[c, d]} f(x, y)\;dx\;dy.\]</div>
<p>Suppose that we wish to compute this integral. My claim is that we can
do this by iteratively computing first the integral in <span class="math notranslate nohighlight">\(x\)</span> and
then shifting to the integral in <span class="math notranslate nohighlight">\(y\)</span>, that is to say</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-24">
<span class="eqno">(22.5.27)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-24" title="Permalink to this equation">¶</a></span>\[\int_{[a, b]\times[c, d]} f(x, y)\;dx\;dy = \int_c^{d} \left(\int_a^{b} f(x, y) \;dx\right) \; dy.\]</div>
<p>Let’s see why this is.</p>
<p>Consider the figure above where we have split the function into
<span class="math notranslate nohighlight">\(\epsilon \times \epsilon\)</span> squares which we will index with
integer coordinates <span class="math notranslate nohighlight">\(i, j\)</span>. In this case, our integral is
approximately</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-25">
<span class="eqno">(22.5.28)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-25" title="Permalink to this equation">¶</a></span>\[\sum_{i, j} \epsilon^{2} f(\epsilon i, \epsilon j).\]</div>
<p>Once we discretize the problem, we may add up the values on these
squares in whatever order we like, and not worry about changing the
values. This is illustrated in <a class="reference internal" href="#fig-sum-order"><span class="std std-numref">Fig. 22.5.3</span></a>. In particular,
we can say that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-26">
<span class="eqno">(22.5.29)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-26" title="Permalink to this equation">¶</a></span>\[\sum _ {j} \epsilon \left(\sum_{i} \epsilon f(\epsilon i, \epsilon j)\right).\]</div>
<div class="figure align-default" id="id3">
<span id="fig-sum-order"></span><img alt="../_images/sum-order.svg" src="../_images/sum-order.svg" /><p class="caption"><span class="caption-number">Fig. 22.5.3 </span><span class="caption-text">Illustrating how to decompose a sum over many squares as a sum over
first the columns (1), then adding the column sums together (2).</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<p>The sum on the inside is precisely the discretization of the integral</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-27">
<span class="eqno">(22.5.30)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-27" title="Permalink to this equation">¶</a></span>\[G(\epsilon j) = \int _a^{b} f(x, \epsilon j) \; dx.\]</div>
<p>Finally, notice that if we combine these two expressions we get</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-28">
<span class="eqno">(22.5.31)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-28" title="Permalink to this equation">¶</a></span>\[\sum _ {j} \epsilon G(\epsilon j) \approx \int _ {c}^{d} G(y) \; dy = \int _ {[a, b]\times[c, d]} f(x, y)\;dx\;dy.\]</div>
<p>Thus putting it all together, we have that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-29">
<span class="eqno">(22.5.32)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-29" title="Permalink to this equation">¶</a></span>\[\int _ {[a, b]\times[c, d]} f(x, y)\;dx\;dy = \int _ c^{d} \left(\int _ a^{b} f(x, y) \;dx\right) \; dy.\]</div>
<p>Notice that, once discretized, all we did was rearrange the order in
which we added a list of numbers. This may make it seem like it is
nothing, however this result (called <em>Fubini’s Theorem</em>) is not always
true! For the type of mathematics encountered when doing machine
learning (continuous functions), there is no concern, however it is
possible to create examples where it fails (for example the function
<span class="math notranslate nohighlight">\(f(x, y) = xy(x^2-y^2)/(x^2+y^2)^3\)</span> over the rectangle
<span class="math notranslate nohighlight">\([0,2]\times[0,1]\)</span>).</p>
<p>Note that the choice to do the integral in <span class="math notranslate nohighlight">\(x\)</span> first, and then the
integral in <span class="math notranslate nohighlight">\(y\)</span> was arbitrary. We could have equally well chosen
to do <span class="math notranslate nohighlight">\(y\)</span> first and then <span class="math notranslate nohighlight">\(x\)</span> to see</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-30">
<span class="eqno">(22.5.33)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-30" title="Permalink to this equation">¶</a></span>\[\int _ {[a, b]\times[c, d]} f(x, y)\;dx\;dy = \int _ a^{b} \left(\int _ c^{d} f(x, y) \;dy\right) \; dx.\]</div>
<p>Often times, we will condense down to vector notation, and say that for
<span class="math notranslate nohighlight">\(U = [a, b]\times [c, d]\)</span> this is</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-31">
<span class="eqno">(22.5.34)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-31" title="Permalink to this equation">¶</a></span>\[\int _ U f(\mathbf{x})\;d\mathbf{x}.\]</div>
</div>
<div class="section" id="change-of-variables-in-multiple-integrals">
<h2><span class="section-number">22.5.6. </span>Change of Variables in Multiple Integrals<a class="headerlink" href="#change-of-variables-in-multiple-integrals" title="Permalink to this heading">¶</a></h2>
<p>As with single variables in <a class="reference internal" href="#equation-eq-change-var">(22.5.18)</a>, the ability to
change variables inside a higher dimensional integral is a key tool.
Let’s summarize the result without derivation.</p>
<p>We need a function that reparametrizes our domain of integration. We can
take this to be <span class="math notranslate nohighlight">\(\phi : \mathbb{R}^n \rightarrow \mathbb{R}^n\)</span>,
that is any function which takes in <span class="math notranslate nohighlight">\(n\)</span> real variables and returns
another <span class="math notranslate nohighlight">\(n\)</span>. To keep the expressions clean, we will assume that
<span class="math notranslate nohighlight">\(\phi\)</span> is <em>injective</em> which is to say it never folds over itself
(<span class="math notranslate nohighlight">\(\phi(\mathbf{x}) = \phi(\mathbf{y}) \implies \mathbf{x} = \mathbf{y}\)</span>).</p>
<p>In this case, we can say that</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-32">
<span class="eqno">(22.5.35)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-32" title="Permalink to this equation">¶</a></span>\[\int _ {\phi(U)} f(\mathbf{x})\;d\mathbf{x} = \int _ {U} f(\phi(\mathbf{x})) \left|\det(D\phi(\mathbf{x}))\right|\;d\mathbf{x}.\]</div>
<p>where <span class="math notranslate nohighlight">\(D\phi\)</span> is the <em>Jacobian</em> of <span class="math notranslate nohighlight">\(\phi\)</span>, which is the
matrix of partial derivatives of
<span class="math notranslate nohighlight">\(\boldsymbol{\phi} = (\phi_1(x_1, \ldots, x_n), \ldots, \phi_n(x_1, \ldots, x_n))\)</span>,</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-33">
<span class="eqno">(22.5.36)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-33" title="Permalink to this equation">¶</a></span>\[\begin{split}D\boldsymbol{\phi} = \begin{bmatrix}
\frac{\partial \phi _ 1}{\partial x _ 1} &amp; \cdots &amp; \frac{\partial \phi _ 1}{\partial x _ n} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial \phi _ n}{\partial x _ 1} &amp; \cdots &amp; \frac{\partial \phi _ n}{\partial x _ n}
\end{bmatrix}.\end{split}\]</div>
<p>Looking closely, we see that this is similar to the single variable
chain rule <a class="reference internal" href="#equation-eq-change-var">(22.5.18)</a>, except we have replaced the term
<span class="math notranslate nohighlight">\(\frac{du}{dx}(x)\)</span> with
<span class="math notranslate nohighlight">\(\left|\det(D\phi(\mathbf{x}))\right|\)</span>. Let’s see how we can to
interpret this term. Recall that the <span class="math notranslate nohighlight">\(\frac{du}{dx}(x)\)</span> term
existed to say how much we stretched our <span class="math notranslate nohighlight">\(x\)</span>-axis by applying
<span class="math notranslate nohighlight">\(u\)</span>. The same process in higher dimensions is to determine how
much we stretch the area (or volume, or hyper-volume) of a little square
(or little <em>hyper-cube</em>) by applying <span class="math notranslate nohighlight">\(\boldsymbol{\phi}\)</span>. If
<span class="math notranslate nohighlight">\(\boldsymbol{\phi}\)</span> was the multiplication by a matrix, then we
know how the determinant already gives the answer.</p>
<p>With some work, one can show that the <em>Jacobian</em> provides the best
approximation to a multivariable function <span class="math notranslate nohighlight">\(\boldsymbol{\phi}\)</span> at a
point by a matrix in the same way we could approximate by lines or
planes with derivatives and gradients. Thus the determinant of the
Jacobian exactly mirrors the scaling factor we identified in one
dimension.</p>
<p>It takes some work to fill in the details to this, so do not worry if
they are not clear now. Let’s see at least one example we will make use
of later on. Consider the integral</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-34">
<span class="eqno">(22.5.37)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-34" title="Permalink to this equation">¶</a></span>\[\int _ {-\infty}^{\infty} \int _ {-\infty}^{\infty} e^{-x^{2}-y^{2}} \;dx\;dy.\]</div>
<p>Playing with this integral directly will get us no-where, but if we
change variables, we can make significant progress. If we let
<span class="math notranslate nohighlight">\(\boldsymbol{\phi}(r, \theta) = (r \cos(\theta), r\sin(\theta))\)</span>
(which is to say that <span class="math notranslate nohighlight">\(x = r \cos(\theta)\)</span>,
<span class="math notranslate nohighlight">\(y = r \sin(\theta)\)</span>), then we can apply the change of variable
formula to see that this is the same thing as</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-35">
<span class="eqno">(22.5.38)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-35" title="Permalink to this equation">¶</a></span>\[\int _ 0^\infty \int_0 ^ {2\pi} e^{-r^{2}} \left|\det(D\mathbf{\phi}(\mathbf{x}))\right|\;d\theta\;dr,\]</div>
<p>where</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-36">
<span class="eqno">(22.5.39)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-36" title="Permalink to this equation">¶</a></span>\[\begin{split}\left|\det(D\mathbf{\phi}(\mathbf{x}))\right| = \left|\det\begin{bmatrix}
\cos(\theta) &amp; -r\sin(\theta) \\
\sin(\theta) &amp; r\cos(\theta)
\end{bmatrix}\right| = r(\cos^{2}(\theta) + \sin^{2}(\theta)) = r.\end{split}\]</div>
<p>Thus, the integral is</p>
<div class="math notranslate nohighlight" id="equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-37">
<span class="eqno">(22.5.40)<a class="headerlink" href="#equation-chapter-appendix-mathematics-for-deep-learning-integral-calculus-37" title="Permalink to this equation">¶</a></span>\[\int _ 0^\infty \int _ 0 ^ {2\pi} re^{-r^{2}} \;d\theta\;dr = 2\pi\int _ 0^\infty re^{-r^{2}} \;dr = \pi,\]</div>
<p>where the final equality follows by the same computation that we used in
section <a class="reference internal" href="#subsec-integral-example"><span class="std std-numref">Section 22.5.3</span></a>.</p>
<p>We will meet this integral again when we study continuous random
variables in <a class="reference internal" href="random-variables.html#sec-random-variables"><span class="std std-numref">Section 22.6</span></a>.</p>
</div>
<div class="section" id="summary">
<h2><span class="section-number">22.5.7. </span>Summary<a class="headerlink" href="#summary" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>The theory of integration allows us to answer questions about areas
or volumes.</p></li>
<li><p>The fundamental theorem of calculus allows us to leverage knowledge
about derivatives to compute areas via the observation that the
derivative of the area up to some point is given by the value of the
function being integrated.</p></li>
<li><p>Integrals in higher dimensions can be computed by iterating single
variable integrals.</p></li>
</ul>
</div>
<div class="section" id="exercises">
<h2><span class="section-number">22.5.8. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this heading">¶</a></h2>
<ol class="arabic simple">
<li><p>What is <span class="math notranslate nohighlight">\(\int_1^2 \frac{1}{x} \;dx\)</span>?</p></li>
<li><p>Use the change of variables formula to integrate
<span class="math notranslate nohighlight">\(\int_0^{\sqrt{\pi}}x\sin(x^2)\;dx\)</span>.</p></li>
<li><p>What is <span class="math notranslate nohighlight">\(\int_{[0,1]^2} xy \;dx\;dy\)</span>?</p></li>
<li><p>Use the change of variables formula to compute
<span class="math notranslate nohighlight">\(\int_0^2\int_0^1xy(x^2-y^2)/(x^2+y^2)^3\;dy\;dx\)</span> and
<span class="math notranslate nohighlight">\(\int_0^1\int_0^2f(x, y) = xy(x^2-y^2)/(x^2+y^2)^3\;dx\;dy\)</span> to
see they are different.</p></li>
</ol>
<div class="mdl-tabs mdl-js-tabs mdl-js-ripple-effect"><div class="mdl-tabs__tab-bar text"><a href="#pytorch-9-0" onclick="tagClick('pytorch'); return false;" class="mdl-tabs__tab is-active">pytorch</a><a href="#mxnet-9-1" onclick="tagClick('mxnet'); return false;" class="mdl-tabs__tab ">mxnet</a><a href="#tensorflow-9-2" onclick="tagClick('tensorflow'); return false;" class="mdl-tabs__tab ">tensorflow</a></div><div class="mdl-tabs__panel is-active" id="pytorch-9-0"><p><a class="reference external" href="https://discuss.d2l.ai/t/1092">Discussions</a></p>
</div><div class="mdl-tabs__panel " id="mxnet-9-1"><p><a class="reference external" href="https://discuss.d2l.ai/t/414">Discussions</a></p>
</div><div class="mdl-tabs__panel " id="tensorflow-9-2"><p><a class="reference external" href="https://discuss.d2l.ai/t/1093">Discussions</a></p>
</div></div></div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">22.5. Integral Calculus</a><ul>
<li><a class="reference internal" href="#geometric-interpretation">22.5.1. Geometric Interpretation</a></li>
<li><a class="reference internal" href="#the-fundamental-theorem-of-calculus">22.5.2. The Fundamental Theorem of Calculus</a></li>
<li><a class="reference internal" href="#change-of-variables">22.5.3. Change of Variables</a></li>
<li><a class="reference internal" href="#a-comment-on-sign-conventions">22.5.4. A Comment on Sign Conventions</a></li>
<li><a class="reference internal" href="#multiple-integrals">22.5.5. Multiple Integrals</a></li>
<li><a class="reference internal" href="#change-of-variables-in-multiple-integrals">22.5.6. Change of Variables in Multiple Integrals</a></li>
<li><a class="reference internal" href="#summary">22.5.7. Summary</a></li>
<li><a class="reference internal" href="#exercises">22.5.8. Exercises</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="multivariable-calculus.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>22.4. Multivariable Calculus</div>
         </div>
     </a>
     <a id="button-next" href="random-variables.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>22.6. Random Variables</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>